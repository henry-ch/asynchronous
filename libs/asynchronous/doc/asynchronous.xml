<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="http://www.oasis-open.org/docbook/xml/5.0/rng/docbook.rng" type="xml"?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
    <info>
        <title>Boost Asynchronous</title>
        <author>
            <personname>Christophe Henry</personname>
            <email>christophe.j.henry@googlemail.com</email>
        </author>
        <copyright>
            <year>20013</year>
            <holder>
                <phrase> Distributed under the Boost Software License, Version 1.0. (See
                    accompanying file LICENSE_1_0.txt or copy at <link
                        xlink:href="http://www.boost.org/LICENSE_1_0.txt"
                        >http://www.boost.org/LICENSE_1_0.txt</link> ) </phrase>
            </holder>
        </copyright>
    </info>
    <preface>
        <title>Preface</title>
        <para>
            <emphasis role="underline">Note</emphasis>: Asynchronous is not part of the Boost
            library. It is planed to be offered for Review in 2014. At the moment it is still in
            development.</para>
        <para>Herb Sutter wrote in an article <command
                xlink:href="http://www.gotw.ca/publications/concurrency-ddj.htm"/> "The Free Lunch
            Is Over", meaning that developpers will be forced to learn to develop multi-threaded
            applications. This, however, brings a fundamental issue: multithreading is hard, it's
            full of ugly beasts waiting hidden for our mistakes: races, deadlocks, all kinds of
            subtle bugs. Worse yet, these bugs are hard to find because they are never reproducible
            when you are looking for them, which leaves us with backtrace analysis, and this is when
            we are lucky enough to have a backtrace in the first place.</para>
        <para>This is not even the only danger. CPUs are a magnitude faster than memory, I/O
            operations, network communications, which all stall our programms and degrade our
            performance, which means long sessions with coverage or analysis tools.</para>
        <para>Well, maybe the free lunch is not completely over yet, or at least maybe we can still
            get one a bit longer for a bargain. This is what Boost Asynchronous is helping
            solve.</para>
        <para>Boost Asynchronous is a library making it easy to write asynchronous code similar to
            the Proactor<command xlink:href="http://www.cs.wustl.edu/~schmidt/PDF/proactor.pdf"/>
            pattern. To achieve this, it offers tools for asynchronous designs: ActiveObject,
            threadpools, servants, proxies, queues, algorithms, etc. </para>
        <para>Asynchronous programming has the advantage of making it easier to design your code
            nonblocking, single-threaded while still getting your cores to work at full capacity.
            And all this while forgetting what a mutex is. Incorrect mutex usage is a huge source of
            bugs and Asynchronous helps you avoid them.</para>
        <para>However, the goal of this library is not to help you write massively parallel code.
            There are other solutions for this. This library is for the other 99% of us who happen
            to work on 4-12 cores hardware because 1000 cores are not affordable, but would still
            like to get the best of it while avoiding ugly bugs, get better diagnostic of what our
            application is doing and planing ourselves what our cores are doing. Asynchronous is not:<itemizedlist>
                <listitem>
                    <para>std/boost::async: both are asynchronous in a very limited way. One posts
                        asynchronously some work, but then? Well, then the choice is between
                        blocking for the future result (taboo) or polling from time to time, as in
                        the (bad) old times. Furthermore, one has no control on the
                        scheduler.</para>
                </listitem>
                <listitem>
                    <para>Intel TBB: this is a wonderful parallel library. But it's not asynchronous
                        as one needs to wait for the end of a parallel_ call. Sure, you have
                        pipelines, but when your application is complex, good luck to understand the
                        code. Again, one has limited control on the scheduler.</para>
                </listitem>
                <listitem>
                    <para>N3428: this is an interesting approach as it was at least recognized that
                        std::async is not asynchronous. So now, you get a kind of state machine
                        hidden behind a .then, when_any, when_all, which are a poor man's state
                        machines, besides ugly limitations like finding out which when_all threw an
                        exception. When did we give up writing design diagrams to document and
                        understand our code later? When a 15+ states state machine with guards,
                        event deferring and control flow has to be written with .then, please don't
                        ask me to review the code.</para>
                </listitem>
            </itemizedlist></para>
        <para>Let's have a quick look at code using futures and .then (taken from N3428):</para>
        <para>
            <programlisting>future&lt;int> f1 = async([]() { return 123; });
future&lt;string> f2 = f1.then([](future&lt;int> f) {return f.get().to_string();}); // here .get() won’t block
f2.get(); // just a "small get" at the end?</programlisting>
        </para>
        <para> Saying that there is only a "small get" at the end is, for an application with
            real-time constraints, equivalent to saying at a lockfree conference something like
            "what is all the fuss about? Can't we just add a small lock at the end?". Just try
            it...</para>
        <para>This brings us to a central point of Asynchronous: if we build a system with strict
            real-time constraints, there is no such thing as a small get(). We need to be able to
            react to any event in the system in a timely manner. And we can't affort to have lots of
            functions potentially waiting too long everywhere in our code. Therefore, .then is only
            good for an application of a few hunderds of lines. What about using a timed_wait
            instead? Nope. Either we wait too long before handling an error (this just limits the
            amount of time we wate waiting, that's all), or we wait not enough and we poll. In any
            case, while waiting, our thread cannot react to other events.</para>
        <para>All these libraries also have the disadvantage of working with functions, not classes.
            But we, normal developers, do use classes. And we want them safe. And you can hardly
            make a class safe if you simply pass it to another thread. Consider the following
            example:</para>
        <para>
            <programlisting>class Bad : public boost::signals::trackable
{
   int foo();
};
boost::shared_ptr&lt;Bad> b;
future&lt;int> f = async([b](){return b->foo()});          </programlisting>
        </para>
        <para> Now you have the ugly problem of not knowing in which thread Bad will be destroyed.
            And as it's pretty hard to have a thread-safe destructor, you find yourself with a race
            condition in it. Maybe you'll find a solution for signals, but are you sure nobody is
            ever going to do something dangerous in the destructor? This clearly is not thinking in
            the future sense.</para>
        <para>Another particularity of Asynchronous is that it's not hiding anything: one gets to
            pick a pool for a particular application, choose how many threads are to be used, and
            the type of queue which best corresponds to the job to do, which allows finer
            tuning.</para>
        <para>An image being more worth than thousand words, the following story will explain in a
            few minutes what Asynchronous is about. Consider some fast-food restaurant:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor1.jpg"/>
                </imageobject>
            </inlinemediaobject>
        </para>
        <para>This restaurant has a single employee, Worker, delivers burgers through a burger queue
            and drinks. A Customer comes. Then another, who waits until the first customer is
            served.</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor2.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>To keep customers happy by reducing waiting time, the restaurant owner hires a second
            employee:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor3.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>Unfortunately, this brings chaos in the restaurant. Sometimes, employes fight to get a
            burger to their own customer first:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-RC.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>And sometimes, they stay in each other's way:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-DL.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>This clearly is a not an optimal solution. Not only the additional employee brings
            additional costs, but both employees now spend much more time waiting. It also is not a
            scalable solution if even more customers want to eat because it's lunch-time right now.
            Even worse, as they fight for resources and stay in each other's way, the restaurant now
            serves people less fast than before. Customers flee and the restaurant gets bankrupt. A
            sad story, isn't it? To avoid this, the owner decides to go asynchronous. He keeps a
            single worker, who moves from cash desk to cash desk:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-async.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>The worker never waits because it would increase customer's waiting time. Instead, he
            runs from cash desks to the burger queue, beverage machine using a self-made strategy: <itemizedlist>
                <listitem>
                    <para>ask what the customer wants and keep an up-to-date information of the
                        customer's state.</para>
                </listitem>
                <listitem>
                    <para>if we have another customer at a desk, ask what he wants. For both
                        customers, remember the state of the order (waiting for customer choice,
                        getting food, getting drink, delivering, getting payment, etc.)</para>
                </listitem>
                <listitem>
                    <para>as soon as some new state is detected (customer choice, burger in the
                        queue, drink ready), handle it.</para>
                </listitem>
                <listitem>
                    <para>priorities are defined: start the longest-lasting tasks first, serve
                        angry-looking customers first, etc.</para>
                </listitem>
            </itemizedlist></para>
        <para>The following diagram shows us the busy worker in action:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-async2.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>Of course the owner needs a worker who runs fast, and has a pretty good memory so he
            can remember what customers are waiting for. </para>
        <para>This is what Asynchronous is for. A worker (thread) runs as long as there are waiting
            customers, following a precisely defined algorithm, and lots of state machines to manage
            the asynchronous behaviour. In case of customers, we could have a state machine: Waiting
            -> PickingMenu -> WaitingForFood -> Paying.</para>
        <para>We also need some queues (Burger queue, Beverage glass positioning) and some
            Asynchronous Operation Processor (for example a threadpool made of workers in the
            kitchen), event of different types (Drinks delivery). Maybe you also want some work
            stealing (someone in the kitchen serving drinks as he has no more burger to prepare. He
            will be slower than the machine, but still bring some time gain).</para>
        <para><emphasis role="bold">To make this work, the worker must not block, never,
                ever</emphasis>. And whatever he's doing has to be as fast as possible, otherwise
            the whole process stalls.</para>
    </preface>
    <part>
        <title>Concepts</title>
        <chapter>
            <title>Related designs: std::async, Active Object, Proactor</title>
            <sect1>
                <title>std::async</title>
                <subtitle>What is wrong with it</subtitle>
                <para>The following code is a classical use of std::async as it can be found in
                    articles, books, etc.</para>
                <programlisting>std::future&lt;int> f = std::async([](){return 42;}); // executes asynchronously
int res = f.get(); // wait for result, block until ready</programlisting>
                <para>It looks simple, easy to use, and everybody can get it. The problem is, well,
                    that it's not really asynchronous. True, our lambda will execute in another
                    thread. Actually, it's not even guaranteed either. But then, what do we do with
                    our future? Do we poll it? Or call get() as in the example? But then we will
                    block, right? And if we block, are we still asynchronous? We will postulate that
                    no, we're not. If we block, we cannot react to any event happening in our system
                    any more, we are unresponsive for a while (are we back to the old times of
                    freezing programs, the old time before threads?). We also probably miss some
                    parallelizing opportunities as we could be doing something more useful at the
                    same time, as in our fast-food example. And diagnostics are looking bad too as
                    we are blocked and cannot return any. What is left to us is polling. Polling?
                    No, it cannot be true, this is what the C++ standard offers us? And if we get
                    more and more futures, do we carry a bag of them with us at any time and check
                    them from time to time? Do we need some functions to, at a given point, wait for
                    all futures or any of them to be ready? </para>
                <para>Wait, yes it exists, <code>wait_for_all</code> and
                    <code>wait_for_any</code>... </para>
                <para>And what about this example from an online documentation?</para>
                <para>
                    <programlisting>{ 
   std::async(std::launch::async, []{ f(); }); 
   std::async(std::launch::async, []{ g(); });
}</programlisting>
                </para>
                <para>Every std::async returns you a future, a particularly mean one which blocks
                    upon destruction. This means that the second line will not execute until f()
                    completes. Now this is not only not asynchronous, it's also much slower than
                    calling sequentially f and g.</para>
                <para>No, really, this does not look good. Do we have alternatives?</para>
            </sect1>
            <sect1>
                <title>N3558 / N3650</title>
                <para>Of course it did not go unnoticed that std::async has some limitations. And so
                    do we see some tries to save it instead of giving it up. Usually, it goes around
                    the lines of blocking, but later.</para>
                <para>
                    <programlisting>future&lt;int> f1 = async([]() { return 123; }); 
future&lt;string> f2 = f1.then([](future&lt;int> f) 
{ 
  return f.get().to_string(); // here .get() won’t block 
});
// and here?
string s= f2.get();</programlisting>
                </para>
                <para>The idea is to make std::async more asynchronous (oh my, this already just
                    sounds bad) by adding something (.then) to be called when the asynchronous
                    action finishes. While it sounds good, it still does not fly:<itemizedlist>
                        <listitem>
                            <para>at some point, we will have to block, thus ending our asynchronous
                                behavior</para>
                        </listitem>
                        <listitem>
                            <para>This works only for very small programs. Do we imagine a 500k
                                lines program built that way?</para>
                        </listitem>
                    </itemizedlist></para>
                <para>And what about the suggestion of adding new keywords, async and await, as in
                    N3650? Nope. First because, as await suggests, someone will need, at some point,
                    to block waiting. Second because as we have no future, we also lose our polling
                    option.</para>
            </sect1>
            <sect1>
                <title>Active Object</title>
                <subtitle>Design</subtitle>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/ActiveObject.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>This simplified diagram shows a possible design variant of an Active Object
                    pattern.</para>
                <para>A thread-unsafe Servant is hidden behind a Proxy, which offers the same
                    members as the Servant itself. This Proxy is called by clients and delivers a
                    future object, which will, at some later point, contain the result of the
                    corresponding member called on the servant. The Proxy packs a MethodRequest
                    corresponding to a Servant call into the ActivationQueue. The Scheduler waits
                    permanently for MethodRequests in the queue, dequeues them, and executes them.
                    As only one scheduler waits for requests, it serializes access to the Servant,
                    thus providing thread-safety.</para>
                <para>However, this pattern presents some liabilities:<itemizedlist>
                        <listitem>
                            <para>Performance overhead: depending on the system, data moving and
                                context switching can be a performance drain.</para>
                        </listitem>
                        <listitem>
                            <para>Memory overhead: for every Servant, a thread has to be created,
                                consuming resources.</para>
                        </listitem>
                        <listitem>
                            <para>Usage: getting a future doesn't bring you as much asynchronous
                                behaviour as one might think. Usually, docs tell you to do something
                                else and check it later. But most cases simply mean that the client
                                will earlier or later block until the future is ready. This also
                                applies to std/boost::async.</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Proactor</title>
                <subtitle>Design</subtitle>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/Proactor.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>TODO</para>
            </sect1>
        </chapter>
        <chapter>
            <title>Features</title>
            <sect1>
                <title>Active Component</title>
                <subtitle>Extending Active Objects with more servants within a thread
                    context</subtitle>
                <para>A commonly cited drawback of Active Objects is that it's awfully expensive. A
                    thread per object is really a waste of ressources. Boost.Asynchronous extends
                    this concept by allowing an unlimited number of objects to live within a single
                    thread context, thus amortizing the costs.</para>
                <para>This brings another difference with ActiveObjects. As many objects are
                    potentially living in a thread context, none should be allowed to process
                    long-lasting tasks as it would reduce reactivity of the whole component. In this
                    aspect, Asynchronous' philosophy is closer to a Proactor.</para>
                <para>As long-lasting tasks do happen, Boost.Asynchronous provides several
                    implementations of threadpools and the infrastructure to make it safe to post
                    work to threadpools and get aynchronously a callback. It also provides safe
                    mechanisms to shutdown Active Components and threadpools.</para>
            </sect1>
            <sect1>
                <title>Shutting down</title>
                <para>Shutting down a thread turns out to be harder in practice than expected, as
                    shown by several posts of surprise when Boost.Thread tried to match the C++
                    Standard. Asynchronous hides all these ugly details. What users see is a proxy
                    object, which can be shared by any number of objects executing within any number
                    of threads. </para>
                <para>When the last instance of the inner-ActiveComponent scheduler object is
                    destroyed, the scheduler thread is stopped. When the last instance of a
                    scheduler proxy is destroyed, the scheduler thread is joined. It's as simple as
                    that. This makes threads shared objects.</para>
            </sect1>
            <sect1>
                <title>Object lifetime</title>
                <para>There are subtle bugs when living in a multithreaded world. Consider the
                    following class:</para>
                <para>
                    <programlisting>struct Unsafe
{
void foo()
{
  m_mutex.lock();
  // call private member
  m_mutex.unlock();
}
private:
void foobar()
{
  //we are already locked when called, do something while locked
}
boost::mutex m_mutex;
};            </programlisting>
                </para>
                <para>This is called a thread-safe interface pattern. Public members lock, private
                    do not. Simple enough, right? Unfortunately, it doesn't fly.</para>
                <para>First you have the risk of deadlock if a private member calls a public one
                    while being called from another public member. Forget to check one path of
                    execution within your class implementation and you get a nice deadlock. You'll
                    have to test every single path of execution to prove your code is correct. And
                    this at every change. Hmmm, does not sound reassuring.</para>
                <para>Anyway, let's face it, for any complex class, where there's a mutex, there is
                    a race or a deadlock...</para>
                <para>But even worse, the principle itself is not correct in C++. It supposes that a
                    class can protect itself. Well, no, it can't. Why? You can't protect the
                    destructor. If the object (and the mutex) gets destroyed when a thread waits for
                    it in foo(), we're toast. Ok, then we can use a shared_ptr, and all is good,
                    right? Then you have no destructor called as someone keeps the object alive,
                    right? Well, you still have a risk of a signal, callback, etc. </para>
                <para>What you need is protect your object with a shared_ptr and have no other way
                    to access the object. Asynchronous provides this.</para>
                <para>There are more lifetime issues, even without mutexes. If you have ever used
                    Boost.Asio, a common mistake and an easy one is when a callback is called in the
                    proactor thread after an asynchronous operation, but the object called is long
                    gone and the callback invalid. Asynchronous provides <command
                        xlink:href="#trackable_servant">trackable_servant</command> which makes sure
                    that a callback is not called if the object which called the asynchronous
                    operation is gone. It also prevents a task posted in a threadpool to be called
                    if this condition occurs, which improves performance. Some helper members even
                    make sure you get no crash, even while facing a Boost.Asio callback.</para>
            </sect1>
            <sect1>
                <title>Servant Proxies</title>
                <para>Asynchronous offers <code>servant_proxy</code>, which make the outside world
                    call members of a servant as if it was not living in an ActiveObject. It looks
                    like a thread-safe interface, but safe from deadlock and race conditions unles
                    you really try hard. </para>
            </sect1>
            <sect1>
                <title>Interrupting</title>
                <subtitle>Or how to catch back if you're drowning. </subtitle>
                <para>Let's say you posted so many tasks to your threadpool that all your cores are
                    full, still, your application is slipping more and more behind plan. You need to
                    give up some tasks to catch back a little.</para>
                <para>Asynchronous can give you an interruptible cookie when you post a task to a
                    scheduler, and you can use it to <command xlink:href="#interrupting_tasks">stop a posted task</command>. If not running yet, the
                    task will not start, if running, it will stop at the next interruption point,
                    which are documented in the <link
                        xlink:href="http://www.boost.org/doc/libs/1_54_0/doc/html/thread/thread_management.html#thread.thread_management.tutorial.interruption"
                        >Boost.Thread documentation</link>. Diagnostics will show that a task was
                    interrupted.</para>
            </sect1>
            <sect1>
                <title>Diagnostics</title>
                <para>Finding out how good your software is doing is not an easy task. You need to
                    add lots of logging to find out which function call takes too long and becomes a
                    bottleneck. Finding out the minimum required hardware to run your application is
                    even harder.</para>
                <para>Asynchronous design helps here too. By logging the required time and the
                    frequency of tasks, it is easy to find out how many cores are needed.
                    Bottlenecks can be found by logging what the Active Component is doing and how
                    long. Finally, designing the asynchronous Active Component as state machines and
                    logging state changes will allow a better understanding of your system and make
                    visible potential for concurrency. Even for non-parallel algorithms, finding
                    out, using a state machine, the earliest point a task can be thrown to a
                    threadpool will give some easy concurrency. Throw enough tasks to the threadpool
                    and manage this with a state machine and you might use your cores with little
                    effort. Parallelization can then be improved by logging which tasks are long
                    enough to be parallelized.</para>
            </sect1>
            <sect1>
                <title>Continuations</title>
                <para>Callback are great when you have a complex flow of operations which require a
                    state machine for management, however there are cases where callbacks are not an
                    ideal solution. Either because your application requires a constant switching of
                    context between single-threaded and parallel parts, or because the
                    single-threaded part might be busy, which would delay completion of the
                    algorithm. A known example of this is a parallel fibonacci. In this case, one
                    can register a <command xlink:href="#continuations">continuation</command>, which is to be executed upon completion of one or
                    several tasks. </para>
                <para>This mechanism is flexible so that you can use it with futures coming from
                    another library, thus removing any need for a
                        <code>wait_for_all(futures...)</code> or a
                        <code>wait_for_any(futures...)</code>.</para>
            </sect1>
            <sect1>
                <title>Want more power? What about extra machines?</title>
                <para>What to do if your threadpools are using all of your cores but there simply
                    are not enough cores for the job? Buy more cores? Unfortunately, the number of
                    cores a single-machine can use is limited, unless you have unlimited money. A
                    dual 6-core Xeon, 24 threads with hyperthreading will cost much more than 2 x
                    6-core i7, and will usually have a lesser clock frequency and an older
                    architecture. </para>
                <para>The solution could be: start with the i7, then if you need more power, add
                    some more machines which will steal jobs from your threadpools with <command xlink:href="#distributing">TCP</command>. This
                    can be done quite easily with Asynchronous.</para>
                <para>Want to build your own hierarchical network of servers? It's hard to make it
                    easier.</para>
            </sect1>
            <sect1>
                <title>Parallel algorithms</title>
                <para>The library also comes with <command xlink:href="#parallel_algos">non-blocking
                        algorithms</command> with iterators or ranges, support for TCP, which fit
                    well in the asynchronous system, with more to come. If you want to contribute
                    some more, be welcome. At the moment, the library offers:<itemizedlist>
                        <listitem>
                            <para>parallel_for:</para>
                        </listitem>
                        <listitem>
                            <para>parallel_reduce</para>
                        </listitem>
                        <listitem>
                            <para>parallel_extremum</para>
                        </listitem>
                        <listitem>
                            <para>parallel_count</para>
                        </listitem>
                        <listitem>
                            <para>parallel_find_all</para>
                        </listitem>
                        <listitem>
                            <para>parallel_invoke</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Task Priority</title>
                <para>Asynchronous offers this possibility for all schedulers at low performance
                    cost. This means you not only have the possibility to influence task execution
                    order in a threadpool but also in Active Objects.</para>
                <para>This is achieved by posting a task to the queue with the corresponding
                    priority. It is also possible to get it even more fine-grained by using a queue
                    of queues of queues, etc.</para>
            </sect1>
            <sect1>
                <title>Integrating with Boost.Asio</title>
                <para>Asynchronous offers a Boost.Asio based <command xlink:href="#asio_scheduler">scheduler</command> allowing you to easily write
                    a Servant using Asio, or an Asio based threadpool. An advantage is that you get
                    safe callbacks and easily get your Asio application to scale. Writing a server
                    has never been easier.</para>
                <para>Asynchronous also uses Boost.Asio to provide a timer with callbacks.</para>
            </sect1>
            <sect1>
                <title>Integrating with Qt</title>
                <para>What about getting the power of Asynchronous within a Qt application? Use
                    Asynchronous' threadpools, algorithms and other cool features easily.</para>
            </sect1>
            <sect1>
                <title>Work Stealing</title>
                <para>Work stealing is supported both within the threads of a threadpool but also
                    between different threadpools. Please have a look at Asynchronous' composite
                    scheduler.</para>
            </sect1>
            <sect1>
                <title>Extending the library</title>
                <para>Asynchronous has been written with the design goal of allowing anybody to
                    extend the library. In particular, the author is hoping to be offered the
                    following extensions:<itemizedlist>
                        <listitem>
                            <para>Schedulers, Threadpools</para>
                        </listitem>
                        <listitem>
                            <para>Queues</para>
                        </listitem>
                        <listitem>
                            <para>Parallel algorithms</para>
                        </listitem>
                        <listitem>
                            <para>Integration with other libraries</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Design Diagrams</title>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/AsynchronousDesign.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>This diagram shows an overview of the design behind Asynchronous. One or more
                    Servant objects live in a single-theaded world, communicating with the outside
                    world only with one or several queues, from which the single-threaded scheduler
                    pops tasks. Tasks are pushed by calling a member on a proxy object.</para>
                <para>Like an Active Object, a client uses a proxy (a shared object type), which
                    offers the same members as the real servant, with the same parameters, the only
                    difference being the return type, a boost::future&lt;R>, with R being the return
                    type of the servant's member. All calls to a servant from the client side are
                    posted, which includes the servant constructor and destructor. When the last
                    instance of a servant is destroyed, be it used inside the Active Component or
                    outside, the servant destructor is posted.</para>
                <para>any_shared_scheduler is the part of the Active Object scheduler living inside
                    the Active Component. Servants do not hold it directly but hold an
                    any_weak_scheduler instead. The library will use it to create a posted callback
                    when a task executing in a worker threadpool is completed.</para>
                <para>Shutting down an Active Component is done automatically by not needing it. It
                    happens in the following order:<itemizedlist>
                        <listitem>
                            <para>While a servant proxy is alive, no shutdown</para>
                        </listitem>
                        <listitem>
                            <para>When the last servant proxy goes out of scope, the servant
                                destructor is posted.</para>
                        </listitem>
                        <listitem>
                            <para>if jobs from servants are running in a threadpool, they get a
                                chance to stop earlier by running into an interruption point or will
                                not even start.</para>
                        </listitem>
                        <listitem>
                            <para>threadpool(s) is (are) shut down.</para>
                        </listitem>
                        <listitem>
                            <para>The Active Component scheduler is stopped and its thread
                                terminates.</para>
                        </listitem>
                        <listitem>
                            <para>The last instance of any_shared_scheduler_proxy goes out of scope
                                with the last servant proxy and joins.</para>
                        </listitem>
                    </itemizedlist>
                </para>
                <para>It is usually accepted that threads are orthogonal to an OO design and
                    therefore are to manage as they don't belong to an object. Asynchronous comes
                    close to this: threads are not directly used, but instead owned by a scheduler,
                    in which one creates objects and tasks.</para>
            </sect1>
        </chapter>
    </part>
    <part>
        <title>User Guide</title>
        <chapter>
            <title>Using Asynchronous</title>
            <sect1>
                <title>Hello, asynchronous world</title>
                <para>The following code shows a very basic usage (a complete example <link
                        xlink:href="examples/example_post_future.cpp">here</link>), this is not
                    really asynchronous yet:</para>
                <programlisting>#include &lt;boost/asynchronous/scheduler/threadpool_scheduler.hpp>
#include &lt;boost/asynchronous/queue/lockfree_queue.hpp>
#include &lt;boost/asynchronous/scheduler_shared_proxy.hpp>
#include &lt;boost/asynchronous/post.hpp>
struct void_task
{
    void operator()()const
    {
        std::cout &lt;&lt; "void_task called" &lt;&lt; std::endl;
    }
};
struct int_task
{
    int operator()()const
    {
        std::cout &lt;&lt; "int_task called" &lt;&lt; std::endl;
        return 42;
    }
};  

// create a threadpool scheduler with 3 threads and communicate with it using a threadsafe_list
// we use auto as it is easier than boost::asynchronous::any_shared_scheduler_proxy&lt;>
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                            new boost::asynchronous::threadpool_scheduler&lt;
                                boost::asynchronous::lockfree_queue&lt;> >(3));
// post a simple task and wait for execution to complete
boost::shared_future&lt;void> fuv = boost::asynchronous::post_future(scheduler, void_task());
fuv.get();
// post a simple task and wait for result
boost::shared_future&lt;int> fui = boost::asynchronous::post_future(scheduler, int_task());
int res = fui.get();
  </programlisting>
                <para>Of course this works with C++11 lambdas:</para>
                <programlisting>auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                            new boost::asynchronous::threadpool_scheduler&lt;
                                boost::asynchronous::lockfree_queue&lt;> >(3));
// post a simple task and wait for execution to complete
boost::shared_future&lt;void> fuv =
                boost::asynchronous::post_future(scheduler, [](){std::cout &lt;&lt; "void lambda" &lt;&lt; std::endl;});
fuv.get();
// post a simple task and wait for result
boost::shared_future&lt;int> fui =
                boost::asynchronous::post_future(scheduler, [](){std::cout &lt;&lt; "int lambda" &lt;&lt; std::endl;return 42;});
int res = fui.get();   </programlisting>
                <para>boost::asynchronous::post_future posts a piece of work to a threadpool
                    scheduler with 3 threads and using a simple threadsafe_list. We get a
                    boost::future&lt;the type of the task return type>.</para>
                <para>This looks like much std::async, but we're just getting started. Let's move on
                    to something more asynchronous.</para>
            </sect1>
            <sect1>
                <title>A servant proxy</title>
                <para>We now want to create a single-threaded scheduler, populate it with some
                    servant(s), and exercise some members of the servant from an outside thread. We
                    first need a servant:</para>
                <programlisting>struct Servant
{
    // optional: the servant has such an easy constructor, no need to post it
    typedef int simple_ctor;
    Servant(int data): m_data(data){}
    int doIt()const
    {
        std::cout &lt;&lt; "Servant::doIt with m_data:" &lt;&lt; m_data &lt;&lt; std::endl;
        return 5;
    }
    void foo(int&amp; i)const
    {
        std::cout &lt;&lt; "Servant::foo with int:" &lt;&lt; i &lt;&lt; std::endl;
        i = 100;
    }
    void foobar(int i, char c)const
    {
        std::cout &lt;&lt; "Servant::foobar with int:" &lt;&lt; i &lt;&lt; " and char:" &lt;&lt; c &lt;&lt;std::endl;
    }
    int m_data;
}; </programlisting>
                <para>We now create a proxy type to be used in other threads:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,<emphasis role="bold">Servant</emphasis>>
{
public:
    // forwarding constructor. Scheduler to servant_proxy, followed by arguments to Servant.
    template &lt;class Scheduler>
    ServantProxy(Scheduler s, int data):
        boost::asynchronous::servant_proxy&lt;ServantProxy,<emphasis role="bold">Servant</emphasis>>(s, data)
    {}
    // the following members must be available "outside"
    // foo and foobar, just as a post (no interesting return value)
    BOOST_ASYNC_POST_MEMBER(<emphasis role="bold">foo</emphasis>)
    BOOST_ASYNC_POST_MEMBER(<emphasis role="bold">foobar</emphasis>)
    // for doIt, we'd like a future
    BOOST_ASYNC_FUTURE_MEMBER(<emphasis role="bold">doIt</emphasis>)
};</programlisting>
                <para>Let's use our newly defined proxy:</para>
                <programlisting>int something = 3;
{
    // with c++11
    auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;> >);

    {
        // arguments (here 42) are forwarded to Servant's constructor
        ServantProxy proxy(scheduler,42);
        // post a call to foobar, arguments are forwarded.
        proxy.foobar(1,'a');
        // post a call to foo. To avoid races, the reference is ignored.
        proxy.foo(something);
        // post and get a future because we're interested in the result.
        boost::shared_future&lt;int> fu = proxy.doIt();
        std::cout&lt;&lt; "future:" &lt;&lt; fu.get() &lt;&lt; std::endl;
    }// here, Servant's destructor is posted
}// scheduler is gone, its thread has been joined
std::cout&lt;&lt; "something:" &lt;&lt; something &lt;&lt; std::endl; // something was not changed</programlisting>
                <para>We can call members on the proxy, almost as if they were called on Servant.
                    The library takes care of the posting and forwarding the arguments. When
                    required, a future is returned. Stack unwinding works, and when the servant
                    proxy goes out of scope, the servant destructor is posted. When the scheduler
                    goes out of scope, its thread is stopped and joined. The queue is processed
                    completely first. Of course, as many servants as desired can be created in this
                    scheduler context. Please have a look at <link
                        xlink:href="examples/example_simple_servant.cpp">the complete
                    example</link>.</para>
            </sect1>
            <sect1>
                <title>Using a threadpool</title>
                <para>If you remember the principles of Asynchronous, blocking a single-thread
                    scheduler is taboo as it blocks the thread doing all the management of a system.
                    But what to do when one needs to execute long tasks? Asynchronous provides a
                    whole set of threadpools. A servant posts something to a threadpool, provides a
                    callback, then gets a result. Wait a minute. Callback? Is this not
                    thread-unsafe? Why not threadpools with futures, like usual? Because in a
                    perfectly asynchronous world, waiting for a future means blocking a servant
                    scheduler. One would argue that it is possible not to block on the future, and
                    instead ask if there is a result. But then, what if not? Is the alternative to
                    poll? Like in the "good" all times?</para>
                <para>If we accept the future argument, but what about thread-safety? Asynchronous
                    takes care of this. A callback is never called from a threadpool, but instead
                    posted back to the queue of the scheduler which posted the work. All the servant
                    has to do is to do nothing and wait until the callback is executed. Note that
                    this is not the same as a blocking wait, the servant can still react to
                    events.</para>
                <para>Clearly, this brings some new challenges as the flow of control gets harder to
                    follow. This is why a servant is often written using state machines. The
                    (biased) author suggests to have a look at the <link
                        xlink:href="http://svn.boost.org/svn/boost/trunk/libs/msm/doc/HTML/index.html"
                        > Meta State Machine library </link> , which plays nicely with
                    Asynchronous.</para>
                <para>But what about the usual proactor issues of crashing when the servant has long
                    been destroyed when the callback is posted. Gone. Asynchronous provides
                    <command xml:id="trackable_servant"/><code>trackable_servant</code> which will ensure that a callback is not
                    called if the servant is gone. Better even, if the servant has been destroyed,
                    an unstarted posted task will not be executed.</para>
                <para>Again comes another issue. And what if I post a task, say a lambda, which
                    captures a shared_ptr to an object per value, and this object is a
                    boost::signal? Then when the task object has been executed and is destroyed,
                    I'll get a race on the signal deregistration. Good point, but again no.
                    Asynchronous ensures that a task created within a scheduler context gets
                    destroyed in this context.</para>
                <para>This is about the best protection you can get. What Asynchronous cannot
                    protect you from are self-made races within a task (if you post a task with a
                    pointer to the servant, you're on your own and have to protect your servant). A
                    good rule of thumb is to consider data passed to a task as moved. To support
                    this, Asynchronous does not copy tasks but move them.</para>
                <para>Armed with these protections, let's give a try to a threadpool, starting with
                    the most basic one, <code>threadpool_scheduler</code> (more to come):</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;> scheduler)
        : boost::asynchronous::trackable_servant&lt;>(scheduler,
                                               // <emphasis role="bold">threadpool with 3 threads</emphasis> and a lockfree_queue
                                               boost::asynchronous::create_shared_scheduler_proxy(
                                                   new <emphasis role="bold">boost::asynchronous::<emphasis role="bold">threadpool_scheduler</emphasis></emphasis>&lt;
                                                           boost::asynchronous::lockfree_queue&lt;> >(<emphasis role="bold">3</emphasis>))){}
    // call to this is posted and executes in our (safe) single-thread scheduler
    void start_async_work()
    {
       //ok, let's post some work and wait for an answer
       <emphasis role="bold">post_callback</emphasis>(
                    [](){std::cout &lt;&lt; "Long Work" &lt;&lt; std::endl;}, //work, do not use "this" here
                    [/*this*/](boost::future&lt;void>){...}// callback. Safe to use "this" as callback is only called if Servant is alive
        );
    }
};</programlisting>
                <para>We now have a servant, ready to be created in its own thread, which posts some
                    long work to a 3 thread-threadpool and gets a callback, but only if still alive.
                    Similarly, the long work will be executed by the threadpool only if Servant is
                    alive. Everything else stays the same, one creates a proxy for the servant and
                    posts calls to its members, so we'll skip it for conciseness, the complete
                    example can be found <link
                        xlink:href="examples/example_post_trackable_threadpool.cpp"
                    >here</link>.</para>
            </sect1>
            <sect1>
                <title>A servant using another servant proxy</title>
                <para>Often, in a layered design, you'll need that a servant in a single-threaded
                    scheduler calls a member of a servant living in another one. And you'll want to
                    get a callback, not a future like in our previous example, because you
                    absolutely refuse to block waiting for a future (and you'll be very right of
                    course!). Ideally, except for main(), you won't want any of your objects to wait
                    for a future. There is another servant_proxy macro for this,
                        <code>BOOST_ASYNC_UNSAFE_MEMBER</code>(unsafe because you get no
                    thread-safety from if and you'll take care of this yourself, or better,
                        <code>trackable_servant</code> will take care of it for you):</para>
                <para>
                    <programlisting>// Proxy for a basic servant 
class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s, int data):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s, data)
    {}
    BOOST_ASYNC_UNSAFE_MEMBER(foo)
    BOOST_ASYNC_UNSAFE_MEMBER(foobar)
};   </programlisting>
                    <programlisting>// Servant using the first one
struct Servant2 : boost::asynchronous::trackable_servant&lt;>
{
    Servant2(boost::asynchronous::any_weak_scheduler&lt;> scheduler,ServantProxy worker)
        :boost::asynchronous::trackable_servant&lt;>(scheduler)
        ,m_worker(worker) // the proxy allowing access to Servant
    boost::shared_future&lt;void> doIt    
    {                 
         call_callback(m_worker.get_proxy(), // Servant's outer proxy, for posting tasks
                       m_worker.foo(), // what we want to call on Servant
                      // callback functor, when done.
                      [](boost::future&lt;int> result){...} );// future&lt;return type of foo> 
    }
};</programlisting>
                </para>
                <para>Call of <code>foo()</code> will be posted to <code>Servant</code>'s scheduler,
                    and the callback lambda will be posted to <code>Servant2</code> when completed.
                    All this thread-safe of course. Destruction is also safe. When
                        <code>Servant2</code> goes out of scope, it will shutdown
                        <code>Servant</code>'s scheduler, then will his scheduler be shutdown
                    (provided no more object is living there), and all threads joined. The <link
                        xlink:href="examples/example_two_simple_servants.cpp">complete example
                    </link> shows a few more calls too.</para>
            </sect1>
            <sect1>
                <title><command xml:id="interrupting_tasks"/>Interrupting tasks</title>
                <para>Imagine a manager object (a state machine for example) posted some
                    long-lasting work to a threadpool, but this long-lasting work really takes too
                    long. As we are in an asynchronous world and non-blocking, the manager object
                    realizes there is a problem and decides the task must be stopped otherwise the
                    whole application starts failing some real-time constraints. This is also
                    possible, one uses another version of posting, gets some handle, on which one
                    can require interruption. As Asynchronous does not kill threads, it means that
                    we'll have to use one of Boost.Thread predefined interruption points. Supposing
                    we have well-behaved tasks, they will be interrupted at the next interruption
                    point if they started, or if they did not start yet because they are waiting in
                    a queue, then they will never start. In this <link
                        xlink:href="examples/example_interrupt.cpp">example</link>, we have very
                    little to change but the post call:</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
     ... // as usual
    void start_async_work()
    {
        // start long interruptible tasks
        // we get an interruptible handler representing the task
        <emphasis role="bold">boost::asynchronous::any_interruptible</emphasis> interruptible =
        <emphasis role="bold">interruptible_post_callback</emphasis>(
                // interruptible task
               [](){
                    std::cout &lt;&lt; "Long Work" &lt;&lt; std::endl;
                    boost::this_thread::sleep(boost::posix_time::milliseconds(1000));}, // sleep is an interrupting point
               // callback functor.
               [](boost::future&lt;void> ){std::cout &lt;&lt; "Callback will most likely not be called" &lt;&lt; std::endl;}
        );
        // let the task start (not sure but likely)
        // if it had no time to start, well, then it will never.
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // actually, we changed our mind and want to interrupt the task
        interruptible.interrupt();
        // the callback will never be called as the task was interrupted
    }
};                </programlisting>
            </sect1>
            <sect1>
                <title><command xml:id="logging_tasks"/>Logging tasks</title>
                <para>Developers are notoriously famous for being bad at guessing which part of
                    their code is inefficient or has potential for long execution time. This is bad
                    in itself, but even worse for a control class like our post-callback servant as
                    it reduces responsiveness. Knowing how long a posted call or a callback lasts is
                    therefore very useful. Knowing how long take tasks executing in the threadpools
                    is also essential to plan what hardware one needs for an application(4 cores? Or
                    100?). We need to know what our program is doing. Asynchronous provides some
                    logging per task to help there. Let's have a look at some code. It's also time
                    to start using our template parameters for <code>trackable_servant</code>, in
                    case you wondered why they are here.</para>
                <programlisting>// we will be using loggable jobs internally
typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;
// the type of our log
typedef std::map&lt;std::string,std::list&lt;boost::asynchronous::diagnostic_item&lt;boost::chrono::high_resolution_clock> > > <emphasis role="bold">diag_type</emphasis>;
// we log our scheduler and our threadpool scheduler (both use servant_job)
struct Servant : boost::asynchronous::trackable_servant&lt;<emphasis role="bold">servant_job</emphasis>,<emphasis role="bold">servant_job</emphasis>>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;servant_job> scheduler) //servant_job is our job type
        : boost::asynchronous::trackable_servant&lt;<emphasis role="bold">servant_job,servant_job</emphasis>>(scheduler,
                                               boost::asynchronous::create_shared_scheduler_proxy(
                                                   // threadpool with 3 threads and a simple threadsafe_list queue
                                                   // Furthermore, it logs posted tasks
                                                   new boost::asynchronous::threadpool_scheduler&lt;
                                                           //servant_job is our job type
                                                           boost::asynchronous::lockfree_queue&lt; <emphasis role="bold">servant_job</emphasis> > >(3))){}
    void start_async_work()
    {
         post_callback(
               // task posted to threadpool
               [](){...}, // will return an int
               [](boost::future&lt;int> res){...},// callback functor.
               // the task / callback name for logging
               <emphasis role="bold">"int_async_work"</emphasis>
        );
    }
    // we happily provide a way for the outside world to know what our threadpool did.
    // get_worker is provided by trackable_servant and gives the proxy of our threadpool
    diag_type get_diagnostics() const
    {
        return (*get_worker()).get_diagnostics();
    }
};</programlisting>
                <para>The proxy is also slightly different, as it uses a _LOG macro and an argument
                    representing the name of the task.</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant,servant_job>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant,servant_job>(s)
    {}
    // the _LOG macros do the same as the others, but take an extra argument, the logged task name
    BOOST_ASYNC_FUTURE_MEMBER<emphasis role="bold">_LOG</emphasis>(start_async_work,<emphasis role="bold">"proxy::start_async_work"</emphasis>)
    BOOST_ASYNC_FUTURE_MEMBER<emphasis role="bold">_LOG</emphasis>(get_diagnostics,<emphasis role="bold">"proxy::get_diagnostics"</emphasis>)
    };               </programlisting>
                <para> We now can get diagnostics from both schedulers, the single-threaded and the
                    threadpool (as external code has no access to it, we ask Servant to help us
                    there through a get_diagnostics() member).</para>
                <programlisting>// create a scheduler with logging
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                    boost::asynchronous::lockfree_queue&lt;servant_job> >);
// create a Servant                    
ServantProxy proxy(scheduler); 
...
// let's ask the single-threaded scheduler what it did.
diag_type single_thread_sched_diag = (*scheduler).get_diagnostics(); 
for (auto mit = single_thread_sched_diag.begin(); mit != single_thread_sched_diag.end() ; ++mit)
{
     std::cout &lt;&lt; "job type: " &lt;&lt; (*mit).first &lt;&lt; std::endl;
     for (auto jit = (*mit).second.begin(); jit != (*mit).second.end();++jit)
     {
          std::cout &lt;&lt; "job waited in us: " &lt;&lt; boost::chrono::nanoseconds((*jit).get_started_time() - (*jit).<emphasis role="bold">get_posted_time()</emphasis>).count() / 1000 &lt;&lt; std::endl;
          std::cout &lt;&lt; "job lasted in us: " &lt;&lt; boost::chrono::nanoseconds((*jit).get_finished_time() - (*jit).<emphasis role="bold">get_started_time()</emphasis>).count() / 1000 &lt;&lt; std::endl;
          std::cout &lt;&lt; "job interrupted? "  &lt;&lt; std::boolalpha &lt;&lt; (*jit).<emphasis role="bold">is_interrupted()</emphasis> &lt;&lt; std::endl;
     }
     }              </programlisting>
                <para>It goes similarly with the threapool scheduler, with the slight difference
                    that we ask the Servant to deliver diagnostic information through a proxy
                    member. The <link xlink:href="examples/example_log.cpp">complete example</link>
                    shows all this, plus an interrupted job (you might have noticed in the previous
                    listing that a diagnostic offers an <code>is_interrupted</code> member).</para>
            </sect1>
            <sect1>
                <title>Queue container with priority</title>
                <para>Sometimes, all jobs posted to a scheduler do not have the same priority. For
                    threadpool schedulers, <code>composite_threadpool_scheduler</code> is an option.
                    For a single-threaded scheduler, Asynchronous does not provide a priority queue
                    but a queue container, which itself contains any number of queues, of different
                    types if needed. This has several advantages:<itemizedlist>
                        <listitem>
                            <para>Priority is defined simply by posting to the queue with the
                                desired priority, so there is no need for expensive priority
                                algorithms.</para>
                        </listitem>
                        <listitem>
                            <para>Ones gets also reduced contention if many threads of a threadpool
                                post something to the queue of a single-threaded scheduler. If no
                                priority is defined, one queue will be picked, according to a
                                configurable policy, reducing contention on a single queue.</para>
                        </listitem>
                        <listitem>
                            <para>It is possible to mix queues to get the best of each.</para>
                        </listitem>
                        <listitem>
                            <para>One can build a queue container of queue containers, etc.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Note: This applies to any scheduler. We'll start with single-threaded
                    schedulers used by managing servants for simplicity, but it is possible to have
                    composite schedulers using queue containers for finest granularity and least
                    contention.</para>
                <para>First, we need to create a single-threaded scheduler with several queues for
                    our servant to live in, for example, one threadsafe list and and lockfree
                    queues:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
                           boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                        boost::asynchronous::any_queue_container&lt;> >
                        (boost::asynchronous::any_queue_container_config&lt;boost::asynchronous::threadsafe_list&lt;> >(1),
                         boost::asynchronous::any_queue_container_config&lt;boost::asynchronous::lockfree_queue&lt;> >(3,100)
                         ));</programlisting>
                <para><code>any_queue_container</code> takes as constructor arguments a variadic
                    sequence of <code>any_queue_container_config</code>, with a queue type as
                    template argument, and in the constructor the number of objects of this queue
                    (in the above example, one <code>threadsafe_list</code> and 3
                        <code>lockfree_queue</code> instances, then the parameters that these queues
                    require in their constructor (100 is the capacity of the underlying
                        <code>boost::lockfree_queue</code>). This means, that our
                        <code>single_thread_scheduler</code> has 4 queues:<itemizedlist>
                        <listitem>
                            <para>a threadsafe_list at index 1</para>
                        </listitem>
                        <listitem>
                            <para>lockfree queues at indexes 2,3,4</para>
                        </listitem>
                        <listitem>
                            <para>>= 4 means the queue with the least priority.</para>
                        </listitem>
                        <listitem>
                            <para>0 means "any queue" and is the default</para>
                        </listitem>
                    </itemizedlist></para>
                <para>The scheduler will handle these queues as having priorities: as long as there
                    are work items in the first queue, take them, if there are no, try in the
                    second, etc. If all queues are empty, the thread gives up his time slice and
                    sleeps until some work item arrives. If no priority is defined by posting, a
                    queue will be chosen (by default randomly, but you can configure this with a
                    policy). This has the advantage of reducing contention of the queue, even when
                    not using priorities. The servant defines the priority of the tasks it provides.
                    While this might seem surprising, it is a design choice to avoid someone using a
                    servant proxy interface to think about it, as you will see in the second
                    listing. To define a priority for a servant proxy, there is a second field in
                    the macros:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s)
    {}
    <emphasis role="bold">BOOST_ASYNC_SERVANT_POST_CTOR(3)</emphasis>
    <emphasis role="bold">BOOST_ASYNC_SERVANT_POST_DTOR(4)</emphasis>
    BOOST_ASYNC_FUTURE_MEMBER(start_async_work,<emphasis role="bold">1</emphasis>)
};</programlisting>
                <para>BOOST_ASYNC_FUTURE_MEMBER and other similar macros can be given an optional
                    priority parameter, in this case 1, which is our threadsafe list. Notice how you
                    can then define the priority of the posted servant constructor and
                    destructor.</para>
                <programlisting>ServantProxy proxy(scheduler);
                boost::shared_future&lt;boost::shared_future&lt;int> > fu = proxy.start_async_work();</programlisting>
                <para>Calling our proxy member stays unchanged because the macro defines the
                    priority of the call.</para>
                <para>We also have an extended version of <code>post_callback</code>, called by a
                    servant posting work to a threadpool:</para>
                <programlisting>post_callback(
               [](){return 42;}// work
                ,
               [this](boost::shared_future&lt;int> res){}// callback functor.
               ,"",
               <emphasis role="bold">2,2</emphasis>
               );</programlisting>
                <para>Note the two added priority values: the first one for the task posted to the
                    threadpool, the second for the priority of the callback posted back to the
                    servant scheduler. The string is the log name of the task, which we choose to
                    ignore here.</para>
                <para>The priority is in any case an indication, the scheduler is free to ignore it
                    if not supported. In the <link xlink:href="examples/example_queue_container.cpp"
                        >example</link>, the single threaded scheduler will honor the request, but
                    the threadpool has a normal queue and cannot honor the request, but a threadpool
                    with an <code>any_queue_container</code> or a
                        <code>composite_threadpool_scheduler</code> can. The <link
                        xlink:href="examples/example_queue_container_log.cpp">same example</link>
                    can be rewritten to make use of the logging mechanism.</para>
                <para><code>any_queue_container</code> has two template arguments. The first, the
                    job type, is as always, a callable (<code>any_callable</code>) job. The second
                    is the policy which Asynchronous usses to find the desired queue for a job. The
                    default is <code>default_find_position</code>, which is as described above, 0
                    means any position, all other values map to a queue, priorities >= number of
                    queues means last queue. Any position is by default random
                        (<code>default_random_push_policy</code>), but you might pick
                        <code>sequential_push_policy</code>, which keeps an atomic counter and
                    always adds to the next queue.</para>
                <para>If your idea is to build a queue container of queue containers, you'll
                    probably want to provide your own policy.</para>
            </sect1>
            <sect1>
                <title>Multiqueue Schedulers' priority</title>
                <para>TODO</para>
            </sect1>
            <sect1>
                <title>Threadpool Schedulers with several queues</title>
                <para>A queue container has advantages (different queue types, priority for single
                    threaded schedulers) but also disadvantages (takes jobs from one end of the
                    queue, which means potential cache misses, more typing work). If you don't need
                    different queue types for a threadpool but want to reduce contention, multiqueue
                    schedulers are for you. A normal <code>threadpool_scheduler</code> has x threads
                    and one queue, serving them. A <code>multiqueue_threadpool_scheduler</code> has
                    x threads and x queues, each serving a worker thread. Each thread looks for work
                    in its queue. If it doesn't find any, it looks for work in the previous one,
                    etc. until it finds one or inspected all the queues. As all threads steal from
                    the previous queue, there is little contention. The construction of this
                    threadpool is very similar to the simple
                    <code>threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                // 4 threads and 4 lockfree queues of 10 capacity
                new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> >(4,10));</programlisting>
                <para>The first argument is, as usual, the number of worker threads, which is at the
                    same time the number of queues. As for every scheduler, if the queue constructor
                    takes arguments, they come next and are forwarded to the queue.</para>
                <para>This is the advised scheduler for standard cases as it offers lesser
                    contention and task stealing between the queues it uses for task
                    transfer.</para>
                <para>There is a limitation, these schedulers cannot have 0 thread like their
                    single-queue counterparts.</para>
            </sect1>
            <sect1>
                <title>Composite Threadpool Scheduler</title>
                <para>When a project becomes more complex, having a single threadpool for the whole
                    application does not offer enough flexibility in load planning. It is pretty
                    hard to avoid either oversubscription (more busy threads than available hardware
                    threads) or undersubscription. One would need one big threadpool with exactly
                    the number of threads available in the hardware. Unfortunately, if we have a
                    hardware with, say 12 hardware threads, parallelizing some work using all 12
                    might be slowlier than using only 8. One would need different threadpools of
                    different number of threads for the application. This, however, has the serious
                    drawback that there is a risk that some threadpools will be in overload, while
                    others are out of work unless we have work stealing between different
                    threadpools.</para>
                <para>The second issue is task priority. One can define priorities with several
                    queues or a queue container, but this ensures that only highest priority tasks
                    get executed if the system is coming close to overload. Ideally, it would be
                    great if we could decide how much compute power we give to each task
                    type.</para>
                <para>This is what <code>composite_threadpool_scheduler</code> solves. This pool
                    supports, like any other pool, the
                    <code>any_shared_scheduler_proxy</code>concept so you can use it in place of the
                    ones we used so far. The pool is composed of other pools
                        (<code>any_shared_scheduler_proxy</code> pools). It implements work stealing
                    between pools if a) the pools support it and b) the queue of a pool also does.
                    For example, if we define as worker of a servant inside a single-threaded
                    scheduler:</para>
                <para>
                    <programlisting>// create a composite threadpool made of:
// a multiqueue_threadpool_scheduler, 1 thread, with a lockfree_queue of capacity 100. 
// This scheduler does not steal from other schedulers, but will lend its queue for stealing
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp = boost::asynchronous::create_shared_scheduler_proxy( 
               new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> > (1,100));

// a stealing_multiqueue_threadpool_scheduler, 3 threads, each with a threadsafe_list
// this scheduler will steal from other schedulers if it can. In this case it will manage only with tp, not tp3
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp2 = boost::asynchronous::create_shared_scheduler_proxy( 
                    new boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;boost::asynchronous::threadsafe_list&lt;> > (3));

// a multiqueue_threadpool_scheduler, 4 threads, each with a lockfree_spsc_queue of capacity 100
// this works because there will be no stealing as the queue can't, and only this single-thread scheduler will be the producer
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp3 = boost::asynchronous::create_shared_scheduler_proxy( 
               new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_spsc_queue&lt;> > (4,100));

// create a composite pool made of the 3 previous ones
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp_worker =
             boost::make_shared&lt;boost::asynchronous::composite_threadpool_scheduler&lt;> > (tp,tp2,tp3);
                    </programlisting>
                </para>
                <para>We can use this pool:<itemizedlist>
                        <listitem>
                            <para>As a big worker pool. In this case, the priority argument we use
                                for posting refers to the (1-based) index of the subpool
                                (post_callback(func1,func2,"task name",<emphasis role="bold"
                                    >1</emphasis>,0);). "1" means post to the first pool. But the
                                second pool could steal the work.</para>
                        </listitem>
                        <listitem>
                            <para>As a pool container, but different parts of the code will get to
                                see only the subpools. For example, the pools tp, tp2 and tp3 can
                                still be used independently as a worker pool. Calling
                                composite_threadpool_scheduler&lt;>::get_scheduler(std::size_t
                                index_of_pool) will also give us the corresponding pool (1-based, as
                                always).</para>
                        </listitem>
                    </itemizedlist></para>
                <para>A good example of why to use this pool is if you have a threadpool for an
                    asio-based communication. Using such a pool inside the composite pool will allow
                    the threads of this pool to help (steal) other pools if they have nothing to do. </para>
                <para>Stealing is done with priority. A stealing poll first tries to steal from the
                    first pool, then from the second, etc.</para>
                <para>The <link xlink:href="examples/example_composite_threadpool.cpp">following
                        example</link> shows a complete servant implementation, and the <command
                        xlink:href="#asio_scheduler">ASIO section</command> will show how an ASIO
                    pool can steal.</para>
                <para>The threadpool schedulers we saw so far are not stealing from other pools. The
                    single-queue schedulers are not stealing, and the multiqueue schedulers steal
                    from the queues of other threads of the same pool. The stealing schedulers
                    usually indicate this by appending a <code>stealing_</code> to their name:<itemizedlist>
                        <listitem>
                            <para><code>stealing_threadpool_scheduler</code> is a
                                    <code>threadpool_scheduler</code> which steals from other
                                pools.</para>
                        </listitem>
                        <listitem>
                            <para><code>stealing_multiqueue_threadpool_scheduler</code> is a
                                    <code>multiqueue_threadpool scheduler</code> which steals from
                                other pools.</para>
                        </listitem>
                        <listitem>
                            <para><code>asio_scheduler steals</code>.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>The only difference with their not stealing equivalent is that they need a
                    composite_scheduler to tell them from which queues they can steal.</para>
                <para>Not all schedulers offer a queue to steal from. A
                        <code>single_thread_scheduler</code> does not as it would likely bring race
                    conditions in active objects. If you do want to allow stealing, use a threadpool
                    with 1 thread. An <code>asio_scheduler</code> also offers no queue to steal from
                    although it can steal from other queues because Boost.Asio does not offer this
                    in its interface. Future extensions will overcome this.</para>
                <para>TODO priority posting</para>
                <para>Another interesting usage will be when planning for extra machines to help a
                    threadpool by processing some of the work: jobs can be stolen from a threadpool
                    by a <command xlink:href="#distributing">tcp_server_scheduler</command> from which other machines can
                    get them. Just pack both pools in a <code>composite_threadpool_scheduler</code>
                    and you're ready to go.</para>
            </sect1>
            <sect1>
                <title><command xml:id="asio_scheduler"/>asio_scheduler</title>
                <para>Asynchronous supports the possibility to use Boost.Asio as a threadpool
                    provider. This has several advantages:<itemizedlist>
                        <listitem>
                            <para>asio_scheduler is delivered with a way to access Asio's io_service
                                from a servant object living inside the scheduler.</para>
                        </listitem>
                        <listitem>
                            <para>asio_scheduler handles the necessary work for creating a pool of
                                threads for multithreaded-multi-io_service communication.</para>
                        </listitem>
                        <listitem>
                            <para>asio_scheduler threads implement work-stealing from other
                                Asynchronous schedulers. This allows communication threads to help
                                other threadpools when no I/O communication is happening. This helps
                                reducing thread oversubscription.</para>
                        </listitem>
                        <listitem>
                            <para>One has all the usual goodies of Asynchronous: safe callbacks,
                                object tracking, servant proxies, etc.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Let's create a simple but powerful example to illustrate its usage. We want to
                    create a TCP client, which connects several times to the same server, gets data
                    from it (in our case, the Boost license will do), then checks if the data is
                    coherent by comparing the results two-by-two. Of course, the client has to be
                    perfectly asynchronous and never block. We also want to guarantee some threads
                    for the communication and some for the calculation work. We also want to
                    communication threads to "help" by stealing some work if necessary.</para>
                <para>Let's start by creating a TCP client using Boost.Asio. A slightly modified
                    version of the async TCP client from the Asio documentation will do. All we
                    change is pass it a callback which it will call when the requested data is
                    ready. We now pack it into an Asynchronous trackable client:</para>
                <programlisting>// Objects of this type are made to live inside an asio_scheduler,
// they get their associated io_service object from TLS
struct AsioCommunicationServant : boost::asynchronous::trackable_servant&lt;>
{
    AsioCommunicationServant(boost::asynchronous::any_weak_scheduler&lt;> scheduler,
                             const std::string&amp; server, const std::string&amp; path)
        : boost::asynchronous::trackable_servant&lt;>(scheduler)
        , m_client(*<emphasis role="bold">boost::asynchronous::get_io_service&lt;>()</emphasis>,server,path)
    {}
    void test(std::function&lt;void(std::string)> cb)
    {
        // just forward call to asio asynchronous http client
        // the only change being the (safe) callback which will be called when http get is done
        m_client.request_content(cb);
    }
private:
    client m_client; //client is from Asio example
};</programlisting>
                <para>The main noteworthy thing to notice is the call to <emphasis role="bold"
                        >boost::asynchronous::get_io_service&lt;>()</emphasis>, which, using
                    thread-local-storage, gives us the io_service associated with this thread (one
                    io_service per thread). This is needed by the Asio TCP client. Also noteworthy
                    is the argument to <code>test()</code>, a callback when the data is available. </para>
                <para>Wait a minute, is this not unsafe (called from an asio worker thread)? It is
                    but it will be made safe in a minute.</para>
                <para>We now need a proxy so that this communication servant can be safely used by
                    others, as usual:</para>
                <programlisting>class AsioCommunicationServantProxy: public boost::asynchronous::servant_proxy&lt;AsioCommunicationServantProxy,AsioCommunicationServant >
{
public:
    // ctor arguments are forwarded to AsioCommunicationServant
    template &lt;class Scheduler>
    AsioCommunicationServantProxy(Scheduler s,const std::string&amp; server, const std::string&amp; path):
        boost::asynchronous::servant_proxy&lt;AsioCommunicationServantProxy,AsioCommunicationServant >(s,server,path)
    {}
    // we offer a single member for posting
    BOOST_ASYNC_POST_MEMBER(test)
};                   </programlisting>
                <para>A single member, <code>test</code>, is used in the proxy. The constructor
                    takes the server and relative path to the desired page. We now need a manager
                    object, which will trigger the communication, wait for data, check that the data
                    is coherent:</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;> scheduler,const std::string&amp; server, const std::string&amp; path)
        : boost::asynchronous::trackable_servant&lt;>(scheduler)
        , m_check_string_count(0)
    {
        // as worker we use a simple threadpool scheduler with 4 threads (0 would also do as the asio pool steals)
        auto worker_tp = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> > (4,10));

        // for tcp communication we use an asio-based scheduler with 3 threads
        auto asio_workers = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::asio_scheduler&lt;>(3));

        // we create a composite pool whose only goal is to allow asio worker threads to steal tasks from the threadpool
        m_pools = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::composite_threadpool_scheduler&lt;> (worker_tp,asio_workers));

        set_worker(worker_tp);
        // we create one asynchronous communication manager in each thread
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
    }
... //to be continued                 
                </programlisting>
                <para>We create 3 pools:<itemizedlist>
                        <listitem>
                            <para>A worker pool for calculations (page comparisons)</para>
                        </listitem>
                        <listitem>
                            <para>An asio threadpool with 3 threads in which we create 3
                                communication objects.</para>
                        </listitem>
                        <listitem>
                            <para>A composite pool which binds both pools together into one stealing
                                unit. You could even set the worker pool to 0 thread, in which case
                                the worker will get its work done when the asio threads have nothing
                                to do. Only non- multiqueue schedulers support this. This composite
                                pool is now made to be the worker pool of this object using
                                    <code>set_worker()</code>.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>We then create our communication objects inside the asio pool.</para>
                <para><emphasis role="underline">Note</emphasis>: at the moment, asio pools can
                    steal from other pools but not be stolen from. Let's move on to the most
                    interesting part:</para>
                <programlisting>void get_data()
{
    // provide this callback (executing in our thread) to all asio servants as task result. A string will contain the page
    std::function&lt;void(std::string)> f =            
...
    m_asio_comm[0].test(make_safe_callback(f));
    m_asio_comm[1].test(make_safe_callback(f));
    m_asio_comm[2].test(make_safe_callback(f));
    }</programlisting>
                <para>We skip the body of f for the moment. f is task which will be posted to each
                    communication servant so that they can do the same work:<itemizedlist>
                        <listitem>
                            <para>call the same http get on an asio servants</para>
                        </listitem>
                        <listitem>
                            <para>at each callback, check if we got all three callbacks</para>
                        </listitem>
                        <listitem>
                            <para>if yes, post some work to worker threadpool, compare the returned
                                strings (should be all the same)</para>
                        </listitem>
                        <listitem>
                            <para>if all strings equal as they should be, cout the page</para>
                        </listitem>
                    </itemizedlist></para>
                <para>All this will be doine in a single functor. This functor is passed to each
                    communication servant, packed into a make_safe_callback, which, as its name
                    says, transforms the unsafe functor into one which posts this callback functor
                    to the manager thread and also tracks it to check if still alive at the time of
                    the callback. By calling <code>test()</code>, we trigger the 3 communications,
                    and f will be called 3 times. The body of f is:</para>
                <programlisting>std::function&lt;void(std::string)> f =
                [this](std::string s)
                {
                   this->m_requested_data.push_back(s);
                   // poor man's state machine saying we got the result of our asio requests :)
                   if (this->m_requested_data.size() == 3)
                   {
                       // ok, this has really been called for all servants, compare.
                       // but it could be long, so we will post it to threadpool
                       std::cout &lt;&lt; "got all tcp data, parallel check it's correct" &lt;&lt; std::endl;
                       std::string s1 = this->m_requested_data[0];
                       std::string s2 = this->m_requested_data[1];
                       std::string s3 = this->m_requested_data[2];
                       // this callback (executing in our thread) will be called after each comparison
                       auto cb1 = [this,s1](boost::future&lt;bool> res)
                       {
                          if (res.get())
                              ++this->m_check_string_count;
                          else
                              std::cout &lt;&lt; "uh oh, the pages do not match, data not confirmed" &lt;&lt; std::endl;
                          if (this->m_check_string_count ==2)
                          {
                              // we started 2 comparisons, so it was the last one, data confirmed
                              std::cout &lt;&lt; "data has been confirmed, here it is:" &lt;&lt; std::endl;
                              std::cout &lt;&lt; s1;
                          }
                       };
                       auto cb2=cb1;
                       // post 2 string comparison tasks, provide callback where the last step will run
                       this->post_callback([s1,s2](){return s1 == s2;},std::move(cb1));
                       this->post_callback([s2,s3](){return s2 == s3;},std::move(cb2));
                   }
                };        
                </programlisting>
                <para> We start by checking if this is the third time this functor is called (this,
                    the manager, is nicely serving as holder, kind of poor man's state machine
                    counting to 3). If yes, we prepare a call to the worker pool to compare the 3
                    returned strings 2 by 2 (cb1, cb2). Again, simple state machine, if the callback
                    is called twice, we are done comparing string 1 and 2, and 2 and 3, in which
                    case the page is confirmed and cout'ed. The last 2 lines trigger the work and
                    post to our worker pool (which is the threadpool scheduler, or, if stealing
                    happens, the asio pool) two comparison tasks and the callbacks.</para>
                <para>Our manager is now ready, we still need to create for it a proxy so that it
                    can be called from the outside world asynchronously, then create it in its own
                    thread, as usual:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s,const std::string&amp; server, const std::string&amp; path):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s,server,path)
    {}
    // get_data is posted, no future, no callback
    BOOST_ASYNC_POST_MEMBER(get_data)
};
...              
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::threadsafe_list&lt;> >);
{
   ServantProxy proxy(scheduler,"www.boost.org","/LICENSE_1_0.txt");
   // call member, as if it was from Servant
   proxy.get_data();
   // if too short, no problem, we will simply give up the tcp requests
   // this is simply to simulate a main() doing nothing but waiting for a termination request
   boost::this_thread::sleep(boost::posix_time::milliseconds(2000));
}
                </programlisting>
                <para> As usual, <link xlink:href="examples/example_asio_http_client.cpp">here the
                        complete, ready-to-use example</link> and the implementeation of the <link
                        xlink:href="examples/asio/asio_http_async_client.cpp">Boost.Asio HTTP
                        client</link>. </para>
            </sect1>
            <sect1>
                <title>Timers</title>
                <para>Very often, an Active Object servant acting as an asynchronous dispatcher will
                    post tasks which have to be done until a certain point in the future, or which
                    will start only at a later point. State machines also regularly make use of a
                    "time" event.</para>
                <para>For this we need a timer, but a safe one:<itemizedlist>
                        <listitem>
                            <para>The timer callback has to be posted to the Active Object thread to
                                avoid races.</para>
                        </listitem>
                        <listitem>
                            <para>The timer callback shall not be called in the servant making the
                                request has been deleted (it can be an awfully long time until the
                                callback).</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Asynchronous itself has no timer, but Boost.Asio has, so the library provides
                    a wrapper around it and will allow us to create a timer using an io_service
                    running in its own thread or in an asio threadpool, also provided by the
                    library.</para>
                <sect2>
                    <title>Constructing a timer</title>
                    <para>One first needs an <code>asio_scheduler</code> with at least one
                        thread:</para>
                    <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> asio_sched = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::asio_scheduler&lt;>(1));               
                    </programlisting>
                    <para>The Servant living in its ActiveObject thread then creates a timer (as
                        attribute to keep it alive as destroying the object will cancel the timer)
                        using this scheduler and a timer value:</para>
                    <programlisting> boost::asynchronous::asio_deadline_timer_proxy m_timer (asio_sched,boost::posix_time::milliseconds(1000));                   
                    </programlisting>
                    <para>It can now start the timer using <code>trackable_servant</code> (its base
                            class)<code>::async_wait</code>, passing it a functor call when timer
                        expires / is cancelled:</para>
                    <programlisting> async_wait(m_timer,
            [](const ::boost::system::error_code&amp; err)
            {
                std::cout &lt;&lt; "timer expired? "&lt;&lt; std::boolalpha &lt;&lt; (bool)err &lt;&lt; std::endl; //true if expired, false if cancelled
            } 
            );                  </programlisting>
                    <para>Canceling the timer means destroying (and possibly recreating) the timer
                        object:</para>
                    <programlisting> m_timer =  boost::asynchronous::asio_deadline_timer_proxy(get_worker(),boost::posix_time::milliseconds(1000));                                   
                    </programlisting>
                    <para>The <link xlink:href="examples/example_asio_deadline_timer.cpp">following
                            example </link> displays a servant using an asio scheduler as a thread
                        pool and creating there its timer object. Not how the timer is created using
                            <code>trackable_servant</code> (its base
                            class)<code>::get_worker()</code>.</para>
                </sect2>
            </sect1>
            <sect1>
                <title><command xml:id="continuations"/>Continuation tasks</title>
                <para>A common limitation of threadpools is support for recursive tasks: tasks start
                    other tasks, which start other tasks, until all threads in the threadpool are
                    busy waiting. At this point, one could add more threads, but threads are
                    expensive. Similarly, you might post a task which posts more tasks and wait for
                    them to complete to do a merge of the result. Of course you can achieve this
                    with a controller object or state machine in a single-threaded scheduler waiting
                    for callbacks, but for very small tasks, using callbacks might just be too
                    expensive. In such cases, Asynchronous provides continuations: a task executes,
                    does something then creates a continuation which will wake up when ready.</para>
                <para>A common example of recursive tasks is a parallel fibonacci. Usually, this
                    means a task calculating fib(n) will start a fib(n-1) and fib(n-2) and blocks
                    until both are done. These tasks will start more tasks, etc. until a cutoff
                    number, at which point recursion stops and fibonacci is calculated serially.
                    This approach has some problems: to avoid thread explosion, we would need
                    fibers, which are not available in Boost at the time of this writing, and even
                    in fibers, tasks would block, which means interrupting them is not possible,
                    which we would want to avoid. In any case, blocking simply isn't part of the
                    asynchronous philosophy of the library. Let's have a look how continuation tasks
                    let us implement a parallel fibonacci.</para>
                <para>First of all, we need a serial fibonacci to use for the cutoff. This is a
                    classical one:</para>
                <programlisting> long serial_fib( long n ) {
    if( n&lt;2 )
        return n;
    else
        return serial_fib(n-1)+serial_fib(n-2);
}                                   
                </programlisting>
                <para> We now need a recursive fibonacci task: </para>
                <programlisting>// our recursive fibonacci tasks. Needs to inherit continuation_task&lt;value type returned by this task>
struct fib_task : public boost::asynchronous::continuation_task&lt;long>
{
    fib_task(long n,long cutoff):n_(n),cutoff_(cutoff){}
    // called inside of threadpool
    void operator()()const
    {
        // the result of this task, will be either set directly if &lt; cutoff, otherwise when taks is ready
        boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();
        if (n_&lt;cutoff_)
        {
            // n &lt; cutoff => execute immediately
            task_res.set_value(serial_fib(n_));
        }
        else
        {
            // n>= cutoff, create 2 new tasks and when both are done, set our result (res(task1) + res(task2))
            boost::asynchronous::create_continuation(
                        // called when subtasks are done, set our result
                        [task_res](std::tuple&lt;boost::future&lt;long>,boost::future&lt;long> > res)
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get();
                            task_res.set_value(r);
                        },
                        // recursive tasks
                        fib_task(n_-1,cutoff_),
                        fib_task(n_-2,cutoff_));
        }
    }
    long n_;
    long cutoff_;
};                               
                </programlisting>
                <para> This deserves a bit of explanation. Our task need to inherit
                        <code>boost::asynchronous::continuation_task&lt;R></code> where R is the
                    type later returned. This class provides us with <code>this_task_result()</code>
                    where we set the task result. This is done either immediately if n &lt; cutoff
                    (first if clause), or (else clause) using a continuation.</para>
                <para>If n>= cutoff, we create a continuation task. This is a sleeping task, which
                    will get activated when all required tasks complete. In this case, we have two
                    fibonacci sub tasks. The template argument is the return type of the
                    continuation. We create two sub-tasks, for n-1 and n-2 and when they complete,
                    the completion functor passed as first argument is called.</para>
                <para>Note that <code>boost::asynchronous::create_continuation</code> is a variadic
                    function, there can be any number of sub-tasks. The completion functor takes as
                    single argument a tuple of futures, one for each subtask. The template argument
                    of the future is the template argument of
                        <code>boost::asynchronous::continuation_task</code> of each subtask. In this
                    case, all are long, but it's not a requirement.</para>
                <para>When this completion functor is called, we set our result to be result of
                    first task + result of second task and return.</para>
                <para>The main particularity of this solution is that a task does not block until
                    sub-tasks complete but instead provides an asynchronous functor.</para>
                <para>All what we still need to do is create the first task. In the tradition of
                    Asynchronous, we do it inside an asynchronous servant which posts the first task
                    and waits for a callback:</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
...
   void calc_fibonacci(long n,long cutoff)
   {
      post_callback(
            [n,cutoff]()
            {
                // a top-level continuation is the first one in a recursive serie.
                // Its result will be passed to callback
                return boost::asynchronous::top_level_continuation&lt;long>(fib_task(n,cutoff));
            }// work
            ,
            // callback with fibonacci result.
            [](boost::future&lt;long> res){...}// callback functor.
        );                                 
   }  
};          </programlisting>
                <para> We call <code>post_callback</code>, which, as usual, ensure that the callback
                    is posted to the right thread and the servant lifetime is tracked. The posted
                    task calls
                        <code>boost::asynchronous::top_level_continuation&lt;task-return-type></code>
                    to create the first, top-level continuation, passing it a first fib_task. This
                    is non-blocking, a special version of <code>post_callback</code> recognizes a
                    continuation and will call its callback (with a
                        <code>future&lt;task-return-type></code>) only when the calculation is
                    finished.</para>
                <para>As usual, calling get on the future is non-blocking, one gets eaither the
                    result or an exception if thrown by a task.</para>
                <para>Please have a look at the <link xlink:href="examples/example_fibonacci.cpp"
                        >complete example</link>.</para>
                <para>And what about logging? We don't want to give up this feature of course and
                    would like to know how long all these fib_task took to complete. This is done
                    through minor changes. As always we need a job, the same as usual:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> servant_job;                                 
                </programlisting>
                <para> We give the logged name of the task in the constructor of fib_task, for
                    example fib_task_xxx:</para>
                <programlisting>fib_task(long n,long cutoff)
        : boost::asynchronous::continuation_task&lt;long>("fib_task_" + boost::lexical_cast&lt;std::string>(n))
        ,n_(n),cutoff_(cutoff){}                                
                </programlisting>
                <para>And call <code>boost::asynchronous::create_continuation_log</code> instead of
                        <code>boost::asynchronous::create_continuation</code>:</para>
                <programlisting>boost::asynchronous::<emphasis role="bold">create_continuation_log</emphasis>&lt;servant_job>(
                        [task_res](std::tuple&lt;boost::future&lt;long>,boost::future&lt;long> > res)
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get();
                            task_res.set_value(r);
                        },
                        fib_task(n_-1,cutoff_),
                        fib_task(n_-2,cutoff_)
);                               
                </programlisting>
                <para> Inside the servant we might optionally want the version of post_callback with
                    name, and we need to use <code>top_level_continuation_log</code> instead of
                        <code>top_level_continuation</code>:</para>
                <programlisting>post_callback(
              [n,cutoff]()
              {
                   return boost::asynchronous::<emphasis role="bold">top_level_continuation_log</emphasis>&lt;long,servant_job>(fib_task(n,cutoff));
              }// work
              ,
              // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
              [this](boost::future&lt;long> res){...},// callback functor.
              <emphasis role="bold">"calc_fibonacci"</emphasis>
        );                             
                </programlisting>
                <para> The previous example has been <link
                        xlink:href="examples/example_fibonacci_log.cpp">rewritten with logs and a
                        display of all tasks</link> (beware, with higher fibonacci numbers, this can
                    become a long list).. </para>
            </sect1>
            <sect1>
                <title><command xml:id="distributing"/>Distributing work among machines</title>
                <para>At the time of this writing, a core i7-3930K with 6 cores and 3.2 GHz will
                    cost $560, so say $100 per core. Not a bad deal, so you buy it. Unfortunately,
                    some time later you realize you need more power. Ok, there is no i7 with more
                    cores and an Extreme Edition will be quite expensive for only a little more
                    power so you decide to go for a Xeon. A 12-core E5-2697v2 2.7GHz will go for
                    almost $3000 which means $250 per core, and for this you have a lesser
                    frequency. And if you need later even more power, well, it will become really
                    expensive. Can Asynchronous help us use more power for cheap, and at best, with
                    little work? It does, as you guess;-)</para>
                <para>Asynchronous provides a special pool, <code>tcp_server_scheduler</code>, which
                    will behave like any other scheduler but will not execute work itself, waiting
                    instead for clients to connect and get some work. The client execute the work on
                    behalf of the <code>tcp_server_scheduler</code> and send it back the results. </para>
                <para>For this to work, there is however a condition: jobs must be (boost)
                    serializable to be transferred to the client. So does the returned value.</para>
                <para>Let's start with a <link xlink:href="examples/example_tcp_server.cpp">simplest
                        example</link>:</para>
                <programlisting>// notice how the worker pool has a different job type
struct Servant : boost::asynchronous::trackable_servant&lt;boost::asynchronous::any_callable,<emphasis role="bold">boost::asynchronous::any_serializable</emphasis>>
{
  Servant(boost::asynchronous::any_weak_scheduler&lt;> scheduler)
        : boost::asynchronous::trackable_servant&lt;boost::asynchronous::any_callable,<emphasis role="bold">boost::asynchronous::any_serializable</emphasis>>(scheduler)
  {
        // let's build our pool step by step. First we need a worker pool
        // possibly for us, and we want to share it with the tcp pool for its serialization work
        boost::asynchronous::any_shared_scheduler_proxy&lt;> workers = boost::asynchronous::create_shared_scheduler_proxy(
            new boost::asynchronous::threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> >(3));

        // we use a tcp pool using the 3 worker threads we just built
        // our server will listen on "localhost" port 12345
        auto pool= boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">tcp_server_scheduler</emphasis>&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::<emphasis role="bold">any_serializable</emphasis>> >
                                (workers,"localhost",12345));
        // and this will be the worker pool for post_callback
        set_worker(pool);
  }
};</programlisting>
                <para>We start by creating a worker pool. The <code>tcp_server_scheduler</code> will
                    delegate to this pool all its serialization / deserialization work. For maximum
                    scalability we want this work to happen in more than one thread.</para>
                <para>Note that our job type is no more a simple callable, it must be deserializable
                    too (<emphasis role="bold"
                    >boost::asynchronous::any_serializable</emphasis>).</para>
                <para>Then we need a <code>tcp_server_scheduler</code> listening on, in this case,
                    localhost, port 12345. We now have a functioning worker pool and choose to use
                    it as our worker pool so that we do not execute jobs ourselves (other
                    configurations will be shown soon). Let's exercise our new pool. We first need a
                    task to be executed remotely:</para>
                <programlisting>struct dummy_tcp_task : public boost::asynchronous::<emphasis role="bold">serializable_task</emphasis>
{
    dummy_tcp_task(int d):boost::asynchronous::<emphasis role="bold">serializable_task</emphasis>("dummy_tcp_task"),m_data(d){}
    template &lt;class Archive>
    void <emphasis role="bold">serialize</emphasis>(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; m_data;
    }
    int operator()()const
    {
        std::cout &lt;&lt; "dummy_tcp_task operator(): " &lt;&lt; m_data &lt;&lt; std::endl;
        boost::this_thread::sleep(boost::posix_time::milliseconds(2000));
        std::cout &lt;&lt; "dummy_tcp_task operator() finished" &lt;&lt; std::endl;
        return m_data;
    }
    int m_data;
};</programlisting>
                <para>This is a minimum task, only sleeping. All it needs is a
                        <code>serialize</code> member to play nice with Boost.Serialization and it
                    must inherit <code>serializable_task</code>. Let's post to our TCP worker pool
                    some of the tasks, wait for a client to pick them and use the results:</para>
                <programlisting>// start long tasks in threadpool (first lambda) and callback in our thread
for (int i =0 ;i &lt; 10 ; ++i)
{
    std::cout &lt;&lt; "call post_callback with i: " &lt;&lt; i &lt;&lt; std::endl;
    post_callback(
           dummy_tcp_task(i),
           // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
           [this](boost::future&lt;int> res){
                  try{
                        this->on_callback(res.get());
                  }
                  catch(std::exception&amp; e)
                  {
                       std::cout &lt;&lt; "got exception: " &lt;&lt; e.what() &lt;&lt; std::endl;
                       this->on_callback(0);
                  }
            }// callback functor.
    );
}</programlisting>
                <para>We post 10 tasks to the pool. For each task we will get, at some later
                    undefined point (provided some clients are around), a result in form of a
                    (ready) future, possibly an exception if one was thrown by the task.</para>
                <para>Notice it is safe to use <code>this</code> in the callback lambda as it will
                    be only called if the servant still exists.</para>
                <para>We still need a client to execute the task, this is pretty straightforward (we
                    will extend it soon):</para>
                <programlisting>int main(int argc, char* argv[])
{
    std::string server_address = (argc>1) ? argv[1]:"localhost";
    std::string server_port = (argc>2) ? argv[2]:"12346";
    int threads = (argc>3) ? strtol(argv[3],0,0) : 4;
    cout &lt;&lt; "Starting connecting to " &lt;&lt; server_address &lt;&lt; " port " &lt;&lt; server_port &lt;&lt; " with " &lt;&lt; threads &lt;&lt; " threads" &lt;&lt; endl;

    auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                new boost::asynchronous::asio_scheduler&lt;>);
    {
        std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> 
        executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="dummy_tcp_task")
            {
                dummy_tcp_task t(0);
                boost::asynchronous::tcp::<emphasis role="bold">deserialize_and_call_task</emphasis>(t,resp,when_done);
            }
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };

        auto pool = boost::asynchronous::create_shared_scheduler_proxy(
                        new boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable> >(threads));
        boost::asynchronous::tcp::<emphasis role="bold">simple_tcp_client_proxy proxy</emphasis>(scheduler,pool,server_address,server_port,executor,
                                                                    0/*ms between calls to server*/);
        boost::future&lt;boost::future&lt;void> > fu = proxy.run();
        boost::future&lt;void> fu_end = fu.get();
        fu_end.get();
    }
    return 0;
}</programlisting>
                <para>We start by taking as command-line arguments the server address and port and
                    the number of threads the client will use to process stolen jobs from the
                    server. </para>
                <para>We create a single-threaded <code>asio_scheduler</code> for the communication
                    (in our case, this is sufficient, your case might vary) to the server.</para>
                <para>The client then defines an executor function. This function will be called
                    when a job is stolen by the client. As Asynchronous does not know what the job
                    is, you will need to "help" by creating an instance of the task using its name.
                    Calling <code>deserialize_and_call_task</code> will, well, deserialize the task
                    data into our dummy task, then call it. We also choose to return an exception is
                    the task is not known to us.</para>
                <para>Next, we need a pool of threads to execute the work. Usually, you will want
                    more than one thread. Remember, we want to use our several 6-core-i7,
                    right?</para>
                <para>The simplest client that Asynchronous offers is a
                        <code>simple_tcp_client_proxy</code> proxy. We say simple, because it is
                    only a client. Later on, we will see a more powerful tool.
                        <code>simple_tcp_client_proxy</code> will require the asio pool for
                    communication, the server address and port, our executor and a parameter telling
                    it how often it should try to steal a job from a server.</para>
                <para>We are now done, the client will run until killed.</para>
                <para>Let's sum up what we got in these few lines of code:<itemizedlist>
                        <listitem>
                            <para>a pool behaving like any other pool, which can be stolen
                                from</para>
                        </listitem>
                        <listitem>
                            <para>a server which does no work itself, but still scales well as
                                serialization is using whatever threads it is given</para>
                        </listitem>
                        <listitem>
                            <para>a trackable servant working with <code>post_callback</code>, like
                                always</para>
                        </listitem>
                        <listitem>
                            <para>a multithreaded client, which can be tuned precisely to use a
                                given pool for the communication and another (or the same btw.) for
                                work processing.</para>
                        </listitem>
                </itemizedlist></para>
                <para>Interestingly, we have a very versatile client. It is possible to reuse the
                    work processing and communication pools, within the same client application, for
                    a different <code>simple_tcp_client_proxy</code> which would be connecting to another
                    server.</para>
                <para>The server is also quite flexible. It scales well and can handle as many
                    clients as you wish.</para>
                <para>This is only the beginning of our distributed chapter. Let's now revisit our
                    Fibonacci example to make it distributed too.</para>
                <sect2>
                    <title>A distributed, parallel Fibonacci</title>
                    <para>Lets's revisit our parallel Fibonacci example. We realize that with higher
                        Fibonacci numbers, our CPU power doesn't suffice any more. We want to
                        distribute it among several machines while our machine still does some
                        calculazion work. To do this, we'll start with our previous example, and
                        rewrite our Fibonacci task to make it distributable.</para>
                    <para>We remember that we first had to call
                            <code>boost::asynchronous::top_level_continuation</code> in our
                        post_callback to make Asynchronous aware of the later return value. The
                        difference now is that even this one-liner lambda could be serialized and
                        sent away, wo we need to make it a <code>serializable_task</code>:</para>
                    <programlisting>struct serializable_fib_task : public boost::asynchronous::<emphasis role="bold">serializable_task</emphasis>
{
    serializable_fib_task(long n,long cutoff):boost::asynchronous::<emphasis role="bold">serializable_task("serializable_fib_task")</emphasis>,n_(n),cutoff_(cutoff){}
    template &lt;class Archive>
    <emphasis role="bold">void serialize(Archive &amp; ar, const unsigned int /*version*/)</emphasis>
    {
        ar &amp; n_;
        ar &amp; cutoff_;
    }
    auto operator()()const
        -> decltype(boost::asynchronous::top_level_continuation_log&lt;long,boost::asynchronous::any_serializable>
                    (tcp_example::fib_task(long(0),long(0))))
    {
        auto cont =  boost::asynchronous::top_level_continuation_job&lt;long,boost::asynchronous::any_serializable>
                (tcp_example::fib_task(n_,cutoff_));
        return cont;
    }
    long n_;
    long cutoff_;
};</programlisting>
                <para>We need to make our task serializable and give it a name so that the client
                        application can recognize it. We also need a serialize member, as required
                        by Boost.Serialization. And we need an operator() so that the task can be
                        executed. There is in C++11 an ugly decltype, but C++14 will solve this if
                        your compiler supports it. We also need a few changes in our Fibonacci
                        task:</para>
                    <programlisting>// our recursive fibonacci tasks. Needs to inherit continuation_task&lt;value type returned by this task>
struct fib_task : public boost::asynchronous::continuation_task&lt;long>
                <emphasis role="bold">, public boost::asynchronous::serializable_task</emphasis>
{
    fib_task(long n,long cutoff)
        :  boost::asynchronous::continuation_task&lt;long>()
        <emphasis role="bold">, boost::asynchronous::serializable_task("serializable_sub_fib_task")</emphasis>
        ,n_(n),cutoff_(cutoff)
    {
    }
    <emphasis role="bold">template &lt;class Archive>
    void save(Archive &amp; ar, const unsigned int /*version*/)const
    {
        ar &amp; n_;
        ar &amp; cutoff_;
    }
    template &lt;class Archive>
    void load(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; n_;
        ar &amp; cutoff_;
    }
    BOOST_SERIALIZATION_SPLIT_MEMBER()</emphasis>
    void operator()()const
    {
        // the result of this task, will be either set directly if &lt; cutoff, otherwise when taks is ready
        boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();
        if (n_&lt;cutoff_)
        {
            // n &lt; cutoff => execute ourselves
            task_res.set_value(serial_fib(n_));
        }
        else
        {
            // n> cutoff, create 2 new tasks and when both are done, set our result (res(task1) + res(task2))
            boost::asynchronous::create_continuation_log&lt;boost::asynchronous::any_serializable>(
                        // called when subtasks are done, set our result
                        [task_res](std::tuple&lt;boost::future&lt;long>,boost::future&lt;long> > res)
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get();
                            task_res.set_value(r);
                        },
                        // recursive tasks
                        fib_task(n_-1,cutoff_),
                        fib_task(n_-2,cutoff_));
        }
    }
    long n_;
    long cutoff_;
};</programlisting>
                <para> The few changes are highlighted. It needs to be a serializable task with its
                        own name in the constructor, and it needs serialization members. That's it,
                        we're ready to distribute!</para>
                    <para>As we previously said, we will reuse our previous TCP example, using
                            <code>serializable_fib_task</code> as the main posted task. This gives
                        us <link xlink:href="examples/example_tcp_server_fib.cpp">this example</link>.</para>
                    <para>But wait, we promised that out server would itself do some calculation
                        work, and we use as worker pool only a <code>tcp_server_scheduler</code>.
                        Right, let's do it now, throwing in a few more goodies. We need a worker
                        pool, with as many threads as we are willing to offer:</para>
                    <programlisting>// we need a pool where the tasks execute
auto <emphasis role="bold">pool</emphasis> = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">threadpool_scheduler</emphasis>&lt;
                    boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable> >(<emphasis role="bold">threads</emphasis>));</programlisting>
                 <para>This pool will get the fibonacci top-level task we will post, then, if our
                        clients connect after we start, it will get the first sub-tasks. </para>
                    <para>To make it more interesting, let's offer our server to also be a job
                        client. This way, we can build a cooperation network: the server offers
                        fibonacci tasks, but also tries to steal some, thus increasing homogenous
                        work distribution. We'll talk more about this in the next chapter.</para>
                    <programlisting>// a client will steal jobs in this pool
auto cscheduler = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::<emphasis role="bold">asio_scheduler</emphasis>&lt;>);
// jobs we will support
std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,
                   std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="serializable_sub_fib_task")
            {
                tcp_example::fib_task fib(0,0);
                boost::asynchronous::tcp::<emphasis role="bold">deserialize_and_call_continuation_task</emphasis>(fib,resp,when_done);
            }
            else if (task_name=="serializable_fib_task")
            {
                tcp_example::serializable_fib_task fib(0,0);
                boost::asynchronous::tcp::<emphasis role="bold">deserialize_and_call_top_level_continuation_task</emphasis>(fib,resp,when_done);
            }
            // else whatever functor we support
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };
boost::asynchronous::tcp::simple_tcp_client_proxy client_proxy(cscheduler,pool,server_address,server_port,executor,
                                                               10/*ms between calls to server*/);</programlisting>
                <para>Notice how we use our worker pool for job serialization / deserialization.
                        Notice also how we check both possible stolen jobs.</para>
                    <para>We also introduce two new deserialization functions.
                            boost::asynchronous::tcp::<emphasis role="bold"
                            >deserialize_and_call_task</emphasis> was used for normal tasks, we now
                        have boost::asynchronous::tcp::<emphasis role="bold"
                            >deserialize_and_call_top_level_continuation_task</emphasis> for our
                        top-level continuation task, and boost::asynchronous::tcp::<emphasis
                            role="bold">deserialize_and_call_continuation_task</emphasis> for the
                        continuation-sub-task.</para>
                    <para>We now need to build our TCP server, which we decide will get only one
                        thread for job serialization. This ought to be enough, Fibonacci tasks have
                        little data (2 long).</para>
                    <programlisting>// we need a server
// we use a tcp pool using 1 worker
auto server_pool = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;> >(<emphasis role="bold">1</emphasis>));

auto tcp_server= boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">tcp_server_scheduler</emphasis>&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable>,
                            boost::asynchronous::any_callable,true>
                                (server_pool,own_server_address,(unsigned int)own_server_port));</programlisting>
                <para>We have a TCP server pool, as before, even a client to steal jobs ourselves,
                        but how do we get ourselves this combined pool, which executes some work or
                        gives some away? </para>
                    <para>Wait a minute, combined pool? Yes, a
                            <code>composite_threadpool_scheduler</code> will do the trick. As we're
                        at it, we create a servant to coordinate the work, as we now always
                        do:</para>
                    <programlisting>// we need a composite for stealing
auto composite = boost::asynchronous::create_shared_scheduler_proxy
                (new boost::asynchronous::<emphasis role="bold">composite_threadpool_scheduler</emphasis>&lt;boost::asynchronous::any_serializable>
                          (<emphasis role="bold">pool</emphasis>,<emphasis role="bold">tcp_server</emphasis>));

// a single-threaded world, where Servant will live.
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::lockfree_queue&lt;> >);
{
      ServantProxy proxy(scheduler,<emphasis role="bold">pool</emphasis>);
      // result of BOOST_ASYNC_FUTURE_MEMBER is a shared_future,
      // so we have a shared_future of a shared_future(result of start_async_work)
      boost::shared_future&lt;boost::shared_future&lt;long> > fu = proxy.calc_fibonacci(fibo_val,cutoff);
      boost::shared_future&lt;long> resfu = fu.get();
      long res = resfu.get();
}</programlisting>                    
                <para>Notice how we give only the worker "pool" to the servant. This means, the
                        servant will post the top-level task to it, which means it will immediately
                        be called and create 2 Fibonacci tasks, which will create each one 2 more,
                        etc. until at some point a client connects and steals one, which will create
                        2 more, etc.</para>
                    <para>The client will not steal directly from this pool, it will steal from the
                            <code>tcp_server</code> pool, which, as long as a client request comes,
                        will steal from the worker pool, as they belong to the same composite. This
                        will continue until the composite is destroyed, or the work is done. For the
                        sake of the example, we do not give the composite as the Servant's worker
                        pool but keep it alive until the end of calculation. Please have a look at
                        the <link xlink:href="examples/example_tcp_server_fib2.cpp">complete example</link>.</para>
                    <para>In this example, we start taking care of homogenous work distribution by
                        packing a client and a server in the same application. But we need a bit
                        more: our last client would steal jobs so fast, every 10ms that it would
                        starve the server or other potential client applications, so we're going to
                        tell it to only steal if its work queues goes under a certain amount, which
                        we weill empirically determine, according to our hardware, network speed,
                        etc.</para>
                    <programlisting>int main(int argc, char* argv[])
{
    std::string server_address = (argc>1) ? argv[1]:"localhost";
    std::string server_port = (argc>2) ? argv[2]:"12346";
    int threads = (argc>3) ? strtol(argv[3],0,0) : 4;
    // 1..n => check at regular time intervals if the queue is under the given size
    int job_getting_policy = (argc>4) ? strtol(argv[4],0,0):0;
    cout &lt;&lt; "Starting connecting to " &lt;&lt; server_address &lt;&lt; " port " &lt;&lt; server_port &lt;&lt; " with " &lt;&lt; threads &lt;&lt; " threads" &lt;&lt; endl;

    auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                new boost::asynchronous::asio_scheduler&lt;>);
    {
        std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> 
        executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="serializable_fib_task")
            {
                tcp_example::serializable_fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_top_level_continuation_task(fib,resp,when_done);
            }
            else if (task_name=="serializable_sub_fib_task")
            {
                tcp_example::fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_continuation_task(fib,resp,when_done);
            }
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };

        // guarded_deque supports queue size
        auto pool = boost::asynchronous::create_shared_scheduler_proxy(
                        new boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::<emphasis role="bold">guarded_deque</emphasis>&lt;boost::asynchronous::any_serializable> >(threads));
        // more advanced policy
        // or <emphasis role="bold">simple_tcp_client_proxy&lt;boost::asynchronous::tcp::queue_size_check_policy&lt;>></emphasis> if your compiler can (clang)
        typename boost::asynchronous::tcp::<emphasis role="bold">get_correct_simple_tcp_client_proxy</emphasis>&lt;boost::asynchronous::tcp::queue_size_check_policy&lt;>>::type proxy(
                        scheduler,pool,server_address,server_port,executor,
                        0/*ms between calls to server*/,
                        <emphasis role="bold">job_getting_policy /* number of jobs we try to keep in queue */</emphasis>);
        // run forever
        boost::future&lt;boost::future&lt;void> > fu = proxy.run();
        boost::future&lt;void> fu_end = fu.get();
        fu_end.get();
    }
    return 0;
}</programlisting>                    
                <para>The important new part is highlighted. <code>simple_tcp_client_proxy</code>
                        gets an extra template argument, <code>queue_size_check_policy</code>, and a
                        new constructor argument, the number of jobs in the queue, under which the
                        client will try, every 10ms, to steal a job. Normally, that would be all,
                        but g++ (up to 4.7 at least) is uncooperative and requires an extra level of
                        indirection to get the desired client proxy. Otherwise, there is no
                        change.</para>
                    <para>You will find in the <link xlink:href="examples/simple_tcp_client.cpp"
                            >complete example</link> a few other tasks which we will explain
                        shortly.</para>
                    <para>Let's stop a minute to think about what we just did. We built, with little
                        code, a complete framework for distributing tasks homogenously among
                        machines, by reusing standard component offered by the library: threadpools,
                        composite pools, clients, servers. If we really have client connecting or
                        not is secondary, all what can happen is that calculating our Fibonacci
                        number will last a little longer.</para>
                    <para>We also separate the task (Fibonacci) from the threadpool configuration,
                        from the network configuration, and from the control of the task (Servant),
                        leading us to highly reusable, extendable code.</para>
                    <para>In the next chapter, we will add a way to further distribute work among
                        not only machines, but whole networks. </para>
                </sect2>                
                <sect2>
                    <title>Example: a hierarchical network</title>
                    <para>We already distribute and parallelize work, so we can scale a great deal,
                        but our current model is one server, many clients, which means a potentially
                        high network load and a lesser scalability as more and more clients connect
                        to a server. What we want is a client/server combo application  where the
                        client steals and executes jobs and a server component of the same
                        application which steals jobs from the client on behalf of other clients.
                        What we want is to achieve something like this:</para>
                    <para><inlinemediaobject>
                            <imageobject>
                                <imagedata fileref="pics/TCPHierarchical.jpg"/>
                            </imageobject>
                    </inlinemediaobject></para>
                 <para>We have our server application, as seen until now, called interestingly
                        ServerApplication on a machine called MainJobServer. This machine executes
                        works and offers at the same time a steal-from capability. We also have a
                        simple client called ClientApplication running on ClientMachine1, which
                        steals jobs and executes them itself without further delegating. We have
                        another client machine called ClientMachine2 on which
                        ClientServerApplication runs. This applications has two parts, a client
                        stealing jobs like ClientApplication and a server part stealing jobs from
                        the client part upon request. For example, another simple ClientApplication
                        running on ClientMachine2.1 connects to it and steals further jobs in case
                        ClientMachine2 is not executing them fast enough, or if ClientMachine2 is
                        only seen as a pass-through to move jobs execution to another network.
                        Sounds scalable. How hard is it to build? Not so hard, because in fact, we
                        already saw all we need to build this, so it's kind of a Lego game.</para>
                    <programlisting>int main(int argc, char* argv[])
{
    std::string server_address = (argc>1) ? argv[1]:"localhost";
    std::string server_port = (argc>2) ? argv[2]:"12345";
    std::string own_server_address = (argc>3) ? argv[3]:"localhost";
    long own_server_port = (argc>4) ? strtol(argv[4],0,0):12346;
    int threads = (argc>5) ? strtol(argv[5],0,0) : 4;
    cout &lt;&lt; "Starting connecting to " &lt;&lt; server_address &lt;&lt; " port " &lt;&lt; server_port
         &lt;&lt; " listening on " &lt;&lt; own_server_address &lt;&lt; " port " &lt;&lt; own_server_port &lt;&lt; " with " &lt;&lt; threads &lt;&lt; " threads" &lt;&lt; endl;

// to be continued</programlisting>  
                <para>We take as arguments the address and port of the server we are going to steal
                        from, then our own address and port. We now need a client with its
                        communication asio scheduler and its threadpool for job execution.</para>  
                    <programlisting>auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::<emphasis role="bold">asio_scheduler</emphasis>&lt;>);
    { //block start
        std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,
                           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="serializable_fib_task")
            {
                tcp_example::serializable_fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_top_level_continuation_task(fib,resp,when_done);
            }
            else if (task_name=="serializable_sub_fib_task")
            {
                tcp_example::fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_continuation_task(fib,resp,when_done);
            }
            // else whatever functor we support
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };
        // create pools
        // we need a pool where the tasks execute
        auto pool = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">threadpool_scheduler</emphasis>&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable> >(<emphasis role="bold">threads</emphasis>));
        boost::asynchronous::tcp::<emphasis role="bold">simple_tcp_client_proxy client_proxy</emphasis>(scheduler,pool,server_address,server_port,executor,
                                                                       10/*ms between calls to server*/);
// to be continued</programlisting>   
                <para>We now need a server to which more clients will connect, and a composite
                binding it to our worker pool:</para>   
                    <programlisting>   // we need a server
   // we use a tcp pool using 1 worker
   auto server_pool = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;> >(1));
   auto tcp_server= boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">tcp_server_scheduler</emphasis>&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable>,
                            boost::asynchronous::any_callable,true>
                                (server_pool,own_server_address,(unsigned int)own_server_port));
   // we need a composite for stealing
   auto composite = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::<emphasis role="bold">composite_threadpool_scheduler</emphasis>&lt;boost::asynchronous::any_serializable>
                                                                   (<emphasis role="bold">pool</emphasis>,<emphasis role="bold">tcp_server</emphasis>));

   boost::future&lt;boost::future&lt;void> > fu = client_proxy.run();
   boost::future&lt;void> fu_end = fu.get();
   fu_end.get();
} //end block

 return 0;
 } //end main</programlisting>   
                <para>And we're done! The client part will steal jobs and execute them, while the
                        server part, bound to the client pool, will steal on sub-client-demand.
                        Please have a look at the <link xlink:href="examples/tcp_client_server.cpp">
                            complete code</link>.</para>                    
                </sect2>
            </sect1>
            <sect1>
                <title>Picking your archive</title>
                <para>By default, Asynchronous uses a Boost Text archive (text_oarchive,
                    text_iarchive), which is simple and efficient enough for our Fibonacci example,
                    but inefficient for tasks holding more data.</para>
                <para>Asynchronous supports any archive task, requires however a different job type
                    for this. At the moment, we can use a
                        <code>portable_binary_oarchive</code>/<code>portable_binary_iarchive</code>
                    by selecting <code>any_bin_serializable</code> as job. If Boost supports more
                    archive types, support is easy to add.</para>
                <para>The previous Fibonacci server example has been <link
                        xlink:href="examples/tcp_server_fib2_bin.cpp">rewritten</link> to use this
                    capability. The <link xlink:href="examples/simple_tcp_client_bin_archive.cpp">client</link> has also been rewritten using this new job type.</para>
            </sect1>
            <sect1>
                <title><command xml:id="parallel_algos"/>Parallel Algorithms (Christophe Henry / Tobias Holl)</title>
                <para>Asynchronous supports out of the box some asynchronous parallel algorithms,
                    with more to come, as well as interesting combination usages. These algorithms
                    are continuation-based for simpler usage, often faster callback-based ones will
                    be added later. All these algorithms also support distributed calculations as
                    long as the user-provided functors are (meaning they must be
                    serializable).</para>
                <para>What is the point of adding yet another set of parallel algorithms which can
                    be found elsewhere? Because truly asynchronous algorithms are hard to find. By
                    this we mean non-blocking. If one needs parallel algorithms, it's because they
                    could need long to complete. And if they take long, we really do not want to
                    block until it happens.</para>
                <para>All of the algorithms are made for use in a worker threadpool. They are the
                    work part of a <code>post_callback</code>;</para>
                <para>In the philosophy of Asynchronous, the programmer knows better the task size
                    where he wants to start parallelizing, so all these algorithms take a cutoff.
                    Work is cut into packets of this size.</para>
                <para>All range algorithms also have a version taking a continuation as range
                    argument. This allows to combine algorithms functional way. allowing this (more
                    to come):</para>
                <programlisting>return boost::asynchronous::parallel_for(boost::asynchronous::parallel_for(boost::asynchronous::parallel_for(...)));</programlisting>
                <sect2>
                    <title>parallel_for</title>
                    <para>There are four versions of this algorithm:</para>
                    <programlisting>template &lt;class Iterator, class Func, class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;<emphasis role="bold">void</emphasis>,Job>
parallel_for(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The parallel_for version taking iterators requires that the iterators stay
                        valid until completion. It is the programmer's job to ensure this.</para>
                    <para>The third argument is the predicate applied on each element of the
                        algorithm.</para>
                    <para>The fourth argument is the cutoff, meaning in this case the max. number of
                        elements of the input range in a task.</para>
                    <para>The optional fifth argument is the name of the tasks used for
                        logging.</para>
                    <para>The optional sixth argument is the priority of the tasks in the
                        pool.</para>
                    <para>The return value is a void continuation containing either nothing or an
                        exception if one was thrown from one of the tasks.</para>
                    <para>Example:</para>
                    <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
...
    void start_async_work()
    {
        // start long tasks in threadpool (first lambda) and callback in our thread
        post_callback(
               [this](){
                        <emphasis role="bold">return</emphasis> boost::asynchronous::<emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">this->m_data.begin(),this->m_data.end()</emphasis>,
                                                                 [](int const&amp; i)
                                                                 {
                                                                    const_cast&lt;int&amp;>(i) += 2;
                                                                 },1500);
                      },// work
               // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
               [](boost::future&lt;<emphasis role="bold">void</emphasis>> /*res*/){
                            ...
               }// callback functor.
        );
    }
    std::vector&lt;int> m_data;
};</programlisting>
                <para>The most important parts are highlighted. Do not forget the return as we are
                        returning a continuation and we do not want the lambda to be interpreted as
                        a void lambda. The caller has responsibility of the input data, given in the
                        form of iterators. We use a non-legal modifying functor for the sake of the
                        example.</para>
                    <para>The call will do following:<itemizedlist>
                            <listitem>
                                <para>start tasks in the current worker pool of max 1500 elements of
                                    the input data</para>
                            </listitem>
                            <listitem>
                                <para>add 2 to each element in parallel</para>
                            </listitem>
                            <listitem>
                                <para>return a continuation</para>
                            </listitem>
                            <listitem>
                                <para>Execute the callback lambda when all tasks complete. The
                                    future will be either set or contain an exception</para>
                            </listitem>
                    </itemizedlist></para>
                    <para>Please have a look at <link xlink:href="examples/example_parallel_for.cpp">the complete example</link>.</para>
                    <para>The second version is very similar and takes a range per reference. Again,
                        the range has to stay valid during the call. As previously, the return value
                        is a void continuation.</para>
                    <programlisting>template &lt;class Range, class Func, class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;<emphasis role="bold">void</emphasis>,Job>
parallel_for(Range const&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                <para>The third version takes a range per rvalue reference. This is signal given to
                        Asynchronous that it must take ownership of the range. The return value is
                        then a continuation of the given range type:</para>
                <programlisting>template &lt;class Range, class Func, class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
parallel_for(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                <para>A <code>post_callback</code> will therefore get a future&lt;new range>, for
                example:</para>
                    <programlisting>post_callback(
    []()
    {
       std::vector&lt;int> data;
       return boost::asynchronous::parallel_for(std::move(data),
                                                      [](int const&amp; i)
                                                      {
                                                        const_cast&lt;int&amp;>(i) += 2;
                                                      },1500);
    },
    ](<emphasis role="bold">boost::future&lt;std::vector&lt;int>></emphasis> ){}
);</programlisting>
                <para>In this case, the programmer does not need to ensure the container stays
                        valid, Asynchronous takes care of it.</para>
                    <para>The fourth version of this algorithm takes a range continuation instead of
                        a range as argument and will be invoked after the continuation is
                        ready.</para>
                    <programlisting>// version taking a continuation of a range as first argument
template &lt;class Range, class Func, class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;<emphasis role="bold">typename Range::return_type</emphasis>,Job>
parallel_for(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                <para>This version allows chaining parallel calls. For example, it is now possible
                to write:</para>
                    <programlisting>post_callback(
    []()
    {
       std::vector&lt;int> data;
       return boost::asynchronous::<emphasis role="bold">parallel_for</emphasis>(boost::asynchronous::<emphasis role="bold">parallel_for</emphasis>(boost::asynchronous::<emphasis role="bold">parallel_for</emphasis>(
                                                                 // executed first
                                                                 std::move(data),
                                                                 [](int const&amp; i)
                                                                 {
                                                                    const_cast&lt;int&amp;>(i) += 2;
                                                                 },1500),
                                                              // executed second
                                                              [](int const&amp; i)
                                                              {
                                                                const_cast&lt;int&amp;>(i) += 2;
                                                              },1500),
                                                             // executed third
                                                             [](int const&amp; i)
                                                             {
                                                               const_cast&lt;int&amp;>(i) += 2;
                                                             },1500);
    },
    ](<emphasis role="bold">boost::future&lt;std::vector&lt;int>></emphasis> ){} // callback
);</programlisting>
                <para>This code will be executed as follows:<itemizedlist>
                            <listitem>
                                <para>the most inner parallel_for (parallel execution)</para>
                            </listitem>
                            <listitem>
                                <para>A kind of synchronization point will be done at this point
                                    untl the parallel_for completes</para>
                            </listitem>
                            <listitem>
                                <para>the middle parallel_for will be executed (parallel
                                    execution)</para>
                            </listitem>
                            <listitem>
                                <para>A kind of synchronization point will be done at this point
                                    untl the parallel_for completes</para>
                            </listitem>
                            <listitem>
                                <para>the outer parallel_for will be executed (parallel
                                    execution)</para>
                            </listitem>
                            <listitem>
                                <para>A kind of synchronization point will be done at this point
                                    untl the parallel_for completes</para>
                            </listitem>
                            <listitem>
                                <para>The callback will be called</para>
                            </listitem>
                </itemizedlist></para>
                <para>With "kind of synchronization point", we mean there will be no blocking
                        synchronization, it will just be waited until completion.</para>
                    <para>Finally, we also promised some distributed support, so here it is. We
                        need, as with our Fibonacci example, a serializable sub-task which will be
                        created as often as required by our cutoff and which will handle a part of
                        our range:</para>
                    <programlisting>struct dummy_parallel_for_subtask : public boost::asynchronous::serializable_task
{
    dummy_parallel_for_subtask(int d=0):boost::asynchronous::serializable_task("dummy_parallel_for_subtask"),m_data(d){}
    template &lt;class Archive>
    void <emphasis role="bold">serialize</emphasis>(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; m_data;
    }
    void operator()(int const&amp; i)const
    {
        const_cast&lt;int&amp;>(i) += m_data;
    }
    // some data, so we have something to serialize
    int m_data;
};</programlisting>
                  <para>As always we need a serializable top-level task, creating sub-tasks:</para>
                    <programlisting>struct dummy_parallel_for_task : public boost::asynchronous::serializable_task
{
    dummy_parallel_for_task():boost::asynchronous::serializable_task("dummy_parallel_for_task"),m_data(1000000,1){}
    template &lt;class Archive>
    void <emphasis role="bold">serialize</emphasis>(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; m_data;
    }
    auto operator()() -> decltype(boost::asynchronous::parallel_for&lt;std::vector&lt;int>,dummy_parallel_for_subtask,boost::asynchronous::any_serializable>(
                                      std::move(std::vector&lt;int>()),
                                      dummy_parallel_for_subtask(2),
                                      10))
    {
        <emphasis role="bold">return boost::asynchronous::parallel_for</emphasis>
                &lt;std::vector&lt;int>,<emphasis role="bold">dummy_parallel_for_subtask</emphasis>,boost::asynchronous::any_serializable>(
            std::move(m_data),
            dummy_parallel_for_subtask(2),
            10);
    }
    std::vector&lt;int> m_data;
};</programlisting>
                    <para>We now need to post our top-level task inside a servant:</para>
                    <programlisting>post_callback(
               dummy_parallel_for_task(),
               // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
               [this](boost::future&lt;std::vector&lt;int>> res){
                 try
                 {
                    // do something
                 }
                 catch(std::exception&amp; e)
                 {
                    std::cout &lt;&lt; "got exception: " &lt;&lt; e.what() &lt;&lt; std::endl;
                 }
              }// end of callback functor.
              );</programlisting>
                    <para>Please have a look at the <link xlink:href="examples/example_parallel_for_tcp.cpp">complete server example</link>.</para>
                </sect2>
                <sect2>
                    <title>parallel_reduce</title>
                    <para>Like parallel_for, there are four versions of this algorithm, with the
                        same lifetime behaviour. parallel_reduce applies a predicate to all elements
                        of a range, accumulating the result.</para>
                    <programlisting>template &lt;class Iterator, class Func, class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;<emphasis role="bold">decltype(func(std::declval&lt;typename Iterator::value_type>(), std::declval&lt;typename Iterator::value_type>()))</emphasis>,Job>
<emphasis role="bold">parallel_reduce</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range, class Func, class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;<emphasis role="bold">decltype(func(*(range.begin()), *(range.end())))</emphasis>,Job>
<emphasis role="bold">parallel_reduce</emphasis>(Range const&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range, class Func, class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;<emphasis role="bold">decltype(func(*(range.begin()), *(range.end())))</emphasis>,Job>
<emphasis role="bold">parallel_reduce</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Range, class Func, class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;<emphasis role="bold">decltype(func(std::declval&lt;typename Range::return_type::value_type>(), std::declval&lt;typename Range::return_type::value_type>()))</emphasis>,Job>
<emphasis role="bold">parallel_reduce</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                <para>Don't be worried about the return type. To keep it short, what we get is a
                        continuation of the type returned by the given predicate, for example, using
                        the iterator version:</para>
                    <para>
                        <programlisting>std::vector&lt;int> data;
post_callback(
    [this]()
    {
       return boost::asynchronous::<emphasis role="bold">parallel_reduce</emphasis>(this->data.begin(),this->data.end(),
                                                     [](int const&amp; a, int const&amp; b)
                                                     {
                                                         return a + b; // returns an int
                                                     },
                                                     1500);
    },
    ](<emphasis role="bold">boost::future&lt;int></emphasis> ){} // callback gets an int
);</programlisting>
                    </para>
                    <para>We also have a <link xlink:href="examples/example_parallel_reduce_tcp.cpp">distributed version</link> as an example, which strictly looks like the parallel_for version.</para>
                </sect2>
                <sect2>
                    <title>parallel_invoke</title>
                    <para>parallel_invoke invokes a variadic list of predicates in parallel and
                        returns a (continuation of) tuple of futures containing the result of all of
                        them.</para>
                    <programlisting>template &lt;class Job, typename... Args>
boost::asynchronous::detail::<emphasis role="bold">continuation</emphasis>&lt;typename decltype(boost::asynchronous::detail::make_future_tuple(args...))::element_type,Job>
<emphasis role="bold">parallel_invoke</emphasis>(Args&amp;&amp;... args);</programlisting>
                    <para>Of course, the futures can have exceptions if exceptions are thrown, as in
                        the following example:</para>
                    <programlisting>post_callback(
               []()
               {
                   return boost::asynchronous::parallel_invoke&lt;boost::asynchronous::any_callable>(
                                     boost::asynchronous::<emphasis role="bold">to_continuation_task</emphasis>([](){throw my_exception();}), // void lambda
                                     boost::asynchronous::<emphasis role="bold">to_continuation_task</emphasis>([](){return 42.0;}));         // double lambda
                },// work
                // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
                [this](boost::<emphasis role="bold">future</emphasis>&lt;std::<emphasis role="bold">tuple</emphasis>&lt;<emphasis role="bold">boost::future&lt;void>,boost::future&lt;double></emphasis>>> res)
                {
                   try
                   {
                        auto t = res.get();
                        std::cout &lt;&lt; "got result: " &lt;&lt; (<emphasis role="bold">std::get&lt;1></emphasis>(t)).get() &lt;&lt; std::endl;                // 42.0
                        std::cout &lt;&lt; "got exception?: " &lt;&lt; (<emphasis role="bold">std::get&lt;0></emphasis>(t)).has_exception() &lt;&lt; std::endl;  // true, has exception
                    }
                    catch(std::exception&amp; e)
                    {
                        std::cout &lt;&lt; "got exception: " &lt;&lt; e.what() &lt;&lt; std::endl;
                     }
                }// callback functor.
);</programlisting>
                    <para>Notice the use of <emphasis role="bold">to_continuation_task</emphasis> to
                        convert the lambdas in continuations.</para>
                    <para>As always, the callback lambda will be called when all tasks complete and
                        the futures are non-blocking.</para>
                    <para>Please have a look at the <link
                            xlink:href="examples/example_parallel_invoke.cpp">complete
                            example</link>.</para>
                </sect2>
                <sect2>
                    <title>parallel_find_all</title>
                    <para>This algorithms finds and copies into a returned container all elements of
                        a range for which a predicate returns true. Like parallel_for, we have four
                        versions of the algorithm.</para>
                    <programlisting>template &lt;class Iterator, class Func,
          class ReturnRange=std::vector&lt;typename std::iterator_traits&lt;Iterator>::value_type>,
          class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;ReturnRange,Job>
<emphasis role="bold">parallel_find_all</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range, class Func, class ReturnRange=Range, class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;ReturnRange,Job>
<emphasis role="bold">parallel_find_all</emphasis>(Range const&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range, class Func, class ReturnRange=Range, class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;ReturnRange,Job>
<emphasis role="bold">parallel_find_all</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Range, class Func, class ReturnRange=typename Range::return_type, class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;ReturnRange,Job>
<emphasis role="bold">parallel_find_all</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm will find elements matching the search criteria in parallel
                        and copy all into a new container, by default of the type given as
                        argument:</para>
                    <para>
                        <programlisting><emphasis role="bold">std::vector&lt;int></emphasis> data;
post_callback(
    [this]()
    {
       return boost::asynchronous::<emphasis role="bold">parallel_find_all</emphasis>(this->data.begin(),this->data.end(),
                                                     [](int i)
                                                     {
                                                         return (400 &lt;= i) &amp;&amp; (i &lt; 600);
                                                     },
                                                     1500);
    },
    ](<emphasis role="bold">boost::future&lt;std::vector&lt;int>></emphasis> ){} // callback gets an int
);</programlisting>
                    </para>
                    <para>Please have a look at the <link xlink:href="examples/example_parallel_find_all.cpp">complete example</link>.</para>
                </sect2>
                <sect2>
                    <title>parallel_extremum</title>
                    <para>parallel_extremum finds an extremum (min/max) of a range given by a
                        predicate. It is a good example of using a prallel_reduce for writing new
                        algorithms. We have, as usual, four versions of the algorithm:.</para>
                    <para>
                        <programlisting>template &lt;class Iterator, class Func,class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;typename std::iterator_traits&lt;Iterator>::value_type,Job>
<emphasis role="bold">parallel_extremum</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Func,class Job=boost::asynchronous::any_callable>
decltype(boost::asynchronous::parallel_reduce(...))
<emphasis role="bold">parallel_extremum</emphasis>(Range const&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Func,class Job=boost::asynchronous::any_callable>
decltype(boost::asynchronous::parallel_reduce(...))
<emphasis role="bold">parallel_extremum</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Iterator, class Func,class Job=boost::asynchronous::any_callable>
decltype(boost::asynchronous::parallel_reduce(...))
<emphasis role="bold">parallel_extremum</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    </para>
                    <para>Please have a look at the <link xlink:href="examples/example_parallel_extremum.cpp">complete example</link>.</para>
                </sect2>
                <sect2>
                    <title>parallel_count</title>
                    <para>parallel_count counts the elements of a range satisfying a predicate. As
                        usual, we have four versions of the algorithm.</para>
                    <programlisting>template &lt;class Iterator, class Func,class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;long,Job>
<emphasis role="bold">parallel_count</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Func,class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;long,Job>
<emphasis role="bold">parallel_count</emphasis>(Range const&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Func,class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;long,Job>
<emphasis role="bold">parallel_count</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Iterator, class Func,class Job=boost::asynchronous::any_callable>
boost::asynchronous::detail::continuation&lt;long,Job>
<emphasis role="bold">parallel_count</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>Please have a look at the <link xlink:href="examples/example_parallel_count.cpp">complete example</link>.</para>
                </sect2>
            </sect1>
        </chapter>
        <chapter>
            <title>Tips.</title>
            <sect1>
                <title>Which protections you get, which ones you don't.</title>
                <para>Asynchronous is doing much to protect developers from some ugly beasts around:<itemizedlist>
                        <listitem>
                            <para>(visible) threads</para>
                        </listitem>
                        <listitem>
                            <para>races</para>
                        </listitem>
                        <listitem>
                            <para>deadlocks</para>
                        </listitem>
                        <listitem>
                            <para>crashes at the end of an object lifetime</para>
                        </listitem>
                </itemizedlist></para>
                <para>It also helps parallelizing and improve performance by not blocking. It also
                    helps find out where bottlenecks and hidden possible performance gains
                    are.</para>
                <para>There are, however, things for which it cannot help:<itemizedlist>
                        <listitem>
                            <para>cycles in design</para>
                        </listitem>
                        <listitem>
                            <para>C++ legal ways to work around the protections if one really
                                wants.</para>
                        </listitem>
                        <listitem>
                            <para>blocking on a future if one really wants.</para>
                        </listitem>
                        <listitem>
                            <para>using "this" captured in a task lambda.</para>
                        </listitem>
                        <listitem>
                            <para>writing a not clean task with pointers or references to data used
                                in a servant.</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>No cycle, never!</title>
                 <para>This is one of the first things one learns in a design class. Cycles are
                    evil. Everybody knows it. And yet, designs are often made without care in a too
                    agile process, dependency within an application is not thought out carefully
                    enough and cycles happen. What we do learn in these classes is that cycles make
                    our code monolithic and not reusable. What we however do not learn is how bad,
                    bad, bad this is in face of threads. It becomes impossible to follow the flow of
                    information, resource usage, degradation of performance. But the worst of all,
                    it becomes almost impossible to prevent deadlocks and resource leakage.</para>
                <para>Using Asynchronous will help write clean layered architectures. But it will
                    not replace carefully crafted designs, thinking before writing code and the
                    experience which make a good designer. Asynchronous will not be able to prevent
                    code having cycles in a design. </para>
                <para>Fortunately, there is an easy solution: back to the basics, well-thought
                    designs before coding, writing diagrams, using a real development process (hint:
                    an agile "process" is not all this in the author's mind).</para>
            </sect1>
            <sect1>
                <title>No this within a task.</title>
                <para>A very easy way to see if you are paving the way to a race even using
                    Asynchronous is to have a look at the captured variables of a lambda posted to a
                    threadpool. If you find "this", it's probably bad, unless you really know that
                    the single-thread code will do nothing. Apart from a simple application, this
                    will not be true. By extension, pointers, references, or even shared smart
                    pointers pointing to data living in a single-thread world is usually bad.</para>
                <para>Experience whows that there are only two safe way to pass data to a posted
                    task: copy and move. Keep to this rule and you will be safe.</para>
                <para>On the other hand, "this" is okay in the capture list of a callback task as
                    Asynchronous will only call it if the servant is still alive.</para>
            </sect1>
        </chapter>
    </part>
    <part>
        <title>Reference</title>
        <chapter>
            <title>Queues</title>
            <para> Asynchronous provides a range of queues with different trade-offs. Use
                    <code>threadsafe_list</code> as default for a quickstart with
                Asynchronous.</para>
            <sect1>
                <title>threadsafe_list</title>
                <para>This queue is mostly the one presented in Anthony Williams' book, "C++
                    Concurrency In Action". It is made of a single linked list of nodes, with a
                    mutex at each end of the queue to minimize contention. It is reasonably fast and
                    of simple usage. It can be used in all configurations of pools. Please use this
                    container as default when starting with Asynchronous.</para>
                <para>Its constructor does not require any parameter forwarded from the
                    scheduler.</para>
                <para>Stealing: from the same queue end as pop. Will be implemented better (from the
                    other end to reduce contention) in a future version.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class threadsafe_list;                 
                </programlisting>
            </sect1>
            <sect1>
                <title>lockfree_queue</title>
                <para>This queue is a light wrapper around a <code>boost::lockfree::queue</code>,
                    which gives lockfree behavior at the cost of an extra dynamic memory allocation. </para>
                <para>The container is faster than a <code>threadsafe_list</code>, provided one
                    manages to set the queue size to an optimum value. A too small size will cause
                    expensive memory allocations, a too big size will significantly degrade
                    performance.</para>
                <para>Its constructor requires a default size forwarded from the scheduler.</para>
                <para>Stealing: from the same queue end as pop. Stealing from the other end is not
                    supported by <code>boost::lockfree::queue</code>. It can be used in all
                    configurations of pools.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_queue;                 
                </programlisting>
            </sect1>
            <sect1>
                <title>lockfree_spsc_queue</title>
                <para>This queue is a light wrapper around a
                        <code>boost::lockfree::spsc_queue</code>, which gives lockfree behavior at
                    the cost of an extra dynamic memory allocation. </para>
                <para>Its constructor requires a default size forwarded from the scheduler.</para>
                <para>Stealing: None. Stealing is not supported by
                        <code>boost::lockfree::spsc_queue</code>. It can only be used
                    Single-Producer / Single-Consumer, which reduces its typical usage to a queue of
                    a <code>multiqueue_threadpool_scheduler</code> as consumer, with a
                        <code>single_thread_scheduler</code> as producer.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_spsc_queue;                 
                </programlisting>
            </sect1>
            <sect1>
                <title>lockfree_stack</title>
                <para>This queue is a light wrapper around a <code>boost::lockfree::stack</code>,
                    which gives lockfree behavior at the cost of an extra dynamic memory allocation.
                    This container creates a task inversion as the last posted tasks will be
                    executed first.</para>
                <para>Its constructor requires a default size forwarded from the scheduler.</para>
                <para>Stealing: from the same queue end as pop. Stealing from the other end is not
                    supported by <code>boost::lockfree::stack</code>. It can be used in all
                    configurations of pools.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_stack;                 
                </programlisting>
            </sect1>
        </chapter>
        <chapter>
            <title>Schedulers</title>
            <para> There is not the perfect scheduler. In any case it's a question of trade-off.
                Here are the schedulers offered by Asynchronous.</para>
            <sect1>
                <title>single_thread_scheduler</title>
                <para>The scheduler of choice for all servants which are not thread-safe. Serializes
                    all calls to a single queue and executes them in order. Using
                        <code>any_queue_container</code> as queue will however allow it to support
                    task priority.</para>
                <para>This scheduler does not steal from other queues or pools, only stealing_
                    threadpools do this, and does not get stolen from to avoid races.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue>
class single_thread_scheduler;                 
                </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;> >);  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;> >(10)); // size of queue
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>> >);                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>> >(10)); // size of queue</programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/single_thread_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry>1</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 thread)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>threadpool_scheduler</title>
                <para>The simplest and easiest threadpool using a single queue, though multiqueue
                    behavior could be done using <code>any_queue_container</code>. The advantage is
                    that it allows the pool to be given 0 thread and only be stolen from. The cost
                    is a slight performance loss due to higher contention on the single
                    queue.</para>
                <para>This pool does not steal from other pool's queues.</para>
                <para>Use this pool as default for a quickstart with Asynchronous.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue>
class threadpool_scheduler;                 
                </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;> >(4)); // 4 threads in pool  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;> >(4,10)); // size of queue=10, 4 threads in pool
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>> >(4)); // 4 threads in pool                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>> >(4,10)); // size of queue=10, 4 threads in pool  </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">0</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 queue)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>multiqueue_threadpool_scheduler</title>
                <para>This is a <code>threadpool_scheduler</code> with multiple queues to reduce
                    contention. On the other hand, this pool requires at least one thread.</para>
                <para>This pool does not steal from other pool's queues though pool threads do steal
                    from each other's queues.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class FindPosition=boost::asynchronous::default_find_position&lt; > >
class multiqueue_threadpool_scheduler;                 
                </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;> >(4)); // 4 threads in pool  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;> >(4,10)); // size of queue=10, 4 threads in pool
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>> >(4)); // 4 threads in pool                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>> >(4,10)); // size of queue=10, 4 threads in pool  </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/multiqueue_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>stealing_threadpool_scheduler</title>
                <para>This is a <code>threadpool_scheduler</code> with the added capability to steal
                    from other pool's queues within a <code>composite_threadpool_scheduler</code>.
                    Not used within a <code>composite_threadpool_scheduler</code>, it is a standard
                        <code>threadpool_scheduler</code>.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,bool /* InternalOnly */ = true >
class stealing_threadpool_scheduler;                 
                </programlisting>
                <para>Creation if used within a <code>composite_threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::stealing_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;> >(4)); // 4 threads in pool  
                </programlisting>
                <para> However, if used stand-alone, which has little interest outside of unit
                    tests, we need to add a template parameter to inform it:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::stealing_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;><emphasis role="bold">,true</emphasis> >(4)); // 4 threads in pool  
                </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/stealing_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">0</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 queue)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>stealing_multiqueue_threadpool_scheduler</title>
                <para>This is a <code>multiqueue_threadpool_scheduler</code> with the added
                    capability to steal from other pool's queues within a
                        <code>composite_threadpool_scheduler</code> (of course, threads within this
                    pool do steal from each other queues, with higher priority). Not used within a
                        <code>composite_threadpool_scheduler</code>, it is a standard
                        <code>multiqueue_threadpool_scheduler</code>.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class FindPosition=boost::asynchronous::default_find_position&lt; >,bool /* InternalOnly */= true  >
class stealing_multiqueue_threadpool_scheduler;                 
                </programlisting>
                <para>Creation if used within a <code>composite_threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;> >(4)); // 4 threads in pool  
                </programlisting>
                <para> However, if used stand-alone, which has little interest outside of unit
                    tests, we need to add a template parameter to inform it:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>,boost::asynchronous::default_find_position&lt;>,<emphasis role="bold">true</emphasis>  >(4)); // 4 threads in pool  
                </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/stealing_multiqueue_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>composite_threadpool_scheduler</title>
                <para>This pool has no thread by itself. Its job is to contain other pools,
                    accessible by the priority given by posting, and share all queues of its
                    subpools among them. Only the stealing_* pools and <code>asio_scheduler</code>
                    will make use of this and steal from other pools though.</para>
                <para>For creation we need to create other pool of stealing or not stealing, stolen
                    from or not, schedulers. stealing_xxx pools will try to steal jobs from other
                    pool of the same composite, but only if these schedulers support this. Other
                    threadpools will not steal but get stolen from.
                        <code>single_thread_scheduler</code> will not steal or get stolen
                    from.</para>
                <programlisting>// create a composite threadpool made of:
// a multiqueue_threadpool_scheduler, 0 thread
// This scheduler does not steal from other schedulers, but will lend its queues for stealing
auto tp = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> > (0,100));

// a stealing_multiqueue_threadpool_scheduler, 3 threads, each with a threadsafe_list
// this scheduler will steal from other schedulers if it can. In this case it will manage only with tp, not tp3
auto tp2 = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;boost::asynchronous::threadsafe_list&lt;> > (3));

// composite pool made of the previous 2
auto tp_worker = boost::asynchronous::create_shared_scheduler_proxy(new <emphasis role="bold">boost::asynchronous::composite_threadpool_scheduler&lt;> (tp,tp2)</emphasis>); 
                </programlisting>
                <para>Declaration:</para>
                <programlisting>template&lt;class Job = boost::asynchronous::any_callable,
         class FindPosition=boost::asynchronous::default_find_position&lt; >,
         class Clock = boost::chrono::high_resolution_clock  >
class composite_threadpool_scheduler;                 
                </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/composite_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry>0</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>asio_scheduler</title>
                <para>This pool brings the infrastructure and access to io_service for an integrated
                    usage of Boost.Asio. Furthermore, if used withing a
                        <code>composite_threadpool_scheduler</code>, it will steal jobs from other
                    pool's queues.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class FindPosition=boost::asynchronous::default_find_position&lt; boost::asynchronous::sequential_push_policy > >
class asio_scheduler;                 
                </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::asio_scheduler&lt;>(4)); // 4 threads in pool  
                </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/extensions/asio/asio_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>No*</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
        </chapter>
        <chapter>
            <title>Compiler</title>
            <sect1>
                <title>C++ 11</title>
                <para>Asynchronous is C++11-only. Please check that your compiler has C++11 enabled
                    (-std=c++0x or -std=c++11 in different versions of gcc)</para>
            </sect1>
            <sect1>
                <title>Supported compilers</title>
                <para>At the moment, Asynchronous has only be tested with gcc versions from 4.7 to
                    4.8 and clang 3.3-3.4.</para>
            </sect1>
        </chapter>
    </part>
</book>
