<?xml version="1.0" encoding="UTF-8"?>
<?oxygen RNGSchema="http://www.oasis-open.org/docbook/xml/5.0/rng/docbook.rng" type="xml"?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
    <info>
        <title>Boost Asynchronous</title>
        <author>
            <personname>Christophe Henry</personname>
            <email>christophe.j.henry@gmail.com</email>
        </author>
        <copyright>
            <year>20015</year>
            <holder>
                <phrase> Distributed under the Boost Software License, Version 1.0. (See
                    accompanying file LICENSE_1_0.txt or copy at <link
                        xlink:href="http://www.boost.org/LICENSE_1_0.txt"
                        >http://www.boost.org/LICENSE_1_0.txt</link> ) </phrase>
            </holder>
        </copyright>
    </info>
    <preface>
        <title>Introduction</title>
        <para>
            <emphasis role="underline">Note</emphasis>: Asynchronous is not part of the Boost
            library. It is planed to be offered for Review at the beginning of 2016. </para>
        <para>Herb Sutter wrote <link
                xlink:href="http://www.gotw.ca/publications/concurrency-ddj.htm">in an
                article</link> "The Free Lunch Is Over", meaning that developpers will be forced to
            learn to develop multi-threaded applications. The reason is that we now get our extra
            power in the form of more cores. The problem is: multithreading is hard! It's full of
            ugly beasts waiting hidden for our mistakes: races, deadlocks, crashes, all kinds of
            subtle timing-dependent bugs. Worse yet, these bugs are hard to find because they are
            never reproducible when we are looking for them, which leaves us with backtrace
            analysis, and this is when we are lucky enough to have a backtrace in the first
            place.</para>
        <para>This is not even the only danger. CPUs are a magnitude faster than memory, I/O
            operations, network communications, which all stall our programms and degrade our
            performance, which means long sessions with coverage or analysis tools.</para>
        <para>Trying to solve these problems with tools of the past (mutexes, programmer-managed
            threads) is a dead-end. It's just too hard. This is where Boost Asynchronous is
            helping.</para>
        <para>There are already existing solutions for this. To name a few:</para>
        <para>
            <itemizedlist>
                <listitem>
                    <para>std/boost::async.</para>
                </listitem>
                <listitem>
                    <para>Intel TBB.</para>
                </listitem>
                <listitem>
                    <para>N3428.</para>
                </listitem>
            </itemizedlist>
        </para>
        <para>TBB is a wonderful parallel library. But it's not asynchronous as one needs to wait
            for the end of a parallel call.</para>
        <para>std::async will return us a future. But what will we do with it? Wait for it? This
            would be synchronous. Collect them and then wait for all? This would also be
            synchronous. Collect them, do something else, then check if they are ready? This would
            be wasted opportunity for more calculations. To make it worse, I/O usage will seriously
            degrade performance.</para>
        <para>To solve these problems, NB3428 is an attempt at continuations. Let's have a quick
            look at code using futures and .then (taken from N3428):</para>
        <para>
            <programlisting>future&lt;int> f1 = async([]() { return 123; });
future&lt;string> f2 = f1.then([](future&lt;int> f) {return f.get().to_string();}); // here .get() won’t block
f2.get(); // just a "small get" at the end?</programlisting>
        </para>
        <para>Saying that there is only a "small get" at the end is, for an application with
            real-time constraints, equivalent to saying at a lockfree conference something like
            "what is all the fuss about? Can't we just add a small lock at the end?". Just try
            it...</para>
        <para>Worse yet, it clutters the code, makes it hard to debug and understand. When did we
            give up writing design diagrams? How is this supposed to replace a dynamic behavior
            using a state machine?</para>
        <para>Asynchronous supports this programming model too, though it is advised to use it only
            for simple programs or quick prototyping, or as a step to the more powerful tools
            offered by the library. std::async can be replaced by
            boost::asynchronous::post_future:</para>
        <programlisting>auto pool = boost::asynchronous::make_shared_scheduler_proxy&lt;
                  boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                        boost::asynchronous::lockfree_queue&lt;>>>(8); // create a pool with 8 threads
boost::future&lt;int> fu = boost::asynchronous::post_future(pool,
    []()
    {
        return 123;
    });
f1.get();</programlisting>
        <para>Instead of an ugly future.then, Asynchronous supports continuations as coded into the
            task itself. We will see later how to do it. For the moment, here is a quick example.
            Let's say we want to modify a vector in parallel, then reduce it, also in parallel,
            without having to write synchronization points:</para>
        <programlisting>boost::future&lt;int> fu = boost::asynchronous::post_future(pool, // pool as before
    [this]()
    {
        return boost::asynchronous::parallel_reduce(                   // reduce will be done in parallel after for
            boost::asynchronous::parallel_for(std::move(this->m_data), // our data, a std::vector&lt;int> will be moved, transformed, then reduced and eventually destroyed
                                              [](int&amp; i)
                                              {
                                                  i += 2;              // transform all elements in parallel
                                              }, 1024),                // cutoff (when to go sequential. Will be explained later)
            [](int const&amp; a, int const&amp; b)                             // reduce function
            {
                return a + b;
            }, 1024);                                                  // reduce cutoff
    });
int res = fu.get();</programlisting>
        <para>But this is just the beginning. It is not even really asynchronous. More important,
            Boost Asynchronous is a library which can play a great role in making a thread-correct
            architecture. To achieve this, it offers tools for asynchronous designs: ActiveObject,
            safe callbacks, threadpools, servants, proxies, queues, algorithms, etc. </para>
        <para>Consider the following example showing us why we need an architecture tool:</para>
        <para>
            <programlisting>struct Bad : public boost::signals::trackable
{
   int foo();
};
boost::shared_ptr&lt;Bad> b;
future&lt;int> f = async([b](){return b->foo()});          </programlisting>
        </para>
        <para>Now we have the ugly problem of not knowing in which thread Bad will be destroyed. And
            as it's pretty hard to have a thread-safe destructor, we find ourselves with a race
            condition in it. </para>
        <para>Asynchronous programming has the advantage of allowing to design of code, which is
            nonblocking and single-threaded while still utilizing parallel hardware at full
            capacity. And all this while forgetting what a mutex is. </para>
        <para>This brings us to a central point of Asynchronous: if we build a system with strict
            real-time constraints, there is no such thing as a small blocking get(). We need to be
            able to react to any event in the system in a timely manner. And we can't afford to have
            lots of functions potentially waiting too long everywhere in our code. Therefore,
            .then() is only good for an application of a few hundreds of lines. What about using a
            timed_wait instead? Nope. This just limits the amount of time we waste waiting. Either
            we wait too long before handling an error or result, or we wait not enough and we poll.
            In any case, while waiting, our thread cannot react to other events and wastes
            time.</para>
        <para>An image being more worth than thousand words, the following story will explain in a
            few minutes what Asynchronous is about. Consider some fast-food restaurant:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor1.jpg"/>
                </imageobject>
            </inlinemediaobject>
        </para>
        <para>This restaurant has a single employee, Worker, who delivers burgers through a burger
            queue and drinks. A Customer comes. Then another, who waits until the first customer is
            served.</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor2.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>To keep customers happy by reducing waiting time, the restaurant owner hires a second
            employee:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor3.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>Unfortunately, this brings chaos in the restaurant. Sometimes, employes fight to get a
            burger to their own customer first:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-RC.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>And sometimes, they stay in each other's way:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-DL.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>This clearly is a not an optimal solution. Not only the additional employee brings
            additional costs, but both employees now spend much more time waiting. It also is not a
            scalable solution if even more customers want to eat because it's lunch-time right now.
            Even worse, as they fight for resources and stay in each other's way, the restaurant now
            serves people less fast than before. Customers flee and the restaurant gets bankrupt. A
            sad story, isn't it? To avoid this, the owner decides to go asynchronous. He keeps a
            single worker, who runs in zero time from cash desk to cash desk:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-async.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>The worker never waits because it would increase customer's waiting time. Instead, he
            runs from cash desks to the burger queue, beverage machine using a self-made strategy: <itemizedlist>
                <listitem>
                    <para>ask what the customer wants and keep an up-to-date information of the
                        customer's state.</para>
                </listitem>
                <listitem>
                    <para>if we have another customer at a desk, ask what he wants. For both
                        customers, remember the state of the order (waiting for customer choice,
                        getting food, getting drink, delivering, getting payment, etc.)</para>
                </listitem>
                <listitem>
                    <para>as soon as some new state is detected (customer choice, burger in the
                        queue, drink ready), handle it.</para>
                </listitem>
                <listitem>
                    <para>priorities are defined: start the longest-lasting tasks first, serve
                        angry-looking customers first, etc.</para>
                </listitem>
            </itemizedlist></para>
        <para>The following diagram shows us the busy and really really fast worker in
            action:</para>
        <para><inlinemediaobject>
                <imageobject>
                    <imagedata fileref="pics/Proactor-async2.jpg"/>
                </imageobject>
            </inlinemediaobject></para>
        <para>Of course the owner needs a worker who runs fast, and has a pretty good memory so he
            can remember what customers are waiting for. </para>
        <para>This is what Asynchronous is for. A worker (thread) runs as long as there are waiting
            customers, following a precisely defined algorithm, and lots of state machines to manage
            the asynchronous behaviour. In case of customers, we could have a state machine: Waiting
            -> PickingMenu -> WaitingForFood -> Paying.</para>
        <para>We also need some queues (Burger queue, Beverage glass positioning) and some
            Asynchronous Operation Processor (for example a threadpool made of workers in the
            kitchen), event of different types (Drinks delivery). Maybe we also want some work
            stealing (someone in the kitchen serving drinks as he has no more burger to prepare. He
            will be slower than the machine, but still bring some time gain).</para>
        <para><emphasis role="bold">To make this work, the worker must not block, never,
                ever</emphasis>. And whatever he's doing has to be as fast as possible, otherwise
            the whole process stalls.</para>
    </preface>
    <part>
        <title>Concepts</title>
        <chapter>
            <title>Related designs: std::async, Active Object, Proactor</title>
            <sect1>
                <title>std::async</title>
                <subtitle>What is wrong with it</subtitle>
                <para>The following code is a classical use of std::async as it can be found in
                    articles, books, etc.</para>
                <programlisting>std::future&lt;int> f = std::async([](){return 42;}); // executes asynchronously
int res = f.get(); // wait for result, block until ready</programlisting>
                <para>It looks simple, easy to use, and everybody can get it. The problem is, well,
                    that it's not really asynchronous. True, our lambda will execute in another
                    thread. Actually, it's not even guaranteed either. But then, what do we do with
                    our future? Do we poll it? Or call get() as in the example? But then we will
                    block, right? And if we block, are we still asynchronous? If we block, we cannot
                    react to any event happening in our system any more, we are unresponsive for a
                    while (are we back to the old times of freezing programs, the old time before
                    threads?). We also probably miss some opportunities to fully use our hardware as
                    we could be doing something more useful at the same time, as in our fast-food
                    example. And diagnostics are looking bad too as we are blocked and cannot
                    deliver any. What is left to us is polling. And if we get more and more futures,
                    do we carry a bag of them with us at any time and check them from time to time?
                    Do we need some functions to, at a given point, wait for all futures or any of
                    them to be ready? </para>
                <para>Wait, yes they exist, <code>wait_for_all</code> and
                    <code>wait_for_any</code>... </para>
                <para>And what about this example from an online documentation?</para>
                <para>
                    <programlisting>{ 
   std::async(std::launch::async, []{ f(); }); 
   std::async(std::launch::async, []{ g(); });
}</programlisting>
                </para>
                <para>Every std::async returns you a future, a particularly mean one which blocks
                    upon destruction. This means that the second line will not execute until f()
                    completes. Now this is not only not asynchronous, it's also much slower than
                    calling sequentially f and g while doing the same.</para>
                <para>No, really, this does not look good. Do we have alternatives?</para>
            </sect1>
            <sect1>
                <title>N3558 / N3650</title>
                <para>Of course it did not go unnoticed that std::async has some limitations. And so
                    do we see some tries to save it instead of giving it up. Usually, it goes around
                    the lines of blocking, but later.</para>
                <para>
                    <programlisting>future&lt;int> f1 = async([]() { return 123; }); 
future&lt;string> f2 = f1.then([](future&lt;int> f) 
{ 
  return f.get().to_string(); // here .get() won’t block 
});
// and here?
string s= f2.get();</programlisting>
                </para>
                <para>The idea is to make std::async more asynchronous (this already just sounds
                    bad) by adding something (.then) to be called when the asynchronous action
                    finishes. It still does not fly:<itemizedlist>
                        <listitem>
                            <para>at some point, we will have to block, thus ending our asynchronous
                                behavior</para>
                        </listitem>
                        <listitem>
                            <para>This works only for very small programs. Do we imagine a 500k
                                lines program built that way?</para>
                        </listitem>
                    </itemizedlist></para>
                <para>And what about the suggestion of adding new keywords, async and await, as in
                    N3650? Nope. First because, as await suggests, someone will need, at some point,
                    to block waiting. Second because as we have no future, we also lose our polling
                    option.</para>
            </sect1>
            <sect1>
                <title>Active Object</title>
                <subtitle>Design</subtitle>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/ActiveObject.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>This simplified diagram shows a possible design variation of an Active Object
                    pattern.</para>
                <para>A thread-unsafe Servant is hidden behind a Proxy, which offers the same
                    members as the Servant itself. This Proxy is called by clients and delivers a
                    future object, which will, at some later point, contain the result of the
                    corresponding member called on the servant. The Proxy packs a MethodRequest
                    corresponding to a Servant call into the ActivationQueue. The Scheduler waits
                    permanently for MethodRequests in the queue, dequeues them, and executes them.
                    As only one scheduler waits for requests, it serializes access to the Servant,
                    thus providing thread-safety.</para>
                <para>However, this pattern presents some liabilities:<itemizedlist>
                        <listitem>
                            <para>Performance overhead: depending on the system, data moving and
                                context switching can be a performance drain.</para>
                        </listitem>
                        <listitem>
                            <para>Memory overhead: for every Servant, a thread has to be created,
                                consuming resources.</para>
                        </listitem>
                        <listitem>
                            <para>Usage: getting a future gets us back to the non-asynchronous
                                behaviour we would like to avoid.</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Proactor</title>
                <subtitle>Design</subtitle>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/Proactor.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>This is the design pattern behind Boost.Asio. See: <link
                        xlink:href="http://www.boost.org/doc/libs/1_57_0/doc/html/boost_asio/overview/core/async.html"
                        >Boost.Asio documentation</link> for a full explanation. Boost Asynchronous
                    is very similar. It supports enqueueing asynchronous operations and waiting for
                    callbacks, offering extensions: safe callbacks, threadpools, proxies,
                    etc.</para>
            </sect1>
        </chapter>
        <chapter>
            <title>Features of Boost.Asynchronous</title>
            <sect1>
                <title>Active Component</title>
                <subtitle>Extending Active Objects with more servants within a thread
                    context</subtitle>
                <para>A commonly cited drawback of Active Objects is that they are awfully
                    expensive. A thread per object is really a waste of ressources.
                    Boost.Asynchronous extends this concept by allowing an unlimited number of
                    objects to live within a single thread context, thus amortizing the costs. It
                    even provides a way for n Active Objects to share m threads while still being
                    called single thread. This allows tuning thread usage.</para>
                <para>As many objects are potentially living in a thread context, none should be
                    allowed to process long-lasting tasks as it would reduce reactivity of the whole
                    component. In this aspect, Asynchronous' philosophy is closer to a
                    Proactor.</para>
                <para>As long-lasting tasks do happen, Boost.Asynchronous provides several
                    implementations of threadpools and the needed infrastructure to make it safe to
                    post work to threadpools and get aynchronously a safe callback. It also provides
                    safe mechanisms to shutdown Active Components and threadpools.</para>
            </sect1>
            <sect1>
                <title>Better Architecture</title>
                <para>We all learned in our design books that a software should be organized into
                    layers. This is, however, easier said than done, single-threaded, but much worse
                    when layers are having their own threads. Let's say, layer A is on top and
                    basing itself on layer B. A creates B and keeps it alive as long as it lives
                    itself. A and B are each composed of hundreds of classes / objects. Our standard
                    communication is A => B, meaning A gives orders to B, which executes them. This
                    is the theory. Unfortunately, B needs to give answers, usually delayed, to A.
                    Unfortunately, A and B live in different threads. This means mutexes. Ouch. Now
                    we are forced to check every class of A and protect it. Worse, the object of A
                    getting an answer might have long been destroyed. Ouch again. What to do? We
                    could keep the object of A alive in the callback of B. But then we have a
                    dependency B -> A. Ouch again, bad design. We can also hide the dependency using
                    some type erasure mechanism. We still have a logical one as B keeps its owner,
                    A, alive. Then, we can use a weak_ptr so that B does not keep A alive. But when
                    we lock, we do keep A alive. It's for a short time, but what if A is shutting
                    down? It's lost, our layered design is broken.</para>
                <para>Asynchronous is more that a library providing a better std::async or some
                    parallel algorithms, it's first of all an architectural tool. In the above case,
                    we will decide that every layer will live in its own thread(s), called
                    schedulers in Asynchronous language. Deciding in which thread an object "lives"
                    is a key point of a good design. Then the top layer, A, will make a request to
                    B, aking a future as a result, or much better, providing a callback.
                    Asynchronous offers a callback safe in two ways: thread-safe and checking the
                    lifetime of the callback target. This callback is provided by
                        <code>make_safe_callback</code>. This simple tool is a major help in making
                    a safe and efficient design.</para>
            </sect1>
            <sect1>
                <title>Shutting down</title>
                <para>Shutting down a thread turns out to be harder in practice than expected, as
                    shown by several posts of surprise on the Boost mailing lists when Boost.Thread
                    tried to match the C++ Standard. Asynchronous hides all these ugly details. What
                    users see is a scheduler proxy object, which can be shared by any number of
                    objects, and running any number of threads, managed by a scheduler. The
                    scheduler proxy object manages the lifetime of the scheduler. </para>
                <para>When the last instance of the scheduler object is destroyed, the scheduler
                    thread is stopped. When the last instance of a scheduler proxy is destroyed, the
                    scheduler thread is joined. It's as simple as that. This makes threads shared
                    objects. </para>
            </sect1>
            <sect1>
                <title>Object lifetime</title>
                <para>There are subtle bugs when living in a multithreaded world. Consider the
                    following class:</para>
                <para>
                    <programlisting>struct Unsafe
{
  void foo()
  {
    m_mutex.lock();
    // call private member
    m_mutex.unlock();
  }
private:
  void foobar()
  {
    //we are already locked when called, do something while locked
  }
  boost::mutex m_mutex;
};            </programlisting>
                </para>
                <para>This is called a thread-safe interface pattern. Public members lock, private
                    do not. Simple enough. Unfortunately, it doesn't fly.</para>
                <para>First one has the risk of deadlock if a private member calls a public one
                    while being called from another public member. If we forget to check one path of
                    execution within a class implementation, we get a deadlock. We'll have to test
                    every single path of execution to prove our code is correct. And this at every
                    commit.</para>
                <para>Usually, for any complex class, where there's a mutex, there is a race or a
                    deadlock...</para>
                <para>But even worse, the principle itself is not correct in C++. It supposes that a
                    class can protect itself. Well, no, it can't. Why? One cannot protect the
                    destructor. If the object (and the mutex) gets destroyed when a thread waits for
                    it in foo(), we get a crash or an exception. We can mitigate this with the use
                    of a shared_ptr, then we have no destructor call while someone waits for the
                    mutex. Unfortunately, we still have a risk of a signal, callback, etc. all those
                    things mixing badly with threads. And if we use too many shared_ptr's, we start
                    having lifetime issues or leaks. </para>
                <para>There are more lifetime issues, even without mutexes or threads. If you have
                    ever used Boost.Asio, a common mistake and an easy one is when a callback is
                    called in the proactor thread after an asynchronous operation, but the object
                    called is long gone and the callback invalid. Asynchronous provides <command
                        xlink:href="#trackable_servant">trackable_servant</command> which makes sure
                    that a callback is not called if the object which called the asynchronous
                    operation is gone. It also prevents a task posted in a threadpool to be called
                    if this condition occurs, which improves performance. Asynchronous also provides
                    a safe callback for use as Boost.Asio or similar asynchronous libraries.</para>
            </sect1>
            <sect1>
                <title>Servant Proxies</title>
                <para>Asynchronous offers <code>servant_proxy</code>, which makes the outside world
                    call members of a servant as if it was not living in an ActiveObject. It looks
                    like a thread-safe interface, but safe from deadlock and race conditions. </para>
            </sect1>
            <sect1>
                <title>Interrupting</title>
                <subtitle>Or how to catch back if you're drowning. </subtitle>
                <para>Let's say you posted so many tasks to your threadpool that all your cores are
                    full, still, your application is slipping more and more behind plan. You need to
                    give up some tasks to catch back a little.</para>
                <para>Asynchronous can give us an interruptible cookie when we post a task to a
                    scheduler, and we can use it to <command xlink:href="#interrupting_tasks">stop a
                        posted task</command>. If not running yet, the task will not start, if
                    running, it will stop at the next interruption point, in the sense of the <link
                        xlink:href="http://www.boost.org/doc/libs/1_54_0/doc/html/thread/thread_management.html#thread.thread_management.tutorial.interruption"
                        >Boost.Thread documentation</link>. Diagnostics will show that the task was
                    interrupted.</para>
            </sect1>
            <sect1>
                <title>Diagnostics</title>
                <para>Finding out how good your software is doing is not an easy task. Developers
                    are notoriously bad at it. You need to add lots of logging to find out which
                    function call takes too long and becomes a bottleneck. Finding out the minimum
                    required hardware to run your application is even harder.</para>
                <para>Asynchronous design helps here too. By logging the required time and the
                    frequency of tasks, it is easy to find out how many cores are needed.
                    Bottlenecks can be found by logging what the Active Component is doing and how
                    long. Finally, designing the asynchronous Active Component as state machines and
                    logging state changes will allow a better understanding of your system and make
                    visible potential for concurrency. Even for non-parallel algorithms, finding
                    out, using a state machine, the earliest point a task can be thrown to a
                    threadpool will give some low-hanging-fruit concurrency. Throw enough tasks to
                    the threadpool and manage this with a state machine and you might use your cores
                    with little effort. Parallelization can then be used later on by logging which
                    tasks are worth parallelized.</para>
                <para>Asynchronous offers tools generating nice HTML outputs for every schedulers,
                    including waiting and execution times of tasks, histograms, etc.</para>
                <para>TODO link to html diags example</para>
            </sect1>
            <sect1>
                <title>Continuations</title>
                <para>Callbacks are great when you have a complex flow of operations which require a
                    state machine for management, however there are cases where callbacks are not an
                    ideal solution. Either because your application would require a constant
                    switching of context between single-threaded and parallel schedulers, or because
                    the single-threaded scheduler might be busy, which would delay completion of the
                    algorithm. A known example of this is a parallel fibonacci. In this case, one
                    can register a <command xlink:href="#continuations">continuation</command>,
                    which is to be executed upon completion of one or several tasks. </para>
                <para>This mechanism is flexible so that you can use it with futures coming from
                    another library, thus removing any need for a
                        <code>wait_for_all(futures...)</code> or a
                        <code>wait_for_any(futures...)</code>.</para>
            </sect1>
            <sect1>
                <title>Want more power? What about extra machines?</title>
                <para>What to do if your threadpools are using all of your cores but there simply
                    are not enough cores for the job? Buy more cores? Unfortunately, the number of
                    cores a single-machine can use is limited, unless you have unlimited money. A
                    dual 6-core Xeon, 24 threads with hyperthreading will cost much more than 2 x
                    6-core i7, and will usually have a lesser clock frequency and an older
                    architecture. </para>
                <para>The solution could be: start with the i7, then if you need more power, add
                    some more machines which will steal jobs from your threadpools using <command
                        xlink:href="#distributing">TCP</command>. This can be done quite easily with
                    Asynchronous.</para>
                <para>Want to build your own hierarchical network of servers? It's hard to make it
                    easier.</para>
            </sect1>
            <sect1>
                <title>Parallel algorithms</title>
                <para>The library also comes with <command xlink:href="#parallel_algos">non-blocking
                        algorithms</command> with iterators or ranges, partial support for TCP,
                    which fit well in the asynchronous system, with more to come. If you want to
                    contribute some more, be welcome. At the moment, the library offers:<itemizedlist>
                        <listitem>
                            <para>most STL algorithms</para>
                        </listitem>
                        <listitem>
                            <para>parallel_for / parallel_for_each</para>
                        </listitem>
                        <listitem>
                            <para>parallel_reduce</para>
                        </listitem>
                        <listitem>
                            <para>parallel_extremum</para>
                        </listitem>
                        <listitem>
                            <para>parallel_find_all</para>
                        </listitem>
                        <listitem>
                            <para>parallel_invoke</para>
                        </listitem>
                        <listitem>
                            <para>parallel_sort , parallel_quicksort</para>
                        </listitem>
                        <listitem>
                            <para>parallel_scan</para>
                        </listitem>
                        <listitem>
                            <para>parallelized Boost.Geometry algorithms for polygons
                                (parallel_union, parallel_intersection,
                                parallel_geometry_intersection_of_x,
                                parallel_geometry_union_of_x)</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Task Priority</title>
                <para>Asynchronous offers this possibility for all schedulers at low performance
                    cost. This means you not only have the possibility to influence task execution
                    order in a threadpool but also in Active Objects.</para>
                <para>This is achieved by posting a task to the queue with the corresponding
                    priority. It is also possible to get it even more fine-grained by using a
                    sequence of queues, etc.</para>
            </sect1>
            <sect1>
                <title>Integrating with Boost.Asio</title>
                <para>Asynchronous offers a Boost.Asio based <command xlink:href="#asio_scheduler">scheduler</command> allowing you to easily write
                    a Servant using Asio, or an Asio based threadpool. An advantage is that you get
                    safe callbacks and easily get your Asio application to scale. Writing a server
                    has never been easier.</para>
                <para>Asynchronous also uses Boost.Asio to provide a timer with callbacks.</para>
            </sect1>
            <sect1>
                <title>Integrating with Qt</title>
                <para>What about getting the power of Asynchronous within a Qt application? Use
                    Asynchronous' threadpools, algorithms and other cool features easily.</para>
            </sect1>
            <sect1>
                <title>Work Stealing</title>
                <para>Work stealing is supported both within the threads of a threadpool but also
                    between different threadpools. Please have a look at Asynchronous' composite
                    scheduler.</para>
            </sect1>
            <sect1>
                <title>Extending the library</title>
                <para>Asynchronous has been written with the design goal of allowing anybody to
                    extend the library. In particular, the authors are hoping to be offered the
                    following extensions:<itemizedlist>
                        <listitem>
                            <para>More schedulers, threadpools</para>
                        </listitem>
                        <listitem>
                            <para>Queues</para>
                        </listitem>
                        <listitem>
                            <para>Parallel algorithms</para>
                        </listitem>
                        <listitem>
                            <para>Integration with other libraries</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Design Diagrams</title>
                <para><inlinemediaobject>
                        <imageobject>
                            <imagedata fileref="pics/AsynchronousDesign.jpg"/>
                        </imageobject>
                    </inlinemediaobject></para>
                <para>This diagram shows an overview of the design behind Asynchronous. One or more
                    Servant objects live in a single-theaded world, communicating with the outside
                    world only through one or several queues, from which the single-threaded
                    scheduler pops tasks. Tasks are pushed by calling a member on a proxy
                    object.</para>
                <para>Like an Active Object, a client uses a proxy (a shared object type), which
                    offers the same members as the real servant, with the same parameters, the only
                    difference being the return type, a boost::future&lt;R>, with R being the return
                    type of the servant's member. All calls to a servant from the client side are
                    posted, which includes the servant constructor and destructor. When the last
                    instance of a servant is destroyed, be it used inside the Active Component or
                    outside, the servant destructor is posted.</para>
                <para>any_shared_scheduler is the part of the Active Object scheduler living inside
                    the Active Component. Servants do not hold it directly but hold an
                    any_weak_scheduler instead. The library will use it to create a posted callback
                    when a task executing in a worker threadpool is completed.</para>
                <para>Shutting down an Active Component is done automatically by not needing it. It
                    happens in the following order:<itemizedlist>
                        <listitem>
                            <para>While a servant proxy is alive, no shutdown</para>
                        </listitem>
                        <listitem>
                            <para>When the last servant proxy goes out of scope, the servant
                                destructor is posted.</para>
                        </listitem>
                        <listitem>
                            <para>if jobs from servants are running in a threadpool, they get a
                                chance to stop earlier by running into an interruption point or will
                                not even start.</para>
                        </listitem>
                        <listitem>
                            <para>threadpool(s) is (are) shut down.</para>
                        </listitem>
                        <listitem>
                            <para>The Active Component scheduler is stopped and its thread
                                terminates.</para>
                        </listitem>
                        <listitem>
                            <para>The last instance of any_shared_scheduler_proxy goes out of scope
                                with the last servant proxy and joins.</para>
                        </listitem>
                    </itemizedlist>
                </para>
                <para>It is usually accepted that threads are orthogonal to an OO design and
                    therefore are hard to manage as they don't belong to an object. Asynchronous
                    comes close to this: threads are not directly used, but instead owned by a
                    scheduler, in which one creates objects and tasks.</para>
            </sect1>
        </chapter>
    </part>
    <part>
        <title>User Guide</title>
        <chapter>
            <title>Using Asynchronous</title>
            <sect1>
                <title>Hello, asynchronous world</title>
                <para>The following code shows a very basic usage (a complete example <link
                        xlink:href="examples/example_post_future.cpp">here</link>), this is not
                    really asynchronous yet:</para>
                <programlisting>#include &lt;boost/asynchronous/scheduler/threadpool_scheduler.hpp>
#include &lt;boost/asynchronous/queue/lockfree_queue.hpp>
#include &lt;boost/asynchronous/scheduler_shared_proxy.hpp>
#include &lt;boost/asynchronous/post.hpp>
struct void_task
{
    void operator()()const
    {
        std::cout &lt;&lt; "void_task called" &lt;&lt; std::endl;
    }
};
struct int_task
{
    int operator()()const
    {
        std::cout &lt;&lt; "int_task called" &lt;&lt; std::endl;
        return 42;
    }
};  

// create a threadpool scheduler with 3 threads and communicate with it using a threadsafe_list
// we use auto as it is easier than boost::asynchronous::any_shared_scheduler_proxy&lt;>
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                            new boost::asynchronous::threadpool_scheduler&lt;
                                boost::asynchronous::lockfree_queue&lt;> >(3));
// post a simple task and wait for execution to complete
boost::future&lt;void> fuv = boost::asynchronous::post_future(scheduler, void_task());
fuv.get();
// post a simple task and wait for result
boost::future&lt;int> fui = boost::asynchronous::post_future(scheduler, int_task());
int res = fui.get();
  </programlisting>
                <para>Of course this works with C++11 lambdas:</para>
                <programlisting>auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                            new boost::asynchronous::threadpool_scheduler&lt;
                                boost::asynchronous::lockfree_queue&lt;> >(3));
// post a simple task and wait for execution to complete
boost::future&lt;void> fuv = boost::asynchronous::post_future(scheduler, [](){std::cout &lt;&lt; "void lambda" &lt;&lt; std::endl;});
fuv.get();
// post a simple task and wait for result
boost::future&lt;int> fui = boost::asynchronous::post_future(scheduler, [](){std::cout &lt;&lt; "int lambda" &lt;&lt; std::endl;return 42;});
int res = fui.get();   </programlisting>
                <para>boost::asynchronous::post_future posts a piece of work to a threadpool
                    scheduler with 3 threads and using a lockfree_queue. We get a
                    boost::future&lt;the type of the task return type>.</para>
                <para>This looks like much std::async, but we're just getting started. Let's move on
                    to something more asynchronous.</para>
            </sect1>
            <sect1>
                <title>A servant proxy</title>
                <para>We now want to create a single-threaded scheduler, populate it with some
                    servant(s), and exercise some members of the servant from an outside thread. We
                    first need a servant:</para>
                <programlisting>struct Servant
{
    Servant(int data): m_data(data){}
    int doIt()const
    {
        std::cout &lt;&lt; "Servant::doIt with m_data:" &lt;&lt; m_data &lt;&lt; std::endl;
        return 5;
    }
    void foo(int&amp; i)const
    {
        std::cout &lt;&lt; "Servant::foo with int:" &lt;&lt; i &lt;&lt; std::endl;
        i = 100;
    }
    void foobar(int i, char c)const
    {
        std::cout &lt;&lt; "Servant::foobar with int:" &lt;&lt; i &lt;&lt; " and char:" &lt;&lt; c &lt;&lt;std::endl;
    }
    int m_data;
}; </programlisting>
                <para>We now create a proxy type to be used in other threads:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,<emphasis role="bold">Servant</emphasis>>
{
public:
    // forwarding constructor. Scheduler to servant_proxy, followed by arguments to Servant.
    template &lt;class Scheduler>
    ServantProxy(Scheduler s, int data):
        boost::asynchronous::servant_proxy&lt;ServantProxy,<emphasis role="bold">Servant</emphasis>>(s, data)
    {}
    // the following members must be available "outside"
    // foo and foobar, just as a post (no interesting return value)
    BOOST_ASYNC_POST_MEMBER(<emphasis role="bold">foo</emphasis>)
    BOOST_ASYNC_POST_MEMBER(<emphasis role="bold">foobar</emphasis>)
    // for doIt, we'd like a future
    BOOST_ASYNC_FUTURE_MEMBER(<emphasis role="bold">doIt</emphasis>)
};</programlisting>
                <para>Let's use our newly defined proxy:</para>
                <programlisting>int something = 3;
{
    auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::single_thread_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;> >);

    {
        // arguments (here 42) are forwarded to Servant's constructor
        ServantProxy proxy(scheduler,42);
        // post a call to foobar, arguments are forwarded.
        proxy.foobar(1,'a');
        // post a call to foo. To avoid races, the reference is ignored.
        proxy.foo(something);
        // post and get a future because we're interested in the result.
        boost::future&lt;int> fu = proxy.doIt();
        std::cout&lt;&lt; "future:" &lt;&lt; fu.get() &lt;&lt; std::endl;
    }// here, Servant's destructor is posted and waited for
}// scheduler is gone, its thread has been joined
std::cout&lt;&lt; "something:" &lt;&lt; something &lt;&lt; std::endl; // something was not changed as it was passed by value. You could use a boost::ref if this is not desired.</programlisting>
                <para>We can call members on the proxy, almost as if they were called on Servant.
                    The library takes care of the posting and forwarding the arguments. When
                    required, a future is returned. Stack unwinding works, and when the servant
                    proxy goes out of scope, the servant destructor is posted. When the scheduler
                    goes out of scope, its thread is stopped and joined. The queue is processed
                    completely first. Of course, as many servants as desired can be created in this
                    scheduler context. Please have a look at <link
                        xlink:href="examples/example_simple_servant.cpp">the complete
                    example</link>.</para>
            </sect1>
            <sect1>
                <title>Using a threadpool from within a servant</title>
                <para>If you remember the principles of Asynchronous, blocking a single-thread
                    scheduler is taboo as it blocks the thread doing all the management of a system.
                    But what to do when one needs to execute long tasks? Asynchronous provides a
                    whole set of threadpools. A servant posts something to a threadpool, provides a
                    callback, then gets a result. Wait a minute. Callback? Is this not
                    thread-unsafe? Why not threadpools with futures, like usual? Because in a
                    perfectly asynchronous world, waiting for a future means blocking a servant
                    scheduler. One would argue that it is possible not to block on the future, and
                    instead ask if there is a result. But frankly, polling is not a nice solution
                    either.</para>
                <para>And what about thread-safety? Asynchronous takes care of this. A callback is
                    never called from a threadpool, but instead posted back to the queue of the
                    scheduler which posted the work. All the servant has to do, is to do nothing and
                    wait until the callback is executed. Note that this is not the same as a
                    blocking wait, the servant can still react to events.</para>
                <para>Clearly, this brings some new challenges as the flow of control gets harder to
                    follow. This is why a servant is often written using state machines. The
                    (biased) author suggests to have a look at the <link
                        xlink:href="http://www.boost.org/doc/libs/1_59_0/libs/msm/doc/HTML/index.html"
                        > Meta State Machine library </link> , which plays nicely with
                    Asynchronous.</para>
                <para>But what about the usual proactor issues (crashes) when the servant has long
                    been destroyed when the callback is posted. Gone. Asynchronous <command
                        xml:id="trackable_servant"/><code>trackable_servant</code> post_callback
                    ensures that a callback is not called if the servant is gone. Better even, if
                    the servant has been destroyed, an unstarted posted task will not be
                    executed.</para>
                <para>What about another common issue? If one posts a task, say a lambda, which
                    captures a shared_ptr to an object per value, and this object is a
                    boost::signal? Then when the task object has been executed and is destroyed, one
                    could have a race on the signal deregistration. But again no. Asynchronous
                    ensures that a task created within a scheduler context gets destroyed in this
                    context.</para>
                <para>This is about the best protection one can get. What Asynchronous cannot
                    protect from are self-made races within a task (if you post a task with a
                    pointer to the servant, you're on your own and have to protect your servant). A
                    good rule of thumb is to consider data passed to a task as moved or passed by
                    value. To support this, Asynchronous does not copy tasks but moves them.</para>
                <para>Armed with these protections, let's give a try to a threadpool, starting with
                    the most basic one, <code>threadpool_scheduler</code> (more to come):</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;> scheduler)
        : boost::asynchronous::trackable_servant&lt;>(scheduler,
                                               // <emphasis role="bold">threadpool with 3 threads</emphasis> and a lockfree_queue
                                               boost::asynchronous::create_shared_scheduler_proxy(
                                                   new <emphasis role="bold">boost::asynchronous::<emphasis role="bold">threadpool_scheduler</emphasis></emphasis>&lt;
                                                           boost::asynchronous::lockfree_queue&lt;> >(<emphasis role="bold">3</emphasis>))){}
    // call to this is posted and executes in our (safe) single-thread scheduler
    void start_async_work()
    {
       //ok, let's post some work and wait for an answer
       <emphasis role="bold">post_callback</emphasis>(
                    [](){std::cout &lt;&lt; "Long Work" &lt;&lt; std::endl;}, // work, do not use "this" here
                    [/*this*/](boost::asynchronous::expected&lt;void>){...}// callback. Safe to use "this" as callback is only called if Servant is alive
        );
    }
};</programlisting>
                <para>We now have a servant, living in its own thread, which posts some long work to
                    a three-thread-threadpool and gets a callback, but only if still alive.
                    Similarly, the long work will be executed by the threadpool only if Servant is
                    alive by the time it starts. Everything else stays the same, one creates a proxy
                    for the servant and posts calls to its members, so we'll skip it for
                    conciseness, the complete example can be found <link
                        xlink:href="examples/example_post_trackable_threadpool.cpp"
                    >here</link>.</para>
            </sect1>
            <sect1>
                <title>A servant using another servant proxy</title>
                <para>Often, in a layered design, you'll need that a servant in a single-threaded
                    scheduler calls a member of a servant living in another one. And you'll want to
                    get a callback, not a future, because you absolutely refuse to block waiting for
                    a future (and you'll be very right of course!). Ideally, except for main(), you
                    won't want any of your objects to wait for a future. There is another
                    servant_proxy macro for this, <code>BOOST_ASYNC_UNSAFE_MEMBER</code>(unsafe
                    because you get no thread-safety from if and you'll take care of this yourself,
                    or better, <code>trackable_servant</code> will take care of it for you, as
                    follows):</para>
                <para>
                    <programlisting>// Proxy for a basic servant 
class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s, int data):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s, data)
    {}
    BOOST_ASYNC_UNSAFE_MEMBER(foo)
    BOOST_ASYNC_UNSAFE_MEMBER(foobar)
};   </programlisting>
                    <programlisting>// Servant using the first one
struct Servant2 : boost::asynchronous::trackable_servant&lt;>
{
    Servant2(boost::asynchronous::any_weak_scheduler&lt;> scheduler,ServantProxy worker)
        :boost::asynchronous::trackable_servant&lt;>(scheduler)
        ,m_worker(worker) // the proxy allowing access to Servant
    void doIt()    
    {                 
         call_callback(m_worker.get_proxy(), // Servant's outer proxy, for posting tasks
                       m_worker.foo(), // what we want to call on Servant
                      // callback functor, when done.
                      [](boost::asynchronous::expected&lt;int> result){...} );// expected&lt;return type of foo> 
    }
};</programlisting>
                </para>
                <para>Call of <code>foo()</code> will be posted to <code>Servant</code>'s scheduler,
                    and the callback lambda will be posted to <code>Servant2</code> when completed.
                    All this thread-safe of course. Destruction is also safe. When
                        <code>Servant2</code> goes out of scope, it will shutdown
                        <code>Servant</code>'s scheduler, then will his scheduler be shutdown
                    (provided no more object is living there), and all threads joined. The <link
                        xlink:href="examples/example_two_simple_servants.cpp">complete example
                    </link> shows a few more calls too.</para>
                <para>Asynchronous offers a different syntax to achieve the same result. Which one
                    you use is a matter of taste, both are equivalent. The second method is with
                    BOOST_ASYNC_MEMBER_UNSAFE_CALLBACK(_LOG if you need logging). It takes a
                    callback as argument, other arguments are forwarded. Combined with
                    <code>make_safe_callback</code>, one gets the same effect (safe call) as above.</para>
                <programlisting>// Proxy for a basic servant 
class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s, int data):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s, data)
    {}
    BOOST_ASYNC_MEMBER_UNSAFE_CALLBACK(foo) // say, foo takes an int as argument
};   </programlisting>
                <programlisting>// Servant using the first one
struct Servant2 : boost::asynchronous::trackable_servant&lt;>
{
    Servant2(boost::asynchronous::any_weak_scheduler&lt;> scheduler,ServantProxy worker)
        :boost::asynchronous::trackable_servant&lt;>(scheduler)
        ,m_worker(worker) // the proxy allowing access to Servant
    void doIt()    
    {                 
         m_worker.foo(make_safe_callback([](boost::asynchronous::expected&lt;void> res) // expected&lt;return type of foo> 
                                        {/* callback code*/}), 
                      42 /* arguments of foo*/); 
    }
};</programlisting>
            </sect1>
            <sect1>
                <title><command xml:id="interrupting_tasks"/>Interrupting tasks</title>
                <para>Let's imagine that a manager object (a state machine for example) posted some
                    long-lasting work to a threadpool, but this long-lasting work really takes too
                    long. As we are in an asynchronous world and non-blocking, the manager object
                    realizes there is a problem and decides the task must be stopped otherwise the
                    whole application starts failing some real-time constraints (how would we do if
                    we were blocked, waiting for a future?). This is made possible by using another
                    form of posting, getting a handle, on which one can require interruption. As
                    Asynchronous does not kill threads, we'll use one of Boost.Thread predefined
                    interruption points. Supposing we have well-behaved tasks, they will be
                    interrupted at the next interruption point if they started, or if they did not
                    start yet because they are waiting in a queue, then they will never start. In
                    this <link xlink:href="examples/example_interrupt.cpp">example</link>, we have
                    very little to change but the post call. We use <code>interruptible_post_callback</code>
                    instead of <code>post_callback</code>. We get an <code>any_interruptible object</code>, which offers a
                    single <code>interrupt()</code> member.</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
     ... // as usual
    void start_async_work()
    {
        // start long interruptible tasks
        // we get an interruptible handler representing the task
        <emphasis role="bold">boost::asynchronous::any_interruptible</emphasis> interruptible =
        <emphasis role="bold">interruptible_post_callback</emphasis>(
                // interruptible task
               [](){
                    std::cout &lt;&lt; "Long Work" &lt;&lt; std::endl;
                    boost::this_thread::sleep(boost::posix_time::milliseconds(1000));}, // sleep is an interrupting point
               // callback functor.
               [](boost::asynchronous::expected&lt;void> ){std::cout &lt;&lt; "Callback will most likely not be called" &lt;&lt; std::endl;}
        );
        // let the task start (not sure but likely)
        // if it had no time to start, well, then it will never.
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // actually, we changed our mind and want to interrupt the task
        interruptible.<emphasis role="bold">interrupt()</emphasis>;
        // the callback will likely never be called as the task was interrupted
    }
};                </programlisting>
            </sect1>
            <sect1>
                <title><command xml:id="logging_tasks"/>Logging tasks</title>
                <para>Developers are notoriously famous for being bad at guessing which part of
                    their code is inefficient. This is bad in itself, but even worse for a control
                    class like our post-callback servant as it reduces responsiveness. Knowing how
                    long a posted tasks or a callback lasts is therefore very useful. Knowing how
                    long take tasks executing in the threadpools is also essential to plan what
                    hardware one needs for an application(4 cores? Or 100?). We need to know what
                    our program is doing. Asynchronous provides logging per task to help there.
                    Let's have a look at some code. It's also time to start using our template
                    parameters for <code>trackable_servant</code>, in case you wondered why they are
                    here.</para>
                <programlisting>// we will be using loggable jobs internally
typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;
// the type of our log
typedef std::map&lt;std::string,std::list&lt;boost::asynchronous::diagnostic_item&lt;boost::chrono::high_resolution_clock> > > <emphasis role="bold">diag_type</emphasis>;

// we log our scheduler and our threadpool scheduler (both use servant_job)
struct Servant : boost::asynchronous::trackable_servant&lt;<emphasis role="bold">servant_job</emphasis>,<emphasis role="bold">servant_job</emphasis>>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;servant_job> scheduler) //servant_job is our job type
        : boost::asynchronous::trackable_servant&lt;<emphasis role="bold">servant_job,servant_job</emphasis>>(scheduler,
                                               boost::asynchronous::create_shared_scheduler_proxy(
                                                   // threadpool with 3 threads and a simple threadsafe_list queue
                                                   // Furthermore, it logs posted tasks
                                                   new boost::asynchronous::threadpool_scheduler&lt;
                                                           //servant_job is our job type
                                                           boost::asynchronous::lockfree_queue&lt; <emphasis role="bold">servant_job</emphasis> > >(3))){}
    void start_async_work()
    {
         post_callback(
               // task posted to threadpool
               [](){...}, // will return an int
               [](boost::asynchronous::expected&lt;int> res){...},// callback functor.
               // the task / callback name for logging
               <emphasis role="bold">"int_async_work"</emphasis>
        );
    }
    // we happily provide a way for the outside world to know what our threadpool did.
    // get_worker is provided by trackable_servant and gives the proxy of our threadpool
    diag_type get_diagnostics() const
    {
        return (*get_worker()).get_diagnostics();
    }
};</programlisting>
                <para>The proxy is also slightly different, using a _LOG macro and an argument
                    representing the name of the task.</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant,servant_job>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant,servant_job>(s)
    {}
    // the _LOG macros do the same as the others, but take an extra argument, the logged task name
    BOOST_ASYNC_FUTURE_MEMBER<emphasis role="bold">_LOG</emphasis>(start_async_work,<emphasis role="bold">"proxy::start_async_work"</emphasis>)
    BOOST_ASYNC_FUTURE_MEMBER<emphasis role="bold">_LOG</emphasis>(get_diagnostics,<emphasis role="bold">"proxy::get_diagnostics"</emphasis>)
};               </programlisting>
                <para> We now can get diagnostics from both schedulers, the single-threaded and the
                    threadpool (as external code has no access to it, we ask Servant to help us
                    there through a get_diagnostics() member).</para>
                <programlisting>// create a scheduler with logging
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                    boost::asynchronous::lockfree_queue&lt;servant_job> >);
// create a Servant                    
ServantProxy proxy(scheduler); 
...
// let's ask the single-threaded scheduler what it did.
diag_type single_thread_sched_diag = scheduler.get_diagnostics(); 
for (auto mit = single_thread_sched_diag.begin(); mit != single_thread_sched_diag.end() ; ++mit)
{
     std::cout &lt;&lt; "job type: " &lt;&lt; (*mit).first &lt;&lt; std::endl;
     for (auto jit = (*mit).second.begin(); jit != (*mit).second.end();++jit)
     {
          std::cout &lt;&lt; "job waited in us: " &lt;&lt; boost::chrono::nanoseconds((*jit).get_started_time() - (*jit).<emphasis role="bold">get_posted_time()</emphasis>).count() / 1000 &lt;&lt; std::endl;
          std::cout &lt;&lt; "job lasted in us: " &lt;&lt; boost::chrono::nanoseconds((*jit).get_finished_time() - (*jit).<emphasis role="bold">get_started_time()</emphasis>).count() / 1000 &lt;&lt; std::endl;
          std::cout &lt;&lt; "job interrupted? "  &lt;&lt; std::boolalpha &lt;&lt; (*jit).<emphasis role="bold">is_interrupted()</emphasis> &lt;&lt; std::endl;
          std::cout &lt;&lt; "job failed? "  &lt;&lt; std::boolalpha &lt;&lt; (*jit).<emphasis role="bold">is_failed()</emphasis> &lt;&lt; std::endl; // did this job throw an exception?
     }
}              </programlisting>
                <para>It goes similarly with the threapool scheduler, with the slight difference
                    that we ask the Servant to deliver diagnostic information through a proxy
                    member. The <link xlink:href="examples/example_log.cpp">complete example</link>
                    shows all this, plus an interrupted job.</para>                
            </sect1>
            <sect1>
                <title>Generating HTML diagnostics</title>
                <para>We just saw how to programmatically get diagnostics from schedulers. This is
                    very useful, but nobody likes to do it manually, so the authors went the extra
                    mile and provide an HTML formatter for convenience. The <link
                        xlink:href="examples/example_html_diagnostics.cpp">included example</link>
                    shows how to use it. In this example, we have a Servant, living in its own
                    single-threaded scheduler called "Servant". It uses a threadpool call
                    "Threadpool". When the Servant's foo() method is called, it executes a
                    parallel_reduce(parallel_for(...)), or whatever you like. These operations are
                    named accordingly. We also create a third scheduler, called "Formatter
                    scheduler", which will be used by the formatter code. Yes, even this scheduler
                    will be logged too. The example creates a Servant, calls foo() on the proxy,
                    sleeps for a while (how long is passed to the example as argument), then
                    generates <link xlink:href="examples/in_progress.html">a first output
                        statistics</link>. Depending on the sleep time, the parallel work might or
                    might not be finished, so this is an intermediate result.</para>
                <para>We then wait for the tasks to finish, destroy the servant, so that its
                    destructor is logged too, and we generate a <link
                        xlink:href="examples/final.html">final diagnostics</link>.</para>
                <para>The HTML pages display the statistics for all schedulers, including the
                    formatter. It shows with different colors the waiting times of tasks (called
                    Scheduling time), the execution times, successful or failed separately, and the
                    added total time for each task, with max min, average duration. One can also
                    display the full list of tasks and even histograms. As this is a lot of
                    information, it is possible to hide part of it using checkboxes.</para>
                <para>One also gets the very useful information of how long are the different
                    scheduler queues, which gives a very good indication of how busy the system
                    is.</para>
            </sect1>
            <sect1>
                <title>Queue container with priority</title>
                <para>Sometimes, all jobs posted to a scheduler do not have the same priority. For
                    threadpool schedulers, <code>composite_threadpool_scheduler</code> is an option.
                    For a single-threaded scheduler, Asynchronous does not provide a priority queue
                    but a queue container, which itself contains any number of queues, of different
                    types if needed. This has several advantages:<itemizedlist>
                        <listitem>
                            <para>Priority is defined simply by posting to the queue with the
                                desired priority, so there is no need for expensive priority
                                algorithms.</para>
                        </listitem>
                        <listitem>
                            <para>Reduced contention if many threads of a threadpool post something
                                to the queue of a single-threaded scheduler. If no priority is
                                defined, one queue will be picked, according to a configurable
                                policy, reducing contention on a single queue.</para>
                        </listitem>
                        <listitem>
                            <para>It is possible to mix queues.</para>
                        </listitem>
                        <listitem>
                            <para>It is possible to build a queue container of queue containers,
                                etc.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Note: This applies to any scheduler. We'll start with single-threaded
                    schedulers used by managing servants for simplicity, but it is possible to have
                    composite schedulers using queue containers for finest granularity and least
                    contention.</para>
                <para>First, we need to create a single-threaded scheduler with several queues for
                    our servant to live in, for example, one threadsafe list and three lockfree
                    queues:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
                           boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                        boost::asynchronous::any_queue_container&lt;> >
                        (boost::asynchronous::any_queue_container_config&lt;boost::asynchronous::<emphasis role="bold">threadsafe_list</emphasis>&lt;> >(<emphasis role="bold">1</emphasis>),
                         boost::asynchronous::any_queue_container_config&lt;boost::asynchronous::<emphasis role="bold">lockfree_queue</emphasis>&lt;> >(<emphasis role="bold">3</emphasis>,100)
                         ));</programlisting>
                <para><code>any_queue_container</code> takes as constructor arguments a variadic
                    sequence of <code>any_queue_container_config</code>, with a queue type as
                    template argument, and in the constructor the number of objects of this queue
                    (in the above example, one <code>threadsafe_list</code> and 3
                        <code>lockfree_queue</code> instances, then the parameters that these queues
                    require in their constructor (100 is the capacity of the underlying
                        <code>boost::lockfree_queue</code>). This means, that our
                        <code>single_thread_scheduler</code> has 4 queues:<itemizedlist>
                        <listitem>
                            <para>a threadsafe_list at index 1</para>
                        </listitem>
                        <listitem>
                            <para>lockfree queues at indexes 2,3,4</para>
                        </listitem>
                        <listitem>
                            <para>>= 4 means the queue with the least priority.</para>
                        </listitem>
                        <listitem>
                            <para>0 means "any queue" and is the default</para>
                        </listitem>
                    </itemizedlist></para>
                <para>The scheduler will handle these queues as having priorities: as long as there
                    are work items in the first queue, take them, if there are no, try in the
                    second, etc. If all queues are empty, the thread gives up his time slice and
                    sleeps until some work item arrives. If no priority is defined by posting, a
                    queue will be chosen (by default randomly, but this can be configured with a
                    policy). This has the advantage of reducing contention of the queue, even when
                    not using priorities. The servant defines the priority of the tasks it provides.
                    While this might seem surprising, it is a design choice to avoid that the coder
                    using a servant proxy interface would have to think about it, as you will see in
                    the second listing. To define a priority for a servant proxy, there is a second
                    field in the macros:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s)
    {}
    <emphasis role="bold">BOOST_ASYNC_SERVANT_POST_CTOR(3)</emphasis>
    <emphasis role="bold">BOOST_ASYNC_SERVANT_POST_DTOR(4)</emphasis>
    BOOST_ASYNC_FUTURE_MEMBER(start_async_work,<emphasis role="bold">1</emphasis>)
};</programlisting>
                <para>BOOST_ASYNC_FUTURE_MEMBER and other similar macros can be given an optional
                    priority parameter, in this case 1, which is our threadsafe list. Notice how you
                    can then define the priority of the posted servant constructor and
                    destructor.</para>
                <programlisting>ServantProxy proxy(scheduler);
boost::future&lt;boost::future&lt;int>> fu = proxy.start_async_work();</programlisting>
                <para>Calling our proxy member stays unchanged because the macro defines the
                    priority of the call.</para>
                <para>We also have an extended version of <code>post_callback</code>, called by a
                    servant posting work to a threadpool:</para>
                <programlisting>post_callback(
       [](){return 42;},// work
       [this](boost::asynchronous::expected&lt;int> res){}// callback functor.
       ,"",
       <emphasis role="bold">2</emphasis>, // work prio
       <emphasis>2</emphasis>  // callback prio
);</programlisting>
                <para>Note the two added priority values: the first one for the task posted to the
                    threadpool, the second for the priority of the callback posted back to the
                    servant scheduler. The string is the log name of the task, which we choose to
                    ignore here.</para>
                <para>The priority is in any case an indication, the scheduler is free to ignore it
                    if not supported. In the <link xlink:href="examples/example_queue_container.cpp"
                        >example</link>, the single threaded scheduler will honor the request, but
                    the threadpool has a normal queue and cannot honor the request, but a threadpool
                    with an <code>any_queue_container</code> or a
                        <code>composite_threadpool_scheduler</code> can. The <link
                        xlink:href="examples/example_queue_container_log.cpp">same example</link>
                    can be rewritten to also support logging.</para>
                <para><code>any_queue_container</code> has two template arguments. The first, the
                    job type, is as always by default, a callable (<code>any_callable</code>) job.
                    The second is the policy which Asynchronous uses to find the desired queue for a
                    job. The default is <code>default_find_position</code>, which is as described
                    above, 0 means any position, all other values map to a queue, priorities >=
                    number of queues means last queue. Any position is by default random
                        (<code>default_random_push_policy</code>), but you might pick
                        <code>sequential_push_policy</code>, which keeps an atomic counter to post
                    jobs to queues in a sequential order.</para>
                <para>If you plan to build a queue container of queue containers, you'll probably
                    want to provide your own policy.</para>
            </sect1>
            <sect1>
                <title>Multiqueue Schedulers' priority</title>
                <para>A multiqueue_... threadpool scheduler has a queue for each thread. This
                    reduces contention, making these faster than single queue schedulers, like
                    threadpool_scheduler. Furthermore, these schedulers support priority: the
                    priority given in post_future or post_callback is the (1-based) position of the
                    queue we want to post to. 0 means "any queue". A queue of priority 1 has a
                    higher priority than a queue with priority 2, etc. </para>
                <para>Each queue is serving one thread, but threads steal from each other's queue,
                    according to the priority.</para>
            </sect1>
            <sect1>
                <title>Threadpool Schedulers with several queues</title>
                <para>A queue container has advantages (different queue types, priority for single
                    threaded schedulers) but also disadvantages (takes jobs from one end of the
                    queue, which means potential cache misses, more typing work). If you don't need
                    different queue types for a threadpool but want to reduce contention, multiqueue
                    schedulers are for you. A normal <code>threadpool_scheduler</code> has x threads
                    and one queue, serving them. A <code>multiqueue_threadpool_scheduler</code> has
                    x threads and x queues, each serving a worker thread. Each thread looks for work
                    in its queue. If it doesn't find any, it looks for work in the previous one,
                    etc. until it finds one or inspected all the queues. As all threads steal from
                    the previous queue, there is little contention. The construction of this
                    threadpool is very similar to the simple
                    <code>threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::create_shared_scheduler_proxy(
                // 4 threads and 4 lockfree queues of 10 capacity
                new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> >(4,10));</programlisting>
                <para>The first argument is the number of worker threads, which is at the same time
                    the number of queues. As for every scheduler, if the queue constructor takes
                    arguments, they come next and are forwarded to the queue.</para>
                <para>This is the <emphasis role="underline">advised</emphasis> scheduler for
                    standard cases as it offers lesser contention and task stealing between the
                    queues it uses for task transfer.</para>
                <para><emphasis role="italic">Limitation:</emphasis> these schedulers cannot have 0
                    thread like their single-queue counterparts.</para>
            </sect1>
            <sect1>
                <title>Composite Threadpool Scheduler</title>
                <sect2>
                    <title>Usage</title>
                
                <para>When a project becomes more complex, having a single threadpool for the whole
                    application does not offer enough flexibility in load planning. It is pretty
                    hard to avoid either oversubscription (more busy threads than available hardware
                    threads) or undersubscription. One would need one big threadpool with exactly
                    the number of threads available in the hardware. Unfortunately, if we have a
                    hardware with, say 12 hardware threads, parallelizing some work using all 12
                    might be slowlier than using only 8. One would need different threadpools of
                    different number of threads for the application. This, however, has the serious
                    drawback that there is a risk that some threadpools will be in overload, while
                    others are out of work unless we have work stealing between different
                    threadpools.</para>
                <para>The second issue is task priority. One can define priorities with several
                    queues or a queue container, but this ensures that only highest priority tasks
                    get executed if the system is coming close to overload. Ideally, it would be
                    great if we could decide how much compute power we give to each task
                    type.</para>
                <para>This is what <code>composite_threadpool_scheduler</code> solves. This pool
                        supports, like any other pool, the
                        <code>any_shared_scheduler_proxy</code>concept so you can use it in place of
                        the ones we used so far. The pool is composed of other pools
                            (<code>any_shared_scheduler_proxy</code> pools). It implements work
                        stealing between pools if a) the pools support it and b) the queue of a pool
                        also does. For example, we can create the following worker pool made of 3
                        sub-pools:</para>
                <para>
                    <programlisting>// create a composite threadpool made of:
// a multiqueue_threadpool_scheduler, 1 thread, with a lockfree_queue of capacity 100. 
// This scheduler does not steal from other schedulers, but will lend its queue for stealing
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp = boost::asynchronous::create_shared_scheduler_proxy( 
               new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> > (1,100));

// a stealing_multiqueue_threadpool_scheduler, 3 threads, each with a threadsafe_list
// this scheduler will steal from other schedulers if it can. In this case it will manage only with tp, not tp3
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp2 = boost::asynchronous::create_shared_scheduler_proxy( 
                    new boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;boost::asynchronous::threadsafe_list&lt;> > (3));

// a multiqueue_threadpool_scheduler, 4 threads, each with a lockfree_spsc_queue of capacity 100
// this is safe because there will be no stealing as the queue does not support it, and only the servant single-thread scheduler will be the producer
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp3 = boost::asynchronous::create_shared_scheduler_proxy( 
               new boost::asynchronous::multiqueue_threadpool_scheduler&lt;boost::asynchronous::lockfree_spsc_queue&lt;> > (4,100));

// create a composite pool made of the 3 previous ones
boost::asynchronous::any_shared_scheduler_proxy&lt;> tp_worker =
             boost::make_shared&lt;boost::asynchronous::composite_threadpool_scheduler&lt;> > (tp,tp2,tp3);
                    </programlisting>
                </para>
                <para>We can use this pool:<itemizedlist>
                        <listitem>
                            <para>As a big worker pool. In this case, the priority argument we use
                                for posting refers to the (1-based) index of the subpool
                                (post_callback(func1,func2,"task name",<emphasis role="bold"
                                    >1</emphasis>,0);). "1" means post to the first pool. But
                                another pool could steal the work.</para>
                        </listitem>
                        <listitem>
                            <para>As a pool container, but different parts of the code will get to
                                see only the subpools. For example, the pools tp, tp2 and tp3 can
                                still be used independently as a worker pool. Calling
                                composite_threadpool_scheduler&lt;>::get_scheduler(std::size_t
                                index_of_pool) will also give us the corresponding pool (1-based, as
                                always).</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Another example of why to use this pool is reusing threads allocated to an
                        asio-based communication for helping other schedulers. Addng an asio
                        scheduler to a composite pool will allow the threads of this scheduler to
                        help (steal) other pools when no communication is currently happening. </para>
                <para>Stealing is done with priority. A stealing pool first tries to steal from the
                        first pool, then from the second, etc.</para>
                <para>The <link xlink:href="examples/example_composite_threadpool.cpp">following
                        example</link> shows a complete servant implementation, and the <command
                        xlink:href="#asio_scheduler">ASIO section</command> will show how an ASIO
                    pool can steal.</para>
                <para>The threadpool schedulers we saw so far are not stealing from other pools. The
                        single-queue schedulers are not stealing, and the multiqueue schedulers
                        steal from the queues of other threads of the same pool. The
                        scheduler-stealing schedulers usually indicate this by appending a
                            <code>stealing_</code> to their name:<itemizedlist>
                            <listitem>
                                <para><code>stealing_threadpool_scheduler</code> is a
                                        <code>threadpool_scheduler</code> which steals from other
                                    pools.</para>
                            </listitem>
                            <listitem>
                                <para><code>stealing_multiqueue_threadpool_scheduler</code> is a
                                        <code>multiqueue_threadpool scheduler</code> which steals
                                    from other pools.</para>
                            </listitem>
                            <listitem>
                                <para><code>asio_scheduler steals</code>.</para>
                            </listitem>
                        </itemizedlist></para>
                <para>The only difference with their not stealing equivalent is that they steal from
                        other schedulers. To achieve this, they need a composite_scheduler to tell
                        them from which schedulers they can steal.</para>
                <para>Not all schedulers offer to be stolen from. A
                            <code>single_thread_scheduler</code> does not as it would likely bring
                        race conditions to active objects.</para>
                <para>Another interesting usage will be when planning for extra machines to help a
                        threadpool by processing some of the work: work can be stolen from a
                        threadpool by a <command xlink:href="#distributing"
                            >tcp_server_scheduler</command> from which other machines can get it.
                        Just pack both pools in a <code>composite_threadpool_scheduler</code> and
                        you're ready to go.</para>
                </sect2>
                <sect2>
                    <title>Priority</title>
                    <para>A composite supports priority. The first pool passed in the constructor of
                        the composite pool has priority 1, the second 2, etc. 0 means "any pool" and
                        n where n > number of pools will me modulo-ed.</para>
                    <para>Posting to this scheduler using post_future or post_callback using a given
                        priority will post to the according pool. If a pool supports stealing from
                        other pools (stealing_... pools), it will try to steal from other pools,
                        starting with the highest priority, but only if the to be stolen from pools
                        supports it. For example, we try to post to the first pool, callback to any
                        queue.</para>
                    <programlisting>post_callback(
               [](){},// work
               [this](boost::asynchronous::expected&lt;int>){},// callback functor.
               "", // task and callback name
               1,  // work priority, highest
               0   // callback anywhere
);</programlisting>
                </sect2>
            </sect1>
            <sect1>
                <title>More flexibility in dividing servants among threads</title>
                <para>TODO example and code. We saw how to assign a servant or several servants to a
                    single thread scheduler. We can also create schedulers and divide servants among
                    them. This is very powerful but still has some constraints:<itemizedlist>
                        <listitem>
                            <para>We need to assign servants to schedulers while what we want is to
                                assign them to threads. We also have to consider how many schedulers
                                to create. This is not very flexible.</para>
                        </listitem>
                        <listitem>
                            <para>If a servant is taking too long, it blocks all other servants
                                living inside this thread context. This increases latency.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>We can increase the flexibility and reduce latency by using a
                        <code>multiple_thread_scheduler</code>. This scheduler takes as first
                    argument a number of threads to use and a maximum number of client "worlds"
                    (clients living logically in the same thread context). What it does, is to
                    assign any of its threads to different client worlds, but only one thread can
                    service a world at a time. This means that the thread safety of servants is
                    preserved. At the same time, having any number of threads decreases latency
                    because if a servant keeps its thread busy, it does not block other servants
                    from being serviced. As we can choose the number of threads this scheduler will
                    use, we achieve very fine granularity in planing our thread resources.</para>
                <para>Another interesting characteristics of this scheduler is that its threads
                    service its servants in order. If a thread serviced servant x, it next tries to
                    service servant x+1. This makes for good pipelining capabilities as it increases
                    the odds that task is koved from a pipeline stage to the next one by the same
                    thread and will be hot in its cache.</para>
            </sect1>
            <sect1>
                <title>Processor binding</title>
                <para>TODO example and code.On many systems, it can improve performance to bind
                    threads to a processor: better cache usage is likely as the OS does not move
                    threads from core to core. Mostly for threadpools this is an option you might
                    want to try.</para>
                <para>Usage is very simple. One needs to call
                        <code>processor_bind(core_index)</code> on a scheduler proxy. This function
                    takes a single argument, the core to which the first thread of the pool will be
                    bound. The second thread will be bound to core+1, etc.</para>
            </sect1>
            <sect1>
                <title><command xml:id="asio_scheduler"/>asio_scheduler</title>
                <para>Asynchronous supports the possibility to use Boost.Asio as a threadpool
                    provider. This has several advantages:<itemizedlist>
                        <listitem>
                            <para>asio_scheduler is delivered with a way to access Asio's io_service
                                from a servant object living inside the scheduler.</para>
                        </listitem>
                        <listitem>
                            <para>asio_scheduler handles the necessary work for creating a pool of
                                threads for multithreaded-multi-io_service communication.</para>
                        </listitem>
                        <listitem>
                            <para>asio_scheduler threads implement work-stealing from other
                                Asynchronous schedulers. This allows communication threads to help
                                other threadpools when no I/O communication is happening. This helps
                                reducing thread oversubscription.</para>
                        </listitem>
                        <listitem>
                            <para>One has all the usual goodies of Asynchronous: safe callbacks,
                                object tracking, servant proxies, etc.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Let's create a simple but powerful example to illustrate its usage. We want to
                    create a TCP client, which connects several times to the same server, gets data
                    from it (in our case, the Boost license will do), then checks if the data is
                    coherent by comparing the results two-by-two. Of course, the client has to be
                    perfectly asynchronous and never block. We also want to guarantee some threads
                    for the communication and some for the calculation work. We also want to
                    communication threads to "help" by stealing some work if necessary.</para>
                <para>Let's start by creating a TCP client using Boost.Asio. A slightly modified
                    version of the async TCP client from the Asio documentation will do. All we
                    change is pass it a callback which it will call when the requested data is
                    ready. We now pack it into an Asynchronous trackable servant:</para>
                <programlisting>// Objects of this type are made to live inside an asio_scheduler,
// they get their associated io_service object from Thread Local Storage
struct AsioCommunicationServant : boost::asynchronous::trackable_servant&lt;>
{
    AsioCommunicationServant(boost::asynchronous::any_weak_scheduler&lt;> scheduler,
                             const std::string&amp; server, const std::string&amp; path)
        : boost::asynchronous::trackable_servant&lt;>(scheduler)
        , m_client(*<emphasis role="bold">boost::asynchronous::get_io_service&lt;>()</emphasis>,server,path)
    {}
    void test(std::function&lt;void(std::string)> cb)
    {
        // just forward call to asio asynchronous http client
        // the only change being the (safe) callback which will be called when http get is done
        m_client.request_content(cb);
    }
private:
    client m_client; //client is from Asio example
};</programlisting>
                <para>The main noteworthy thing to notice is the call to <emphasis role="bold"
                        >boost::asynchronous::get_io_service&lt;>()</emphasis>, which, using
                    thread-local-storage, gives us the io_service associated with this thread (one
                    io_service per thread). This is needed by the Asio TCP client. Also noteworthy
                    is the argument to <code>test()</code>, a callback when the data is available. </para>
                <para>Wait a minute, is this not unsafe (called from an asio worker thread)? It is
                    but it will be made safe in a minute.</para>
                <para>We now need a proxy so that this communication servant can be safely used by
                    others, as usual:</para>
                <programlisting>class AsioCommunicationServantProxy: public boost::asynchronous::servant_proxy&lt;AsioCommunicationServantProxy,AsioCommunicationServant >
{
public:
    // ctor arguments are forwarded to AsioCommunicationServant
    template &lt;class Scheduler>
    AsioCommunicationServantProxy(Scheduler s,const std::string&amp; server, const std::string&amp; path):
        boost::asynchronous::servant_proxy&lt;AsioCommunicationServantProxy,AsioCommunicationServant >(s,server,path)
    {}
    // we offer a single member for posting
    BOOST_ASYNC_POST_MEMBER(test)
};                   </programlisting>
                <para>A single member, <code>test</code>, is used in the proxy. The constructor
                    takes the server and relative path to the desired page. We now need a manager
                    object, which will trigger the communication, wait for data, check that the data
                    is coherent:</para>
                <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
    Servant(boost::asynchronous::any_weak_scheduler&lt;> scheduler,const std::string&amp; server, const std::string&amp; path)
        : boost::asynchronous::trackable_servant&lt;>(scheduler)
        , m_check_string_count(0)
    {
        // as worker we use a simple threadpool scheduler with 4 threads (0 would also do as the asio pool steals)
        auto worker_tp = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;> > (4));

        // for tcp communication we use an asio-based scheduler with 3 threads
        auto asio_workers = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::asio_scheduler&lt;>(3));

        // we create a composite pool whose only goal is to allow asio worker threads to steal tasks from the threadpool
        m_pools = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::composite_threadpool_scheduler&lt;> (worker_tp,asio_workers));

        set_worker(worker_tp);
        // we create one asynchronous communication manager in each thread
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
        m_asio_comm.push_back(AsioCommunicationServantProxy(asio_workers,server,path));
    }
... //to be continued                 
                </programlisting>
                <para>We create 3 pools:<itemizedlist>
                        <listitem>
                            <para>A worker pool for calculations (page comparisons)</para>
                        </listitem>
                        <listitem>
                            <para>An asio threadpool with 3 threads in which we create 3
                                communication objects.</para>
                        </listitem>
                        <listitem>
                            <para>A composite pool which binds both pools together into one stealing
                                unit. You could even set the worker pool to 0 thread, in which case
                                the worker will get its work done when the asio threads have nothing
                                to do. Only non- multiqueue schedulers support this. The worker pool
                                is now made to be the worker pool of this object using
                                    <code>set_worker()</code>.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>We then create our communication objects inside the asio pool.</para>
                <para><emphasis role="underline">Note</emphasis>: asio pools can steal from other
                    pools but not be stolen from. Let's move on to the most interesting part:</para>
                <programlisting>void get_data()
{
    // provide this callback (executing in our thread) to all asio servants as task result. A string will contain the page
    std::function&lt;void(std::string)> f =            
...
    m_asio_comm[0].test(make_safe_callback(f));
    m_asio_comm[1].test(make_safe_callback(f));
    m_asio_comm[2].test(make_safe_callback(f));
}</programlisting>
                <para>We skip the body of f for the moment. f is a task which will be posted to each
                    communication servant so that they can do the same work:<itemizedlist>
                        <listitem>
                            <para>call the same http get on an asio servants</para>
                        </listitem>
                        <listitem>
                            <para>at each callback, check if we got all three callbacks</para>
                        </listitem>
                        <listitem>
                            <para>if yes, post some work to worker threadpool, compare the returned
                                strings (should be all the same)</para>
                        </listitem>
                        <listitem>
                            <para>if all strings equal as they should be, cout the page</para>
                        </listitem>
                    </itemizedlist></para>
                <para>All this will be doine in a single functor. This functor is passed to each
                    communication servant, packed into a make_safe_callback, which, as its name
                    says, transforms the unsafe functor into one which posts this callback functor
                    to the manager thread and also tracks it to check if still alive at the time of
                    the callback. By calling <code>test()</code>, we trigger the 3 communications,
                    and f will be called 3 times. The body of f is:</para>
                <programlisting>std::function&lt;void(std::string)> f =
                [this](std::string s)
                {
                   this->m_requested_data.push_back(s);
                   // poor man's state machine saying we got the result of our asio requests :)
                   if (this->m_requested_data.size() == 3)
                   {
                       // ok, this has really been called for all servants, compare.
                       // but it could be long, so we will post it to threadpool
                       std::cout &lt;&lt; "got all tcp data, parallel check it's correct" &lt;&lt; std::endl;
                       std::string s1 = this->m_requested_data[0];
                       std::string s2 = this->m_requested_data[1];
                       std::string s3 = this->m_requested_data[2];
                       // this callback (executing in our thread) will be called after each comparison
                       auto cb1 = [this,s1](boost::asynchronous::expected&lt;bool> res)
                       {
                          if (res.get())
                              ++this->m_check_string_count;
                          else
                              std::cout &lt;&lt; "uh oh, the pages do not match, data not confirmed" &lt;&lt; std::endl;
                          if (this->m_check_string_count ==2)
                          {
                              // we started 2 comparisons, so it was the last one, data confirmed
                              std::cout &lt;&lt; "data has been confirmed, here it is:" &lt;&lt; std::endl;
                              std::cout &lt;&lt; s1;
                          }
                       };
                       auto cb2=cb1;
                       // post 2 string comparison tasks, provide callback where the last step will run
                       this->post_callback([s1,s2](){return s1 == s2;},std::move(cb1));
                       this->post_callback([s2,s3](){return s2 == s3;},std::move(cb2));
                   }
                };        
                </programlisting>
                <para> We start by checking if this is the third time this functor is called (this,
                    the manager, is nicely serving as holder, kind of poor man's state machine
                    counting to 3). If yes, we prepare a call to the worker pool to compare the 3
                    returned strings 2 by 2 (cb1, cb2). Again, simple state machine, if the callback
                    is called twice, we are done comparing string 1 and 2, and 2 and 3, in which
                    case the page is confirmed and cout'ed. The last 2 lines trigger the work and
                    post to our worker pool (which is the threadpool scheduler, or, if stealing
                    happens, the asio pool) two comparison tasks and the callbacks.</para>
                <para>Our manager is now ready, we still need to create for it a proxy so that it
                    can be called from the outside world asynchronously, then create it in its own
                    thread, as usual:</para>
                <programlisting>class ServantProxy : public boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>
{
public:
    template &lt;class Scheduler>
    ServantProxy(Scheduler s,const std::string&amp; server, const std::string&amp; path):
        boost::asynchronous::servant_proxy&lt;ServantProxy,Servant>(s,server,path)
    {}
    // get_data is posted, no future, no callback
    BOOST_ASYNC_POST_MEMBER(get_data)
};
...              
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::threadsafe_list&lt;> >);
{
   ServantProxy proxy(scheduler,"www.boost.org","/LICENSE_1_0.txt");
   // call member, as if it was from Servant
   proxy.get_data();
   // if too short, no problem, we will simply give up the tcp requests
   // this is simply to simulate a main() doing nothing but waiting for a termination request
   boost::this_thread::sleep(boost::posix_time::milliseconds(2000));
}
                </programlisting>
                <para> As usual, <link xlink:href="examples/example_asio_http_client.cpp">here the
                        complete, ready-to-use example</link> and the implementation of the <link
                        xlink:href="examples/asio/asio_http_async_client.hpp">Boost.Asio HTTP
                        client</link>. </para>
            </sect1>
            <sect1>
                <title>Timers</title>
                <para>Very often, an Active Object servant acting as an asynchronous dispatcher will
                    post tasks which have to be done until a certain point in the future, or which
                    will start only at a later point. State machines also regularly make use of a
                    "time" event.</para>
                <para>For this we need a timer, but a safe one:<itemizedlist>
                        <listitem>
                            <para>The timer callback has to be posted to the Active Object thread to
                                avoid races.</para>
                        </listitem>
                        <listitem>
                            <para>The timer callback shall not be called if the servant making the
                                request has been deleted (it can be an awfully long time until the
                                callback).</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Asynchronous itself has no timer, but Boost.Asio does, so the library provides
                    a wrapper around it and will allow us to create a timer using an
                    asio::io_service running in its own thread or in an asio threadpool, provided by
                    the library.</para>
                <sect2>
                    <title>Constructing a timer</title>
                    <para>One first needs an <code>asio_scheduler</code> with at least one
                        thread:</para>
                    <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> asio_sched = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::asio_scheduler&lt;>(1));               
                    </programlisting>
                    <para>The Servant living in its ActiveObject thread then creates a timer (as
                        attribute to keep it alive) using this scheduler and a timer value:</para>
                    <programlisting> boost::asynchronous::asio_deadline_timer_proxy m_timer (asio_sched,boost::posix_time::milliseconds(1000));                   
                    </programlisting>
                    <para>It can now start the timer using <code>trackable_servant</code> (its base
                            class)<code>::async_wait</code>, passing it a functor call when timer
                        expires / is cancelled:</para>
                    <programlisting> async_wait(m_timer,
            [](const ::boost::system::error_code&amp; err)
            {
                std::cout &lt;&lt; "timer expired? "&lt;&lt; std::boolalpha &lt;&lt; (bool)err &lt;&lt; std::endl; //true if expired, false if cancelled
            } 
            );                  </programlisting>
                    <para>Canceling or recreating the timer means destroying (and possibly
                        recreating) the timer object:</para>
                    <programlisting> m_timer =  boost::asynchronous::asio_deadline_timer_proxy(get_worker(),boost::posix_time::milliseconds(1000));                                   
                    </programlisting>
                    <para>Alternatively, asio_deadline_timer_proxy offers a reset(duration) member,
                        which is more efficient than recreating a proxy. The <link
                            xlink:href="examples/example_asio_deadline_timer.cpp">following example
                        </link> displays a servant using an asio scheduler as a thread pool and
                        creating there its timer object. Note how the timer is created using the
                        worker scheduler of its owner.</para>
                </sect2>
            </sect1>
            <sect1>
                <title><command xml:id="callback_continuations"/>Continuation tasks</title>
                <para>A common limitation of threadpools is support for recursive tasks: tasks start
                    other tasks, which start other tasks and wait for them to complete to do a merge
                    of the part-results. Unfortunately, all threads in the threadpool will soon be
                    busy waiting and no task will ever complete. One can achieve this with a
                    controller object or state machine in a single-threaded scheduler waiting for
                    callbacks, but for very small tasks, using callbacks might just be too
                    expensive. In such cases, Asynchronous provides continuations: a task executes,
                    does something then creates a continuation which will be excuted as soon as all
                    child tasks complete.</para>
                <sect2>
                    <title>General</title>
                    <para>The Hello World of recursive tasks is a parallel fibonacci. The naive
                        algorithm creates a task calculating fib(n). For this it will start a fib(n-1)
                        and fib(n-2) and block until both are done. These tasks will start more tasks,
                        etc. until a cutoff number, at which point recursion stops and fibonacci is
                        calculated serially. This approach has some problems: to avoid thread explosion,
                        we would need fibers, which are not available in Boost at the time of this
                        writing. Even with fibers, tasks would block, which means interrupting them is
                        not possible, and a stack will have to be paid for both. Performance will also
                        suffer. Furthermore, blocking simply isn't part of the asynchronous philosophy
                        of the library. Let's have a look how callback continuation tasks let us
                        implement a parallel fibonacci.</para>
                    <para>First of all, we need a serial fibonacci when n is less than the cutoff. This
                        is a classical one:</para>
                    <programlisting> long serial_fib( long n ) {
    if( n&lt;2 )
        return n;
    else
        return serial_fib(n-1)+serial_fib(n-2);
}</programlisting>
                    <para> We now need a recursive-looking fibonacci task: </para>
                    <programlisting>// our recursive fibonacci tasks. Needs to inherit continuation_task&lt;value type returned by this task>
struct fib_task : public <emphasis role="bold">boost::asynchronous::continuation_task&lt;long></emphasis>
{
    fib_task(long n,long cutoff):n_(n),cutoff_(cutoff){}
    // called inside of threadpool
    void operator()()const
    {
        // the result of this task, will be either set directly if &lt; cutoff, otherwise when taks is ready
        boost::asynchronous::<emphasis role="bold">continuation_result&lt;long> task_res = this_task_result()</emphasis>;
        if (n_&lt;cutoff_)
        {
            // n &lt; cutoff => execute immediately
            task_res.set_value(serial_fib(n_));
        }
        else
        {
            // n>= cutoff, create 2 new tasks and when both are done, set our result (res(task1) + res(task2))
            boost::asynchronous::<emphasis role="bold">create_callback_continuation</emphasis>(
                        // called when subtasks are done, set result of the calling task
                        [task_res](std::tuple&lt;boost::asynchronous::expected&lt;long>,boost::asynchronous::expected&lt;long> > res) mutable
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get();
                            task_res.set_value(r);
                        },
                        // recursive tasks
                        fib_task(n_-1,cutoff_),
                        fib_task(n_-2,cutoff_));
        }
    }
    long n_;
    long cutoff_;
};             </programlisting>
                    <para> Our task need to inherit
                        <code>boost::asynchronous::continuation_task&lt;R></code> where R is the
                        returned type. This class provides us with <code>this_task_result()</code> where
                        we set the task result. This is done either immediately if n &lt; cutoff (first
                        if clause), or (else clause) using a continuation.</para>
                    <para>If n>= cutoff, we create a continuation. This is a sleeping task, which will
                        get activated when all required tasks complete. In this case, we have two
                        fibonacci sub tasks. The template argument is the return type of the
                        continuation. We create two sub-tasks, for n-1 and n-2 and when they complete,
                        the completion functor passed as first argument is called.</para>
                    <para>Note that <code>boost::asynchronous::create_continuation</code> is a variadic
                        function, there can be any number of sub-tasks. The completion functor takes as
                        single argument a tuple of <code>expected</code>, one for each subtask. The
                        template argument of the future is the template argument of
                        <code>boost::asynchronous::continuation_task</code> of each subtask. In this
                        case, all are of type long, but it's not a requirement.</para>
                    <para>When this completion functor is called, we set our result to be result of
                        first task + result of second task. </para>
                    <para>The main particularity of this solution is that a task does not block until
                        sub-tasks complete but instead provides a functor to be called asynchronously as
                        soon as subtasks complete.</para>
                    <para>All what we still need to do is create the first task. In the tradition of
                        Asynchronous, we show it inside an asynchronous servant which posts the first
                        task and waits for a callback, but the same is of course possible using
                        <code>post_future</code>:</para>
                    <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
...
   void calc_fibonacci(long n,long cutoff)
   {
      post_callback(
            // work
            [n,cutoff]()
            {
                // a top-level continuation is the first one in a recursive serie.
                // Its result will be passed to callback
                <emphasis role="bold">return</emphasis> boost::asynchronous::<emphasis role="bold">top_level_callback_continuation&lt;long></emphasis>(fib_task(n,cutoff));
            },
            // callback with fibonacci result.
            [](boost::asynchronous::expected&lt;long> res){...}// callback functor.
        );                                 
   }  
};          </programlisting>
                    <para> We call <code>post_callback</code>, which, as usual, ensures that the
                        callback is posted to the right thread and the servant lifetime is tracked.
                        The posted task calls
                            <code>boost::asynchronous::top_level_callback_continuation&lt;task-return-type></code>
                        to create the first, top-level continuation, passing it a first fib_task.
                        This is non-blocking, a special version of <code>post_callback</code>
                        recognizes a continuation and will call its callback (with a
                            <code>expected&lt;task-return-type></code>) only when the calculation is
                        finished, not when the "work" lambda returns. For this to work, <emphasis
                            role="bold">it is essential not to forget the return
                            statement</emphasis>. Without it, the compiler will unhappily remark
                        that an <code>expected&lt;void></code> cannot be casted to an
                            <code>expected&lt;long></code>, or worse if one expects an
                            <code>expected&lt;void></code>, the callback would be called to
                        early.</para>
                    <para>As usual, calling get() on the expected is non-blocking, one gets either the
                        result or an exception if thrown by a task.</para>
                    <para>Please have a look at the <link xlink:href="examples/example_fibonacci.cpp"
                        >complete example</link>.</para>
                </sect2>
                <sect2>
                    <title>Logging</title>
                    <para>What about logging? We don't want to give up this feature of course and
                        would like to know how long all these fib_task took to complete. This is
                        done through minor changes. As always we need a job:</para>
                    <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> servant_job;                                                 </programlisting>
                    <para> We give the logged name of the task in the constructor of fib_task, for
                        example fib_task_xxx:</para>
                    <programlisting>fib_task(long n,long cutoff)
        : boost::asynchronous::continuation_task&lt;long>("fib_task_" + boost::lexical_cast&lt;std::string>(n))
        ,n_(n),cutoff_(cutoff){}                                                </programlisting>
                    <para>And call <code>boost::asynchronous::create_continuation_job</code> instead of
                        <code>boost::asynchronous::create_continuation</code>:</para>
                    <programlisting>boost::asynchronous::<emphasis role="bold">create_callback_continuation_job</emphasis>&lt;servant_job>(
                        [task_res](std::tuple&lt;boost::asynchronous::expected&lt;long>,boost::asynchronous::expected&lt;long> > res)
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get();
                            task_res.set_value(r);
                        },
                        fib_task(n_-1,cutoff_),
                        fib_task(n_-2,cutoff_)
);                                              </programlisting>
                    <para> Inside the servant we might optionally want the version of post_callback with
                        name, and we need to use <code>top_level_continuation_job</code> instead of
                        <code>top_level_continuation</code>:</para>
                    <programlisting>post_callback(
              [n,cutoff]()
              {
                   return boost::asynchronous::<emphasis role="bold">top_level_callback_continuation_job</emphasis>&lt;long,servant_job>(fib_task(n,cutoff));
              },// work
              // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
              [this](boost::asynchronous::expected&lt;long> res){...},// callback functor.
              <emphasis role="bold">"calc_fibonacci"</emphasis>
        );                             
                </programlisting>
                    <para> The previous example has been <link
                        xlink:href="examples/example_fibonacci_log.cpp">rewritten with logs and a
                        display of all tasks</link> (beware, with higher fibonacci numbers, this can
                        become a long list).</para>
                    <para><emphasis role="underline">Limitation</emphasis>: in the current
                        implementation, tasks are logged, but the continuation callback is not. If it
                        might take long, one should post a (loggable) task.</para>
                    <para><emphasis role="underline">Note</emphasis>: to improve performance, the last
                        task passed to <emphasis role="bold"
                            >create_callback_continuation(_job)</emphasis> is not posted but executed
                        directly so it will execute under the name of the task calling <emphasis
                            role="bold">create_callback_continuation(_job)</emphasis>.</para>
                    <para><emphasis role="bold"><emphasis role="underline">Important note about
                        exception safety</emphasis></emphasis>. The passed <emphasis role="bold"
                            >expected</emphasis> contains either a result or an exception. Calling get()
                        will throw contained exceptions. You should catch it, in the continuation
                        callback and in the task itself. Asynchronous will handle the exception, but it
                        cannot set the <emphasis role="bold">continuation_result</emphasis>, which will
                        never be set and the callback part of post_callback never called. This simple
                        example does not throw, so we save ourselves the cost, but more complicated
                        algorithms should take care of this.</para>
                </sect2>
                <sect2>
                    <title>Creating a variable number of tasks for a continuation</title>
                    <para>It is sometimes not possible to know at compile-time the number of tasks
                        or even the types of tasks used in the creation of a continuation. In this
                        cases, Asynchronous provides more possibilities: <itemizedlist>
                            <listitem>
                                <para>Pack all subtasks of a same type into a std::vector, then pass
                                    it to <code>create_callback_continuation or
                                        create_callback_continuation_job</code>. In this case, we
                                    know that these subtasks all have the same type, so our
                                    continuation is called with a
                                        <code>vector&lt;expected&lt;return_type>></code>:</para>
                                            <programlisting>struct sub_task : public boost::asynchronous::continuation_task&lt;long>
{
    // some task with long as result type
};
struct main_task : public boost::asynchronous::continuation_task&lt;long>
{
  void operator()()
  {
     boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();
     <emphasis role="bold">std::vector&lt;sub_task></emphasis> subs;
     subs.push_back(sub_task());
     subs.push_back(sub_task());
     subs.push_back(sub_task()); 
                                         
     boost::asynchronous::<emphasis role="bold">create_callback_continuation</emphasis>(
          [task_res](<emphasis role="bold">std::vector&lt;boost::asynchronous::expected&lt;long>></emphasis> res)
          {
             long r = res[0].get() + res[1].get() + res[2].get();
             task_res.set_value(r);
          },
          <emphasis role="bold">std::move(subs)</emphasis>);
   }
};</programlisting>
                            </listitem>
                            <listitem>
                                <para>If the subtasks have different type, but a common result type,
                                    we can pack them into a
                                    <code>std::vector&lt;boost::asynchronous::any_continuation_task&lt;return_type>></code>
                                    instead, the rest of the code staying the same:</para>
                                        <programlisting><emphasis role="bold">#include &lt;boost/asynchronous/any_continuation_task.hpp></emphasis>

struct sub_task : public boost::asynchronous::continuation_task&lt;long>
{
    // some task with long as result type
};
struct main_task2 : public boost::asynchronous::continuation_task&lt;long>
{
    void operator()()
    {
        boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();
        <emphasis role="bold">std::vector&lt;boost::asynchronous::any_continuation_task&lt;long>></emphasis> subs;
        subs.push_back(sub_task());
        subs.push_back(sub_task2());
        subs.push_back(sub_task3());

        boost::asynchronous::<emphasis role="bold">create_callback_continuation</emphasis>(
             [task_res](<emphasis role="bold">std::vector&lt;boost::asynchronous::expected&lt;long>></emphasis> res)
             {
                 long r = res[0].get() + res[1].get() + res[2].get();
                  task_res.set_value(r);
             },
             <emphasis role="bold">std::move(subs)</emphasis>);
    }
};</programlisting>
                            </listitem>
                            <listitem>
                                <para>Of course, if we have continuations in the first place,
                                    returned by
                                        <code>top_level_callback_continuation&lt;task-return-type></code>
                                    or
                                        <code>top_level_callback_continuation&lt;task-return-type></code>,
                                    as all of Asynchronous' algorithms do, these can be packed into
                                    a vector as well:</para>
                                <programlisting>struct main_task3 : public boost::asynchronous::continuation_task&lt;long>
{
    void operator()()
    {
        boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();
        <emphasis role="bold">std::vector&lt;boost::asynchronous::detail::callback_continuation&lt;long>></emphasis> subs;
        std::vector&lt;long> data1(10000,1);
        std::vector&lt;long> data2(10000,1);
        std::vector&lt;long> data3(10000,1);
        subs.<emphasis role="bold">push_back</emphasis>(boost::asynchronous::<emphasis role="bold">parallel_reduce</emphasis>(std::move(data1),
                                                            [](long const&amp; a, long const&amp; b)
                                                            {
                                                              return a + b;
                                                            },1000));
        subs.<emphasis role="bold">push_back</emphasis>(boost::asynchronous::<emphasis role="bold">parallel_reduce</emphasis>(std::move(data2),
                                                            [](long const&amp; a, long const&amp; b)
                                                            {
                                                              return a + b;
                                                            },1000));
        subs.<emphasis role="bold">push_back</emphasis>(boost::asynchronous::<emphasis role="bold">parallel_reduce</emphasis>(std::move(data3),
                                                            [](long const&amp; a, long const&amp; b)
                                                            {
                                                              return a + b;
                                                            },1000));

        boost::asynchronous::<emphasis role="bold">create_callback_continuation</emphasis>(
                        [task_res](<emphasis role="bold">std::vector&lt;boost::asynchronous::expected&lt;long>></emphasis> res)
                        {
                            long r = res[0].get() + res[1].get() + res[2].get();
                            task_res.set_value(r);
                        },
                        <emphasis role="bold">std::move(subs)</emphasis>);
    }
};</programlisting>                                
                            </listitem>
                        </itemizedlist></para>
                </sect2>
                <sect2>
                    <title>Creating a continuation from a simple functor</title>
                    <para>For very simple tasks, it is in a post C++11 world annoying to have to
                        write a functor class like our above sub_task. For such cases, Asynchronous
                        provides a simple helper function:</para>
                    <para><code>auto make_lambda_continuation_wrapper(functor f, std::string
                            const&amp; name="")</code> where auto will be a
                            <code>continuation_task</code>. We can replace our first case above by a
                        more concise:</para>
                    <programlisting>struct main_task4 : public boost::asynchronous::continuation_task&lt;int>
{
    void operator()()
    {
        // 15, 22,5 are of type int
        boost::asynchronous::continuation_result&lt;<emphasis role="bold">int</emphasis>> task_res = this_task_result();
        <emphasis role="bold">std::vector&lt;boost::asynchronous::any_continuation_task&lt;int>></emphasis> subs;
        subs.push_back(boost::asynchronous::<emphasis role="bold">make_lambda_continuation_wrapper</emphasis>([](){return 15;}));
        subs.push_back(boost::asynchronous::<emphasis role="bold">make_lambda_continuation_wrapper</emphasis>([](){return 22;}));
        subs.push_back(boost::asynchronous::<emphasis role="bold">make_lambda_continuation_wrapper</emphasis>([](){return 5;}));

        boost::asynchronous::create_callback_continuation(
                        [task_res](std::vector&lt;boost::asynchronous::expected&lt;int>> res)
                        {
                            int r = res[0].get() + res[1].get() + res[2].get();
                            task_res.set_value(r);
                        },
                        <emphasis role="bold">std::move(subs)</emphasis>);
    }
};</programlisting>
                </sect2>
            </sect1>
            <sect1>
                <title><command xml:id="continuations"/>Future-based continuations</title>              
                <para>The continuations shown above are the fastest offered by Asynchronous.
                    Sometimes, however, we are forced to use libraries returning us only a future.
                    In this case, Asynchronous also offers "simple" continuations, which are
                    future-based. Consider the following trivial example. We consider we have a
                    task, called sub_task. We will simulate the future-returning library using
                        <code>post_future</code>. We want to divide our work between sub_task
                    instances, getting a callback when all complete. We can create a continuation
                    using these futures:</para>
                <programlisting>// our main algo task. Needs to inherit continuation_task&lt;value type returned by this task>
struct main_task : public boost::asynchronous::continuation_task&lt;long>
{
    void operator()()const
    {
        // the result of this task
       <emphasis role="bold"> boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();</emphasis>

        // we start calculation, then while doing this we see new tasks which can be posted and done concurrently to us
        // when all are done, we will set the result
        // to post tasks, we need a scheduler
        boost::asynchronous::any_weak_scheduler&lt;> weak_scheduler = boost::asynchronous::get_thread_scheduler&lt;>();
        boost::asynchronous::any_shared_scheduler&lt;> locked_scheduler = weak_scheduler.lock();
        if (!locked_scheduler.is_valid())
            // ok, we are shutting down, ok give up
            return;
        // simulate algo work
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        boost::future&lt;int> fu1 = boost::asynchronous::post_future(locked_scheduler,sub_task());
        // simulate more algo work
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        boost::future&lt;int> fu2 = boost::asynchronous::post_future(locked_scheduler,sub_task());
        // simulate algo work
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        boost::future&lt;int> fu3 = boost::asynchronous::post_future(locked_scheduler,sub_task());

        // our algo is now done, wrap all and return
        boost::asynchronous::<emphasis role="bold">create_continuation</emphasis>(
                    // called when subtasks are done, set our result
                    [task_res](std::tuple&lt;boost::future&lt;int>,boost::future&lt;int>,boost::future&lt;int> > res)
                    {
                        try
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get()+ std::get&lt;2>(res).get();
                            <emphasis role="bold">task_res.set_value(r);</emphasis>
                        }
                        catch(std::exception&amp; e)
                        {
                            <emphasis role="bold">task_res.set_exception(boost::copy_exception(e));</emphasis>
                        }
                    },
                    // future results of recursive tasks
                    <emphasis role="bold">std::move(fu1),std::move(fu2),std::move(fu3)</emphasis>);
    }
    };                                               </programlisting>
                <para>Please have a look at <link xlink:href="examples/example_continuation_algo.cpp">the complete
                    example</link></para>
                <para>Our tasks starts by posting 3 instances of sub_task, each time getting a
                    future. We then call <emphasis role="bold">create_continuation(_job)</emphasis>,
                    passing it the futures. When all futures are ready (have a value or an
                    exception), the callback is called, with 3 futures containing the result.</para>
                <para>Advantage:<itemizedlist>
                        <listitem>
                            <para>can be used with any library returning a boost::future</para>
                        </listitem>
                    </itemizedlist></para>
                <para>Drawbacks:<itemizedlist>
                        <listitem>
                            <para>lesser performance</para>
                        </listitem>
                        <listitem>
                            <para>the thread calling <emphasis role="bold"
                                    >create_continuation(_job)</emphasis> polls until all futures
                                are set. If this thread is busy, the callback is delayed.</para>
                        </listitem>
                </itemizedlist></para>
                <para><emphasis role="bold"><emphasis role="underline">Important
                        note</emphasis></emphasis>: Like for the previous callback continuations,
                    tasks and continuation callbacks should catch exceptions.</para>
                <para><emphasis role="bold">create_continuation(_job)</emphasis> has a wider
                    interface. It can also take a vector of futures instead of a variadic
                    version, for example:</para>
                <programlisting>// our main algo task. Needs to inherit continuation_task&lt;value type returned by this task>
struct main_task : public boost::asynchronous::continuation_task&lt;long>
{
    void operator()()const
    {
        // the result of this task
        boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();

        // we start calculation, then while doing this we see new tasks which can be posted and done concurrently to us
        // when all are done, we will set the result
        // to post tasks, we need a scheduler
        boost::asynchronous::any_weak_scheduler&lt;> weak_scheduler = boost::asynchronous::get_thread_scheduler&lt;>();
        boost::asynchronous::any_shared_scheduler&lt;> locked_scheduler = weak_scheduler.lock();
        if (!locked_scheduler.is_valid())
            // ok, we are shutting down, ok give up
            return;
        // simulate algo work
        std::vector&lt;boost::future&lt;int> > fus;
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        boost::future&lt;int> fu1 = boost::asynchronous::post_future(locked_scheduler,sub_task());
        fus.emplace_back(std::move(fu1));
        // simulate more algo work
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        boost::future&lt;int> fu2 = boost::asynchronous::post_future(locked_scheduler,sub_task());
        fus.emplace_back(std::move(fu2));
        // simulate algo work
        boost::this_thread::sleep(boost::posix_time::milliseconds(100));
        // let's say we just found a subtask
        boost::future&lt;int> fu3 = boost::asynchronous::post_future(locked_scheduler,sub_task());
        fus.emplace_back(std::move(fu3));

        // our algo is now done, wrap all and return
        boost::asynchronous::<emphasis role="bold">create_continuation</emphasis>(
                    // called when subtasks are done, set our result
                    [task_res](std::vector&lt;boost::future&lt;int>> res)
                    {
                        try
                        {
                            long r = res[0].get() + res[1].get() + res[2].get();
                            task_res.set_value(r);
                        }
                        catch(std::exception&amp; e)
                        {
                            task_res.set_exception(boost::copy_exception(e));
                        }
                    },
                    // future results of recursive tasks
                    <emphasis role="bold">std::move(fus)</emphasis>);
    }
    };                                            </programlisting>
                <para>The drawback is that in this case, all futures must be of the same type.
                    Please have a look at <link xlink:href="examples/example_continuation_algo2.cpp"
                        >the complete example</link></para>
            </sect1>
            <sect1>
                <title><command xml:id="distributing"/>Distributing work among machines</title>
                <para>At the time of this writing, a core i7-3930K with 6 cores and 3.2 GHz will
                    cost $560, so say $100 per core. Not a bad deal, so you buy it. Unfortunately,
                    some time later you realize you need more power. Ok, there is no i7 with more
                    cores and an Extreme Edition will be quite expensive for only a little more
                    power so you decide to go for a Xeon. A 12-core E5-2697v2 2.7GHz will go for
                    almost $3000 which means $250 per core, and for this you also have a lesser
                    frequency. And if you need later even more power, well, it will become really
                    expensive. Can Asynchronous help us use more power for cheap, and at best, with
                    little work? It does, as you guess ;-)</para>
                <para>Asynchronous provides a special pool, <code>tcp_server_scheduler</code>, which
                    will behave like any other scheduler but will not execute work itself, waiting
                    instead for clients to connect and steal some work. The client execute the work
                    on behalf of the <code>tcp_server_scheduler</code> and sends it back the
                    results. </para>
                <para>For this to work, there is however a condition: jobs must be (boost)
                    serializable to be transferred to the client. So does the returned value.</para>
                <para>Let's start with a <link xlink:href="examples/example_tcp_server.cpp">simplest
                        example</link>:</para>
                <programlisting>// notice how the worker pool has a different job type
struct Servant : boost::asynchronous::trackable_servant&lt;boost::asynchronous::any_callable,<emphasis role="bold">boost::asynchronous::any_serializable</emphasis>>
{
  Servant(boost::asynchronous::any_weak_scheduler&lt;> scheduler)
        : boost::asynchronous::trackable_servant&lt;boost::asynchronous::any_callable,<emphasis role="bold">boost::asynchronous::any_serializable</emphasis>>(scheduler)
  {
        // let's build our pool step by step. First we need a worker pool
        // possibly for us, and we want to share it with the tcp pool for its serialization work
        boost::asynchronous::any_shared_scheduler_proxy&lt;> workers = boost::asynchronous::make_shared_scheduler_proxy&lt;
                                                                            boost::asynchronous::threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;>>>(3);

        // we use a tcp pool using the 3 worker threads we just built
        // our server will listen on "localhost" port 12345
        auto pool= boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::tcp_server_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable>>>
                                (workers,"localhost",12345);
        // and this will be the worker pool for post_callback
        set_worker(pool);
  }
};</programlisting>
                <para>We start by creating a worker pool. The <code>tcp_server_scheduler</code> will
                    delegate to this pool all its serialization / deserialization work. For maximum
                    scalability we want this work to happen in more than one thread.</para>
                <para>Note that our job type is no more a simple callable, it must be
                    (de)serializable too (<emphasis role="bold"
                        >boost::asynchronous::any_serializable</emphasis>).</para>
                <para>Then we need a <code>tcp_server_scheduler</code> listening on, in this case,
                    localhost, port 12345. We now have a functioning worker pool and choose to use
                    it as our worker pool so that we do not execute jobs ourselves (other
                    configurations will be shown soon). Let's exercise our new pool. We first need a
                    task to be executed remotely:</para>
                <programlisting>struct dummy_tcp_task : public boost::asynchronous::<emphasis role="bold">serializable_task</emphasis>
{
    dummy_tcp_task(int d):boost::asynchronous::<emphasis role="bold">serializable_task</emphasis>(<emphasis role="bold">"dummy_tcp_task"</emphasis>),m_data(d){}
    template &lt;class Archive>
    void <emphasis role="bold">serialize</emphasis>(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; m_data;
    }
    int operator()()const
    {
        std::cout &lt;&lt; "dummy_tcp_task operator(): " &lt;&lt; m_data &lt;&lt; std::endl;
        boost::this_thread::sleep(boost::posix_time::milliseconds(2000));
        std::cout &lt;&lt; "dummy_tcp_task operator() finished" &lt;&lt; std::endl;
        return m_data;
    }
    int m_data;
};</programlisting>
                <para>This is a minimum task, only sleeping. All it needs is a
                        <code>serialize</code> member to play nice with Boost.Serialization and it
                    must inherit <code>serializable_task</code>. Giving the task a name is essential
                    as it will allow the client to deserialize it. Let's post to our TCP worker pool
                    some of the tasks, wait for a client to pick them and use the results:</para>
                <programlisting>// start long tasks in threadpool (first lambda) and callback in our thread
for (int i =0 ;i &lt; 10 ; ++i)
{
    std::cout &lt;&lt; "call post_callback with i: " &lt;&lt; i &lt;&lt; std::endl;
    post_callback(
           dummy_tcp_task(i),
           // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
           [this](boost::asynchronous::expected&lt;int> res){
                  try{
                        this->on_callback(res.get());
                  }
                  catch(std::exception&amp; e)
                  {
                       std::cout &lt;&lt; "got exception: " &lt;&lt; e.what() &lt;&lt; std::endl;
                       this->on_callback(0);
                  }
            }// callback functor.
    );
}</programlisting>
                <para>We post 10 tasks to the pool. For each task we will get, at some later
                    undefined point (provided some clients are around), a result in form of a
                    (ready) expected, possibly an exception if one was thrown by the task.</para>
                <para>Notice it is safe to use <code>this</code> in the callback lambda as it will
                    be only called if the servant still exists.</para>
                <para>We still need a client to execute the task, this is pretty straightforward (we
                    will extend it soon):</para>
                <programlisting>int main(int argc, char* argv[])
{
    std::string server_address = (argc>1) ? argv[1]:"localhost";
    std::string server_port = (argc>2) ? argv[2]:"12346";
    int threads = (argc>3) ? strtol(argv[3],0,0) : 4;
    cout &lt;&lt; "Starting connecting to " &lt;&lt; server_address &lt;&lt; " port " &lt;&lt; server_port &lt;&lt; " with " &lt;&lt; threads &lt;&lt; " threads" &lt;&lt; endl;

    auto scheduler = boost::asynchronous::make_shared_scheduler_proxy&lt;boost::asynchronous::asio_scheduler&lt;>>()
    {
        std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> 
        executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="dummy_tcp_task")
            {
                dummy_tcp_task t(0);
                boost::asynchronous::tcp::<emphasis role="bold">deserialize_and_call_task</emphasis>(t,resp,when_done);
            }
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };

        auto pool = boost::asynchronous::make_shared_scheduler_proxy&lt;
                          boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable>>>(threads);
        boost::asynchronous::tcp::<emphasis role="bold">simple_tcp_client_proxy proxy</emphasis>(scheduler,pool,server_address,server_port,executor,
                                                                    0/*ms between calls to server*/);
        boost::future&lt;boost::future&lt;void> > fu = proxy.run();
        boost::future&lt;void> fu_end = fu.get();
        fu_end.get();
    }
    return 0;
}</programlisting>
                <para>We start by taking as command-line arguments the server address and port and
                    the number of threads the client will use to process stolen work from the
                    server. </para>
                <para>We create a single-threaded <code>asio_scheduler</code> for the communication
                    (in our case, this is sufficient, your case might vary) to the server.</para>
                <para>The client then defines an executor function. This function will be called
                    when work is stolen by the client. As Asynchronous does not know what the work
                    type is, we will need to "help" by creating an instance of the task using its
                    name. Calling <code>deserialize_and_call_task</code> will, well, deserialize the
                    task data into our dummy task, then call it. We also choose to return an
                    exception is the task is not known to us.</para>
                <para>Next, we need a pool of threads to execute the work. Usually, you will want
                    more than one thread as we want to use all our cores.</para>
                <para>The simplest client that Asynchronous offers is a
                        <code>simple_tcp_client_proxy</code> proxy. We say simple, because it is
                    only a client. Later on, we will see a more powerful tool.
                        <code>simple_tcp_client_proxy</code> will require the asio pool for
                    communication, the server address and port, our executor and a parameter telling
                    it how often it should try to steal work from a server.</para>
                <para>We are now done, the client will run until killed.</para>
                <para>Let's sum up what we got in these few lines of code:<itemizedlist>
                        <listitem>
                            <para>a pool behaving like any other pool, which can be stolen
                                from</para>
                        </listitem>
                        <listitem>
                            <para>a server which does no work itself, but still scales well as
                                serialization is using whatever threads it is given</para>
                        </listitem>
                        <listitem>
                            <para>a trackable servant working with <code>post_callback</code>, like
                                always</para>
                        </listitem>
                        <listitem>
                            <para>a multithreaded client, which can be tuned precisely to use a
                                given pool for the communication and another (or the same btw.) for
                                work processing.</para>
                        </listitem>
                </itemizedlist></para>
                <para>Interestingly, we have a very versatile client. It is possible to reuse the
                    work processing and communication pools, within the same client application, for
                    a different <code>simple_tcp_client_proxy</code> which would be connecting to another
                    server.</para>
                <para>The server is also quite flexible. It scales well and can handle as many
                    clients as one wishes.</para>
                <para>This is only the beginning of our distributed chapter.</para>
                <sect2>
                    <title>A distributed, parallel Fibonacci</title>
                    <para>Lets's revisit our parallel Fibonacci example. We realize that with higher
                        Fibonacci numbers, our CPU power doesn't suffice any more. We want to
                        distribute it among several machines while our main machine still does some
                        calculation work. To do this, we'll start with our previous example, and
                        rewrite our Fibonacci task to make it distributable.</para>
                    <para>We remember that we first had to call
                            <code>boost::asynchronous::top_level_continuation</code> in our
                        post_callback to make Asynchronous aware of the later return value. The
                        difference now is that even this one-liner lambda could be serialized and
                        sent away, so we need to make it a <code>serializable_task</code>:</para>
                    <programlisting>struct serializable_fib_task : public boost::asynchronous::<emphasis role="bold">serializable_task</emphasis>
{
    serializable_fib_task(long n,long cutoff):boost::asynchronous::<emphasis role="bold">serializable_task("serializable_fib_task")</emphasis>,n_(n),cutoff_(cutoff){}
    template &lt;class Archive>
    <emphasis role="bold">void serialize(Archive &amp; ar, const unsigned int /*version*/)</emphasis>
    {
        ar &amp; n_;
        ar &amp; cutoff_;
    }
    auto operator()()const
        -> decltype(boost::asynchronous::top_level_continuation_log&lt;long,boost::asynchronous::any_serializable>
                    (tcp_example::fib_task(long(0),long(0))))
    {
        auto cont =  boost::asynchronous::top_level_continuation_job&lt;long,boost::asynchronous::any_serializable>
                (tcp_example::fib_task(n_,cutoff_));
        return cont;
    }
    long n_;
    long cutoff_;
};</programlisting>
                <para>We need to make our task serializable and give it a name so that the client
                        application can recognize it. We also need a serialize member, as required
                        by Boost.Serialization. And we need an operator() so that the task can be
                        executed. There is in C++11 an ugly decltype, but C++14 will solve this if
                        your compiler supports it. We also need a few changes in our Fibonacci
                        task:</para>
                    <programlisting>// our recursive fibonacci tasks. Needs to inherit continuation_task&lt;value type returned by this task>
struct fib_task : public boost::asynchronous::continuation_task&lt;long>
                <emphasis role="bold">, public boost::asynchronous::serializable_task</emphasis>
{
    fib_task(long n,long cutoff)
        :  boost::asynchronous::continuation_task&lt;long>()
        <emphasis role="bold">, boost::asynchronous::serializable_task("serializable_sub_fib_task")</emphasis>
        ,n_(n),cutoff_(cutoff)
    {
    }
    <emphasis role="bold">template &lt;class Archive>
    void save(Archive &amp; ar, const unsigned int /*version*/)const
    {
        ar &amp; n_;
        ar &amp; cutoff_;
    }
    template &lt;class Archive>
    void load(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; n_;
        ar &amp; cutoff_;
    }
    BOOST_SERIALIZATION_SPLIT_MEMBER()</emphasis>
    void operator()()const
    {
        // the result of this task, will be either set directly if &lt; cutoff, otherwise when taks is ready
        boost::asynchronous::continuation_result&lt;long> task_res = this_task_result();
        if (n_&lt;cutoff_)
        {
            // n &lt; cutoff => execute ourselves
            task_res.set_value(serial_fib(n_));
        }
        else
        {
            // n>= cutoff, create 2 new tasks and when both are done, set our result (res(task1) + res(task2))
            boost::asynchronous::create_callback_continuation_job&lt;boost::asynchronous::any_serializable>(
                        // called when subtasks are done, set our result
                        [task_res](std::tuple&lt;boost::future&lt;long>,boost::future&lt;long> > res)
                        {
                            long r = std::get&lt;0>(res).get() + std::get&lt;1>(res).get();
                            task_res.set_value(r);
                        },
                        // recursive tasks
                        fib_task(n_-1,cutoff_),
                        fib_task(n_-2,cutoff_));
        }
    }
    long n_;
    long cutoff_;
};</programlisting>
                <para>The few changes are highlighted. The task needs to be a serializable task with
                        its own name in the constructor, and it needs serialization members. That's
                        it, we're ready to distribute!</para>
                    <para>As we previously said, we will reuse our previous TCP example, using
                            <code>serializable_fib_task</code> as the main posted task. This gives
                        us <link xlink:href="examples/example_tcp_server_fib.cpp">this example</link>.</para>
                    <para>But wait, we promised that our server would itself do some calculation
                        work, and we use as worker pool only a <code>tcp_server_scheduler</code>.
                        Right, let's do it now, throwing in a few more goodies. We need a worker
                        pool, with as many threads as we are willing to offer:</para>
                    <programlisting>// we need a pool where the tasks execute
auto <emphasis role="bold">pool</emphasis> = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">threadpool_scheduler</emphasis>&lt;
                    boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable> >(<emphasis role="bold">threads</emphasis>));</programlisting>
                 <para>This pool will get the fibonacci top-level task we will post, then, if our
                        clients connect after we start, it will get the first sub-tasks. </para>
                    <para>To make it more interesting, let's offer our server to also be a job
                        client. This way, we can build a cooperation network: the server offers
                        fibonacci tasks, but also tries to steal some, thus increasing homogenous
                        work distribution. We'll talk more about this in the next chapter.</para>
                    <programlisting>// a client will steal jobs in this pool
auto cscheduler = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::<emphasis role="bold">asio_scheduler</emphasis>&lt;>);
// jobs we will support
std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,
                   std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="serializable_sub_fib_task")
            {
                tcp_example::fib_task fib(0,0);
                boost::asynchronous::tcp::<emphasis role="bold">deserialize_and_call_callback_continuation_task</emphasis>(fib,resp,when_done);
            }
            else if (task_name=="serializable_fib_task")
            {
                tcp_example::serializable_fib_task fib(0,0);
                boost::asynchronous::tcp::<emphasis role="bold">deserialize_and_call_top_level_callback_continuation_task</emphasis>(fib,resp,when_done);
            }
            // else whatever functor we support
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };
boost::asynchronous::tcp::simple_tcp_client_proxy client_proxy(cscheduler,pool,server_address,server_port,executor,
                                                               10/*ms between calls to server*/);</programlisting>
                <para>Notice how we use our worker pool for job serialization / deserialization.
                        Notice also how we check both possible stolen jobs.</para>
                    <para>We also introduce two new deserialization functions.
                            boost::asynchronous::tcp::<emphasis role="bold"
                            >deserialize_and_call_task</emphasis> was used for normal tasks, we now
                        have boost::asynchronous::tcp::<emphasis role="bold"
                            >deserialize_and_call_top_level_callback_continuation_task</emphasis>
                        for our top-level continuation task, and boost::asynchronous::tcp::<emphasis
                            role="bold">deserialize_and_call_callback_continuation_task</emphasis>
                        for the continuation-sub-task.</para>
                    <para>We now need to build our TCP server, which we decide will get only one
                        thread for task serialization. This ought to be enough, Fibonacci tasks have
                        little data (2 long).</para>
                    <programlisting>// we need a server
// we use a tcp pool using 1 worker
auto server_pool = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;> >(<emphasis role="bold">1</emphasis>));

auto tcp_server= boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">tcp_server_scheduler</emphasis>&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable>,
                            boost::asynchronous::any_callable,true>
                                (server_pool,own_server_address,(unsigned int)own_server_port));</programlisting>
                <para>We have a TCP server pool, as before, even a client to steal work ourselves,
                        but how do we get ourselves this combined pool, which executes some work or
                        gives some away? </para>
                    <para>Wait a minute, combined pool? Yes, a
                            <code>composite_threadpool_scheduler</code> will do the trick. As we're
                        at it, we create a servant to coordinate the work, as we now always
                        do:</para>
                    <programlisting>// we need a composite for stealing
auto composite = boost::asynchronous::create_shared_scheduler_proxy
                (new boost::asynchronous::<emphasis role="bold">composite_threadpool_scheduler</emphasis>&lt;boost::asynchronous::any_serializable>
                          (<emphasis role="bold">pool</emphasis>,<emphasis role="bold">tcp_server</emphasis>));

// a single-threaded world, where Servant will live.
auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                                new boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::lockfree_queue&lt;> >);
{
      ServantProxy proxy(scheduler,<emphasis role="bold">pool</emphasis>);
      // result of BOOST_ASYNC_FUTURE_MEMBER is a shared_future,
      // so we have a shared_future of a shared_future(result of start_async_work)
      boost::future&lt;boost::future&lt;long> > fu = proxy.calc_fibonacci(fibo_val,cutoff);
      boost::future&lt;long> resfu = fu.get();
      long res = resfu.get();
}</programlisting>                    
                <para>Notice how we give only the worker "pool" to the servant. This means, the
                        servant will post the top-level task to it, it will immediately be called
                        and create 2 Fibonacci tasks, which will create each one 2 more, etc. until
                        at some point a client connects and steals one, which will create 2 more,
                        etc.</para>
                    <para>The client will not steal directly from this pool, it will steal from the
                            <code>tcp_server</code> pool, which, as long as a client request comes,
                        will steal from the worker pool, as they belong to the same composite. This
                        will continue until the composite is destroyed, or the work is done. For the
                        sake of the example, we do not give the composite as the Servant's worker
                        pool but keep it alive until the end of calculation. Please have a look at
                        the <link xlink:href="examples/example_tcp_server_fib2.cpp">complete example</link>.</para>
                    <para>In this example, we start taking care of homogenous work distribution by
                        packing a client and a server in the same application. But we need a bit
                        more: our last client would steal work so fast, every 10ms that it would
                        starve the server or other potential client applications, so we're going to
                        tell it to only steal if the size of its work queues are under a certain
                        amount, which we will empirically determine, according to our hardware,
                        network speed, etc.</para>
                    <programlisting>int main(int argc, char* argv[])
{
    std::string server_address = (argc>1) ? argv[1]:"localhost";
    std::string server_port = (argc>2) ? argv[2]:"12346";
    int threads = (argc>3) ? strtol(argv[3],0,0) : 4;
    // 1..n => check at regular time intervals if the queue is under the given size
    int job_getting_policy = (argc>4) ? strtol(argv[4],0,0):0;
    cout &lt;&lt; "Starting connecting to " &lt;&lt; server_address &lt;&lt; " port " &lt;&lt; server_port &lt;&lt; " with " &lt;&lt; threads &lt;&lt; " threads" &lt;&lt; endl;

    auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(
                new boost::asynchronous::asio_scheduler&lt;>);
    {
        std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> 
        executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="serializable_fib_task")
            {
                tcp_example::serializable_fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_top_level_callback_continuation_task(fib,resp,when_done);
            }
            else if (task_name=="serializable_sub_fib_task")
            {
                tcp_example::fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_callback_continuation_task(fib,resp,when_done);
            }
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };

        // guarded_deque supports queue size
        auto pool = boost::asynchronous::create_shared_scheduler_proxy(
                        new boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::<emphasis role="bold">guarded_deque</emphasis>&lt;boost::asynchronous::any_serializable> >(threads));
        // more advanced policy
        // or <emphasis role="bold">simple_tcp_client_proxy&lt;boost::asynchronous::tcp::queue_size_check_policy&lt;>></emphasis> if your compiler can (clang)
        typename boost::asynchronous::tcp::<emphasis role="bold">get_correct_simple_tcp_client_proxy</emphasis>&lt;boost::asynchronous::tcp::queue_size_check_policy&lt;>>::type proxy(
                        scheduler,pool,server_address,server_port,executor,
                        0/*ms between calls to server*/,
                        <emphasis role="bold">job_getting_policy /* number of jobs we try to keep in queue */</emphasis>);
        // run forever
        boost::future&lt;boost::future&lt;void> > fu = proxy.run();
        boost::future&lt;void> fu_end = fu.get();
        fu_end.get();
    }
    return 0;
}</programlisting>                    
                <para>The important new part is highlighted. <code>simple_tcp_client_proxy</code>
                        gets an extra template argument, <code>queue_size_check_policy</code>, and a
                        new constructor argument, the number of jobs in the queue, under which the
                        client will try, every 10ms, to steal a job. Normally, that would be all,
                        but g++ (up to 4.7 at least) is uncooperative and requires an extra level of
                        indirection to get the desired client proxy. Otherwise, there is no
                        change.</para>
                    <para>Notice that our standard lockfree queue offers no size() so we use a less
                        efficient guarded_deque.</para>
                    <para>You will find in the <link xlink:href="examples/simple_tcp_client.cpp"
                            >complete example</link> a few other tasks which we will explain
                        shortly.</para>
                    <para>Let's stop a minute to think about what we just did. We built, with little
                        code, a complete framework for distributing tasks homogenously among
                        machines, by reusing standard component offered by the library: threadpools,
                        composite pools, clients, servers. If we really have client connecting or
                        not is secondary, all what can happen is that calculating our Fibonacci
                        number will last a little longer.</para>
                    <para>We also separate the task (Fibonacci) from the threadpool configuration,
                        from the network configuration, and from the control of the task (Servant),
                        leading us to highly reusable, extendable code.</para>
                    <para>In the next chapter, we will add a way to further distribute work among
                        not only machines, but whole networks. </para>
                </sect2>                
                <sect2>
                    <title>Example: a hierarchical network</title>
                    <para>We already distribute and parallelize work, so we can scale a great deal,
                        but our current model is one server, many clients, which means a potentially
                        high network load and a lesser scalability as more and more clients connect
                        to a server. What we want is a client/server combo application  where the
                        client steals and executes jobs and a server component of the same
                        application which steals jobs from the client on behalf of other clients.
                        What we want is to achieve something like this:</para>
                    <para><inlinemediaobject>
                            <imageobject>
                                <imagedata fileref="pics/TCPHierarchical.jpg"/>
                            </imageobject>
                    </inlinemediaobject></para>
                 <para>We have our server application, as seen until now, called interestingly
                        ServerApplication on a machine called MainJobServer. This machine executes
                        work and offers at the same time a steal-from capability. We also have a
                        simple client called ClientApplication running on ClientMachine1, which
                        steals jobs and executes them itself without further delegating. We have
                        another client machine called ClientMachine2 on which
                        ClientServerApplication runs. This applications has two parts, a client
                        stealing jobs like ClientApplication and a server part stealing jobs from
                        the client part upon request. For example, another simple ClientApplication
                        running on ClientMachine2.1 connects to it and steals further jobs in case
                        ClientMachine2 is not executing them fast enough, or if ClientMachine2 is
                        only seen as a pass-through to move jobs execution to another network.
                        Sounds scalable. How hard is it to build? Not so hard, because in fact, we
                        already saw all we need to build this, so it's kind of a Lego game.</para>
                    <programlisting>int main(int argc, char* argv[])
{
    std::string server_address = (argc>1) ? argv[1]:"localhost";
    std::string server_port = (argc>2) ? argv[2]:"12345";
    std::string own_server_address = (argc>3) ? argv[3]:"localhost";
    long own_server_port = (argc>4) ? strtol(argv[4],0,0):12346;
    int threads = (argc>5) ? strtol(argv[5],0,0) : 4;
    cout &lt;&lt; "Starting connecting to " &lt;&lt; server_address &lt;&lt; " port " &lt;&lt; server_port
         &lt;&lt; " listening on " &lt;&lt; own_server_address &lt;&lt; " port " &lt;&lt; own_server_port &lt;&lt; " with " &lt;&lt; threads &lt;&lt; " threads" &lt;&lt; endl;

// to be continued</programlisting>  
                <para>We take as arguments the address and port of the server we are going to steal
                        from, then our own address and port. We now need a client with its
                        communication asio scheduler and its threadpool for job execution.</para>  
                    <programlisting>auto scheduler = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::<emphasis role="bold">asio_scheduler</emphasis>&lt;>);
    { //block start
        std::function&lt;void(std::string const&amp;,boost::asynchronous::tcp::server_reponse,
                           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)>)> executor=
        [](std::string const&amp; task_name,boost::asynchronous::tcp::server_reponse resp,
           std::function&lt;void(boost::asynchronous::tcp::client_request const&amp;)> when_done)
        {
            if (task_name=="serializable_fib_task")
            {
                tcp_example::serializable_fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_top_level_callback_continuation_task(fib,resp,when_done);
            }
            else if (task_name=="serializable_sub_fib_task")
            {
                tcp_example::fib_task fib(0,0);
                boost::asynchronous::tcp::deserialize_and_call_callback_continuation_task(fib,resp,when_done);
            }
            // else whatever functor we support
            else
            {
                std::cout &lt;&lt; "unknown task! Sorry, don't know: " &lt;&lt; task_name &lt;&lt; std::endl;
                throw boost::asynchronous::tcp::transport_exception("unknown task");
            }
        };
        // create pools
        // we need a pool where the tasks execute
        auto pool = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">threadpool_scheduler</emphasis>&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable> >(<emphasis role="bold">threads</emphasis>));
        boost::asynchronous::tcp::<emphasis role="bold">simple_tcp_client_proxy client_proxy</emphasis>(scheduler,<emphasis role="bold">pool</emphasis>,server_address,server_port,executor,
                                                                       10/*ms between calls to server*/);
// to be continued</programlisting>   
                <para>We now need a server to which more clients will connect, and a composite
                binding it to our worker pool:</para>   
                    <programlisting>   // we need a server
   // we use a tcp pool using 1 worker
   auto server_pool = boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::threadpool_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;> >(1));
   auto tcp_server= boost::asynchronous::create_shared_scheduler_proxy(
                    new boost::asynchronous::<emphasis role="bold">tcp_server_scheduler</emphasis>&lt;
                            boost::asynchronous::lockfree_queue&lt;boost::asynchronous::any_serializable>,
                            boost::asynchronous::any_callable,true>
                                (server_pool,own_server_address,(unsigned int)own_server_port));
   // we need a composite for stealing
   auto composite = boost::asynchronous::create_shared_scheduler_proxy(new boost::asynchronous::<emphasis role="bold">composite_threadpool_scheduler</emphasis>&lt;boost::asynchronous::any_serializable>
                                                                   (<emphasis role="bold">pool</emphasis>,<emphasis role="bold">tcp_server</emphasis>));

   boost::future&lt;boost::future&lt;void> > fu = client_proxy.run();
   boost::future&lt;void> fu_end = fu.get();
   fu_end.get();
} //end block

 return 0;
 } //end main</programlisting>   
                <para>And we're done! The client part will steal jobs and execute them, while the
                        server part, bound to the client pool, will steal on sub-client-demand.
                        Please have a look at the <link xlink:href="examples/tcp_client_server.cpp">
                            complete code</link>.</para>                    
                </sect2>
            </sect1>
            <sect1>
                <title>Picking your archive</title>
                <para>By default, Asynchronous uses a Boost Text archive (text_oarchive,
                    text_iarchive), which is simple and efficient enough for our Fibonacci example,
                    but inefficient for tasks holding more data.</para>
                <para>Asynchronous supports any archive task, requires however a different job type
                    for this. At the moment, we can use a
                        <code>portable_binary_oarchive</code>/<code>portable_binary_iarchive</code>
                    by selecting <code>any_bin_serializable</code> as job. If Boost supports more
                    archive types, support is easy to add.</para>
                <para>The previous Fibonacci server example has been <link
                        xlink:href="examples/example_tcp_server_fib2_bin.cpp">rewritten</link> to use this
                    capability. The <link xlink:href="examples/simple_tcp_client_bin_archive.cpp">client</link> has also been rewritten using this new job type.</para>
            </sect1>
            <sect1>
                <title><command xml:id="parallel_algos"/>Parallel Algorithms (Christophe Henry / Tobias Holl)</title>
                <para>Asynchronous supports out of the box quite some asynchronous parallel
                    algorithms, as well as interesting combination usages. These algorithms are
                    callback-continuation-based. Some of these algorithms also support distributed
                    calculations as long as the user-provided functors are (meaning they must be
                    serializable).</para>
                <para>What is the point of adding yet another set of parallel algorithms which can
                    be found elsewhere? Because truly asynchronous algorithms are hard to find. By
                    this we mean non-blocking. If one needs parallel algorithms, it's because they
                    could need long to complete. And if they take long, we really do not want to
                    block until it happens.</para>
                <para>All of the algorithms are made for use in a worker threadpool. They represent
                    the work part of a <code>post_callback</code>;</para>
                <para>In the philosophy of Asynchronous, the programmer knows better the task size
                    where he wants to start parallelizing, so all these algorithms take a cutoff.
                    Work is cut into packets of this size.</para>
                <para>All range algorithms also have a version taking a continuation as range
                    argument. This allows to combine algorithms functional way, for example this
                    (more to come):</para>
                <programlisting>return <emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">parallel_for</emphasis>(...)));</programlisting>
                <sect2>
                    <title>parallel_for</title>
                    <para>There are four versions of this algorithm:</para>
                    <programlisting>template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
parallel_for(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The parallel_for version taking iterators requires that the iterators stay
                        valid until completion. It is the programmer's job to ensure this.</para>
                    <para>The third argument is the predicate applied on each element of the
                        algorithm.</para>
                    <para>The fourth argument is the cutoff, meaning in this case the max. number of
                        elements of the input range in a task.</para>
                    <para>The optional fifth argument is the name of the tasks used for
                        logging.</para>
                    <para>The optional sixth argument is the priority of the tasks in the
                        pool.</para>
                    <para>The return value is a void continuation containing either nothing or an
                        exception if one was thrown from one of the tasks.</para>
                    <para>Example:</para>
                    <programlisting>struct Servant : boost::asynchronous::trackable_servant&lt;>
{
...
    void start_async_work()
    {
        // start long tasks in threadpool (first lambda) and callback in our thread
        post_callback(
               [this](){
                        <emphasis role="bold">return</emphasis> boost::asynchronous::<emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">this->m_data.begin(),this->m_data.end()</emphasis>,
                                                                 [](int const&amp; i)
                                                                 {
                                                                    const_cast&lt;int&amp;>(i) += 2;
                                                                 },1500);
                      },// work
               // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
               [](boost::asynchronous::expected&lt;<emphasis role="bold">void</emphasis>> /*res*/){
                            ...
               }// callback functor.
        );
    }
    std::vector&lt;int> m_data;
};</programlisting>
                <para>The most important parts are highlighted. Do not forget the return statement
                        as we are returning a continuation and we do not want the lambda to be
                        interpreted as a void lambda. The caller has responsibility of the input
                        data, given in the form of iterators. We use a non-legal modifying functor
                        for the sake of the example.</para>
                    <para>The call will do following:<itemizedlist>
                            <listitem>
                                <para>start tasks in the current worker pool of max 1500 elements of
                                    the input data</para>
                            </listitem>
                            <listitem>
                                <para>add 2 to each element in parallel</para>
                            </listitem>
                            <listitem>
                                <para>return a continuation</para>
                            </listitem>
                            <listitem>
                                <para>Execute the callback lambda when all tasks complete. The
                                    expected will be either set or contain an exception</para>
                            </listitem>
                    </itemizedlist></para>
                    <para>Please have a look at <link xlink:href="examples/example_parallel_for.cpp">the complete example</link>.</para>
                    <para>The second version is very similar and takes a range per reference. Again,
                        the range has to stay valid during the call. As previously, the return value
                        is a void continuation.</para>
                    <programlisting>template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">void</emphasis>,Job>
parallel_for(Range const&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                <para>The third version takes a range per rvalue reference. This is signal given to
                        Asynchronous that it must take ownership of the range. The return value is
                        then a continuation of the given range type:</para>
                <programlisting>template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">Range</emphasis>,Job>
parallel_for(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                <para>A <code>post_callback</code> will therefore get a expected&lt;new range>, for
                        example:</para>
                    <programlisting>post_callback(
    []()
    {
       std::vector&lt;int> data;
       return boost::asynchronous::parallel_for(std::move(data),
                                                      [](int const&amp; i)
                                                      {
                                                        const_cast&lt;int&amp;>(i) += 2;
                                                      },1500);
    },
    ](<emphasis role="bold">boost::asynchronous::expected&lt;std::vector&lt;int>></emphasis> ){}
);</programlisting>
                <para>In this case, the programmer does not need to ensure the container stays
                        valid, Asynchronous takes care of it.</para>
                    <para>The fourth version of this algorithm takes a range continuation instead of
                        a range as argument and will be invoked after the continuation is
                        ready.</para>
                    <programlisting>// version taking a continuation of a range as first argument
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">typename Range::return_type</emphasis>,Job>
parallel_for(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                <para>This version allows chaining parallel calls. For example, it is now possible
                to write:</para>
                    <programlisting>post_callback(
    []()
    {
       std::vector&lt;int> data;
       return <emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">parallel_for</emphasis>(<emphasis role="bold">parallel_for</emphasis>(
                                                            // executed first
                                                            std::move(data),
                                                            [](int const&amp; i)
                                                            {
                                                               const_cast&lt;int&amp;>(i) += 2;
                                                            },1500),
                                              // executed second
                                              [](int const&amp; i)
                                              {
                                                  const_cast&lt;int&amp;>(i) += 2;
                                              },1500),
                                 // executed third
                                 [](int const&amp; i)
                                 {
                                      const_cast&lt;int&amp;>(i) += 2;
                                 },1500);
    },
    ](<emphasis role="bold">boost::asynchronous::expected&lt;std::vector&lt;int>></emphasis> ){} // callback
);</programlisting>
                <para>This code will be executed as follows:<itemizedlist>
                            <listitem>
                                <para>the most inner parallel_for (parallel execution)</para>
                            </listitem>
                            <listitem>
                                <para>A kind of synchronization point will be done at this point
                                    until the parallel_for completes</para>
                            </listitem>
                            <listitem>
                                <para>the middle parallel_for will be executed (parallel
                                    execution)</para>
                            </listitem>
                            <listitem>
                                <para>A kind of synchronization point will be done at this point
                                    until the parallel_for completes</para>
                            </listitem>
                            <listitem>
                                <para>the outer parallel_for will be executed (parallel
                                    execution)</para>
                            </listitem>
                            <listitem>
                                <para>A kind of synchronization point will be done at this point
                                    until the parallel_for completes</para>
                            </listitem>
                            <listitem>
                                <para>The callback will be called</para>
                            </listitem>
                </itemizedlist></para>
                <para>With "kind of synchronization point", we mean there will be no blocking
                        synchronization, it will just be waited until completion.</para>
                    <para>Finally, we also promised some distributed support, so here it is. We
                        need, as with our Fibonacci example, a serializable sub-task which will be
                        created as often as required by our cutoff and which will handle a part of
                        our range:</para>
                    <programlisting>struct dummy_parallel_for_subtask : public boost::asynchronous::serializable_task
{
    dummy_parallel_for_subtask(int d=0):boost::asynchronous::serializable_task(<emphasis role="bold">"dummy_parallel_for_subtask"</emphasis>),m_data(d){}
    template &lt;class Archive>
    void <emphasis role="bold">serialize</emphasis>(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; m_data;
    }
    void operator()(int const&amp; i)const
    {
        const_cast&lt;int&amp;>(i) += m_data;
    }
    // some data, so we have something to serialize
    int m_data;
};</programlisting>
                  <para>As always we need a serializable top-level task, creating sub-tasks:</para>
                    <programlisting>struct dummy_parallel_for_task : public boost::asynchronous::serializable_task
{
    dummy_parallel_for_task():boost::asynchronous::serializable_task(<emphasis role="bold">"dummy_parallel_for_task"</emphasis>),m_data(1000000,1){}
    template &lt;class Archive>
    void <emphasis role="bold">serialize</emphasis>(Archive &amp; ar, const unsigned int /*version*/)
    {
        ar &amp; m_data;
    }
    auto operator()() -> decltype(boost::asynchronous::parallel_for&lt;std::vector&lt;int>,dummy_parallel_for_subtask,boost::asynchronous::any_serializable>(
                                      std::move(std::vector&lt;int>()),
                                      dummy_parallel_for_subtask(2),
                                      10))
    {
        <emphasis role="bold">return boost::asynchronous::parallel_for</emphasis>
                &lt;std::vector&lt;int>,<emphasis role="bold">dummy_parallel_for_subtask</emphasis>,boost::asynchronous::any_serializable>(
            std::move(m_data),
            dummy_parallel_for_subtask(2),
            10);
    }
    std::vector&lt;int> m_data;
};</programlisting>
                    <para>We now need to post our top-level task inside a servant:</para>
                    <programlisting>post_callback(
               dummy_parallel_for_task(),
               // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
               [this](boost::asynchronous::expected&lt;std::vector&lt;int>> res){
                 try
                 {
                    // do something
                 }
                 catch(std::exception&amp; e)
                 {
                    std::cout &lt;&lt; "got exception: " &lt;&lt; e.what() &lt;&lt; std::endl;
                 }
              }// end of callback functor.
);</programlisting>
                    <para>Please have a look at the <link xlink:href="examples/example_parallel_for_tcp.cpp">complete server example</link>.</para>
                </sect2>
                <sect2>
                    <title>parallel_reduce</title>
                    <para>Like parallel_for, there are four versions of this algorithm, with the
                        same lifetime behaviour. parallel_reduce applies a predicate to all elements
                        of a range, accumulating the result.</para>
                    <programlisting>template &lt;class Iterator, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">decltype(func(std::declval&lt;typename Iterator::value_type>(), std::declval&lt;typename Iterator::value_type>()))</emphasis>,Job>
<emphasis role="bold">parallel_reduce</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">decltype(func(*(range.begin()), *(range.end())))</emphasis>,Job>
<emphasis role="bold">parallel_reduce</emphasis>(Range const&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">decltype(func(*(range.begin()), *(range.end())))</emphasis>,Job>
<emphasis role="bold">parallel_reduce</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Range, class Func, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;<emphasis role="bold">decltype(func(std::declval&lt;typename Range::return_type::value_type>(), std::declval&lt;typename Range::return_type::value_type>()))</emphasis>,Job>
<emphasis role="bold">parallel_reduce</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                <para>Don't be worried about the return type. To keep it short, what we get is a
                        continuation of the type returned by the given predicate, for example, using
                        the iterator version:</para>
                    <para>
                        <programlisting>std::vector&lt;int> data;
post_callback(
    [this]()
    {
       return boost::asynchronous::<emphasis role="bold">parallel_reduce</emphasis>(this->data.begin(),this->data.end(),
                                                     [](int const&amp; a, int const&amp; b)
                                                     {
                                                         return a + b; // returns an int
                                                     },
                                                     1500);
    },
    ](<emphasis role="bold">boost::asynchronous::expected&lt;int></emphasis> ){} // callback gets an int
);</programlisting>
                    </para>
                    <para>We also have a <link xlink:href="examples/example_parallel_reduce_tcp.cpp">distributed version</link> as an example, which strictly looks like the parallel_for version.</para>
                </sect2>
                <sect2>
                    <title>parallel_invoke</title>
                    <para>parallel_invoke invokes a variadic list of predicates in parallel and
                        returns a (continuation of) tuple of futures containing the result of all of
                        them.</para>
                    <programlisting>template &lt;class Job, typename... Args>
boost::asynchronous::detail::<emphasis role="bold">callback_continuation</emphasis>&lt;typename decltype(boost::asynchronous::detail::make_expected_tuple(args...))::element_type,Job>
<emphasis role="bold">parallel_invoke</emphasis>(Args&amp;&amp;... args);</programlisting>
                    <para>Of course, the futures can have exceptions if exceptions are thrown, as in
                        the following example:</para>
                    <programlisting>post_callback(
               []()
               {
                   return boost::asynchronous::parallel_invoke&lt;boost::asynchronous::any_callable>(
                                     boost::asynchronous::<emphasis role="bold">to_continuation_task</emphasis>([](){throw my_exception();}), // void lambda
                                     boost::asynchronous::<emphasis role="bold">to_continuation_task</emphasis>([](){return 42.0;}));         // double lambda
                },// work
                // the lambda calls Servant, just to show that all is safe, Servant is alive if this is called
                [this](boost::<emphasis role="bold">asynchronous::expected</emphasis>&lt;std::<emphasis role="bold">tuple</emphasis>&lt;<emphasis role="bold">asynchronous::expected&lt;void>,asynchronous::expected&lt;double></emphasis>>> res)
                {
                   try
                   {
                        auto t = res.get();
                        std::cout &lt;&lt; "got result: " &lt;&lt; (<emphasis role="bold">std::get&lt;1></emphasis>(t)).get() &lt;&lt; std::endl;                // 42.0
                        std::cout &lt;&lt; "got exception?: " &lt;&lt; (<emphasis role="bold">std::get&lt;0></emphasis>(t)).has_exception() &lt;&lt; std::endl;  // true, has exception
                    }
                    catch(std::exception&amp; e)
                    {
                        std::cout &lt;&lt; "got exception: " &lt;&lt; e.what() &lt;&lt; std::endl;
                     }
                }// callback functor.
);</programlisting>
                    <para>Notice the use of <emphasis role="bold">to_continuation_task</emphasis> to
                        convert the lambdas in continuations.</para>
                    <para>As always, the callback lambda will be called when all tasks complete and
                        the futures are non-blocking.</para>
                    <para>Please have a look at the <link
                            xlink:href="examples/example_parallel_invoke.cpp">complete
                            example</link>.</para>
                </sect2>
                <sect2>
                    <title>parallel_find_all</title>
                    <para>This algorithms finds and copies into a returned container all elements of
                        a range for which a predicate returns true. Like parallel_for, we have four
                        versions of the algorithm.</para>
                    <programlisting>template &lt;class Iterator, class Func,
          class ReturnRange=std::vector&lt;typename std::iterator_traits&lt;Iterator>::value_type>,
          class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;ReturnRange,Job>
<emphasis role="bold">parallel_find_all</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range, class Func, class ReturnRange=Range, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;ReturnRange,Job>
<emphasis role="bold">parallel_find_all</emphasis>(Range const&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Range, class Func, class ReturnRange=Range, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;ReturnRange,Job>
<emphasis role="bold">parallel_find_all</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Range, class Func, class ReturnRange=typename Range::return_type, class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;ReturnRange,Job>
<emphasis role="bold">parallel_find_all</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>The algorithm will find elements matching the search criteria in parallel
                        and copy all into a new container, by default of the type given as
                        argument:</para>
                    <para>
                        <programlisting><emphasis role="bold">std::vector&lt;int></emphasis> data;
post_callback(
    [this]()
    {
       return boost::asynchronous::<emphasis role="bold">parallel_find_all</emphasis>(this->data.begin(),this->data.end(),
                                                     [](int i)
                                                     {
                                                         return (400 &lt;= i) &amp;&amp; (i &lt; 600);
                                                     },
                                                     1500);
    },
    ](<emphasis role="bold">boost::asynchronous::expected&lt;std::vector&lt;int>></emphasis> ){} // callback gets an int
);</programlisting>
                    </para>
                    <para>Please have a look at the <link xlink:href="examples/example_parallel_find_all.cpp">complete example</link>.</para>
                </sect2>
                <sect2>
                    <title>parallel_extremum</title>
                    <para>parallel_extremum finds an extremum (min/max) of a range given by a
                        predicate. It is a good example of using a prallel_reduce for writing new
                        algorithms. We have, as usual, four versions of the algorithm:.</para>
                    <para>
                        <programlisting>template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;typename std::iterator_traits&lt;Iterator>::value_type,Job>
<emphasis role="bold">parallel_extremum</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
decltype(boost::asynchronous::parallel_reduce(...))
<emphasis role="bold">parallel_extremum</emphasis>(Range const&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
decltype(boost::asynchronous::parallel_reduce(...))
<emphasis role="bold">parallel_extremum</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
decltype(boost::asynchronous::parallel_reduce(...))
<emphasis role="bold">parallel_extremum</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    </para>
                    <para>Please have a look at the <link xlink:href="examples/example_parallel_extremum.cpp">complete example</link>.</para>
                </sect2>
                <sect2>
                    <title>parallel_count</title>
                    <para>parallel_count counts the elements of a range satisfying a predicate. As
                        usual, we have four versions of the algorithm.</para>
                    <programlisting>template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;long,Job>
<emphasis role="bold">parallel_count</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;long,Job>
<emphasis role="bold">parallel_count</emphasis>(Range const&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;long,Job>
<emphasis role="bold">parallel_count</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;long,Job>
<emphasis role="bold">parallel_count</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>Please have a look at the <link xlink:href="examples/example_parallel_count.cpp">complete example</link>.</para>
                </sect2>
                <sect2>
                    <title>parallel_sort / parallel_stable_sort</title>
                    <para>parallel_sort / parallel_stable_sort implement a parallel mergesort. As
                        usual, we have four versions of the algorithm. Func is a binary functor used
                        for sorting, like std::sort.</para>
                    <programlisting>template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;long,Job>
<emphasis role="bold">parallel_sort</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_stable_sort</emphasis>(Iterator beg, Iterator end,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;long,Job>
<emphasis role="bold">parallel_sort</emphasis>(Range&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_stable_sort</emphasis>(Range&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking ownership of the container to be sorted
template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;long,Job>
<emphasis role="bold">parallel_sort_move</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_stable_sort_move</emphasis>(Range&amp;&amp; range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);

// version taking a continuation of a range as first argument
template &lt;class Iterator, class Func,class Job=BOOST_ASYNCHRONOUS_DEFAULT_JOB>
boost::asynchronous::detail::callback_continuation&lt;long,Job>
<emphasis role="bold">parallel_sort</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);
<emphasis role="bold">parallel_stable_sort</emphasis>(Range range,Func func,long cutoff,const std::string&amp; task_name="", std::size_t prio=0);</programlisting>
                    <para>Please have a look at the <link xlink:href="examples/example_parallel_count.cpp">complete example</link>.</para>
                </sect2>                
            </sect1>
            <sect1>
                <title>Parallel containers</title>
                <para>TODO example.</para>
                <para>Any gain made by using a parallel algorithm can be reduced to nothing if the
                    calling codes spends most of its time creating a std::vector. Interestingly,
                    most parallel libraries provide parallel algorithms, but very few offer parallel
                    data structures. This is unfortunate because a container can be parallelized
                    with a great gain as long as the contained type either has a non-simple
                    constructor / destructor or simply is big enough, as our tests show (see
                    test/perf/perf_vector.cpp). Though memory allocating is not parallel,
                    constructors can be made so. Reallocating and resizing can also greatly
                    benefit.</para>
                <para>Asynchronous fills this gap by providing boost::asynchronous::vector. It can
                    be used as a parallel, synchronous type. Apart from the construction, it looks
                    and feels very much like a std::vector with an added threadpool. In this case,
                        <emphasis role="underline">it cannot be posted to its own threadpool without
                        releasing it</emphasis> (see release_scheduler / set_scheduler). It is
                    defined in:</para>
                <para>#include &lt;boost/asynchronous/container/vector.hpp> </para>
                <para>The vector supports the same constructors that std::vector, with as extra
                    parameters, the threadpool for parallel work, and a cutoff. Optionally, a name
                    used for logging and a threadpool priority can be given, for example:</para>
                <programlisting>struct LongOne;
boost::asynchronous::any_shared_scheduler_proxy&lt;> pool = 
               boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                            boost::asynchronous::lockfree_queue&lt;>
                        >>(tpsize,tasks);

<emphasis role="bold">boost::asynchronous::vector</emphasis>&lt;LongOne> vec (pool,1024 /* cutoff */, /* std::vector ctor arguments */ 10000,LongOne()
                                            // optional, name for logging, priority in the threadpool
                                            , "vector", 1);                </programlisting>
                <para>At this point, asynchronous::vector can be used like std::vector, with the
                    difference that constructor, destructor, operator=, assign, clear, push_back,
                    emplace_back, reserve, resize, erase, insert are executed in parallel in the
                    given threadpool.</para>
                <para>The vector adds a few members compared to std::vector:<itemizedlist>
                        <listitem>
                            <para>release_scheduler(): removes the threadpool from vector. At this
                                point, the vector is no more parallel, but can live from within the
                                pool.</para>
                        </listitem>
                        <listitem>
                            <para>set_scheduler(): (re)sets scheduler, so that vector is again
                                parallel. At this point, the vector cannot live from within the
                                pool.</para>
                        </listitem>
                        <listitem>
                            <para>long get_cutoff() const: returns the cutoff as given in
                                constructor.</para>
                        </listitem>
                        <listitem>
                            <para>std::string get_name() const: the logged name, as given in the
                                constructor.</para>
                        </listitem>
                        <listitem>
                            <para>std::size_t get_prio()const: the priority, as given in the
                                constructor.</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
        </chapter>
        <chapter>
            <title>Tips.</title>
            <sect1>
                <title>Which protections you get, which ones you don't.</title>
                <para>Asynchronous is doing much to protect developers from some ugly beasts around:<itemizedlist>
                        <listitem>
                            <para>(visible) threads</para>
                        </listitem>
                        <listitem>
                            <para>races</para>
                        </listitem>
                        <listitem>
                            <para>deadlocks</para>
                        </listitem>
                        <listitem>
                            <para>crashes at the end of an object lifetime</para>
                        </listitem>
                </itemizedlist></para>
                <para>It also helps parallelizing and improve performance by not blocking. It also
                    helps find out where bottlenecks and hidden possible performance gains
                    are.</para>
                <para>There are, however, things for which it cannot help:<itemizedlist>
                        <listitem>
                            <para>cycles in design</para>
                        </listitem>
                        <listitem>
                            <para>C++ legal ways to work around the protections if one really
                                wants.</para>
                        </listitem>
                        <listitem>
                            <para>blocking on a future if one really wants.</para>
                        </listitem>
                        <listitem>
                            <para>using "this" captured in a task lambda.</para>
                        </listitem>
                        <listitem>
                            <para>writing a not clean task with pointers or references to data used
                                in a servant.</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>No cycle, ever</title>
                 <para>This is one of the first things one learns in a design class. Cycles are
                    evil. Everybody knows it. And yet, designs are often made without care in a too
                    agile process, dependency within an application is not thought out carefully
                    enough and cycles happen. What we do learn in these classes is that cycles make
                    our code monolithic and not reusable. What we however do not learn is how bad,
                    bad, bad this is in face of threads. It becomes impossible to follow the flow of
                    information, resource usage, degradation of performance. But the worst of all,
                    it becomes almost impossible to prevent deadlocks and resource leakage.</para>
                <para>Using Asynchronous will help write clean layered architectures. But it will
                    not replace carefully crafted designs, thinking before writing code and the
                    experience which make a good designer. Asynchronous will not be able to prevent
                    code having cycles in a design. </para>
                <para>Fortunately, there is an easy solution: back to the basics, well-thought
                    designs before coding, writing diagrams, using a real development process (hint:
                    an agile "process" is not all this in the author's mind).</para>
            </sect1>
            <sect1>
                <title>No "this" within a task.</title>
                <para>A very easy way to see if you are paving the way to a race even using
                    Asynchronous is to have a look at the captured variables of a lambda posted to a
                    threadpool. If you find "this", it's probably bad, unless you really know that
                    the single-thread code will do nothing. Apart from a simple application, this
                    will not be true. By extension, pointers, references, or even shared smart
                    pointers pointing to data living in a single-thread world is usually bad.</para>
                <para>Experience shows that there are only two safe way to pass data to a posted
                    task: copy for basic types or types having a trivial destructor and move for
                    everything else. Keep to this rule and you will be safe.</para>
                <para>On the other hand, "this" is okay in the capture list of a callback task as
                    Asynchronous will only call it if the servant is still alive.</para>
            </sect1>
        </chapter>
    </part>
    <part>
        <title>Reference</title>
        <chapter>
            <title>Queues</title>
            <para> Asynchronous provides a range of queues with different trade-offs. Use
                    <code>lockfree_queue</code> as default for a quickstart with
                Asynchronous.</para>
            <sect1>
                <title>threadsafe_list</title>
                <para>This queue is mostly the one presented in Anthony Williams' book, "C++
                    Concurrency In Action". It is made of a single linked list of nodes, with a
                    mutex at each end of the queue to minimize contention. It is reasonably fast and
                    of simple usage. It can be used in all configurations of pools.</para>
                <para>Its constructor does not require any parameter forwarded from the
                    scheduler.</para>
                <para>Stealing: from the same queue end as pop. Will be implemented better (from the
                    other end to reduce contention) in a future version.</para>
                <para><emphasis role="underline">Caution</emphasis>: crashes were noticed with gcc
                    4.8 while 4.7 and clang 3.3 seemed ok though the compiler might be the reason.
                    For this reason, lockfree_queue is now the default queue.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class threadsafe_list;</programlisting>
            </sect1>
            <sect1>
                <title>lockfree_queue</title>
                <para>This queue is a light wrapper around a <code>boost::lockfree::queue</code>,
                    which gives lockfree behavior at the cost of an extra dynamic memory allocation.
                    Please use this container as default when starting with Asynchronous.</para>
                <para>The container is faster than a <code>threadsafe_list</code>, provided one
                    manages to set the queue size to an optimum value. A too small size will cause
                    expensive memory allocations, a too big size will significantly degrade
                    performance.</para>
                <para>Its constructor takes optionally a default size forwarded from the
                    scheduler.</para>
                <para>Stealing: from the same queue end as pop. Stealing from the other end is not
                    supported by <code>boost::lockfree::queue</code>. It can be used in all
                    configurations of pools.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_queue;</programlisting>
            </sect1>
            <sect1>
                <title>lockfree_spsc_queue</title>
                <para>This queue is a light wrapper around a
                        <code>boost::lockfree::spsc_queue</code>, which gives lockfree behavior at
                    the cost of an extra dynamic memory allocation. </para>
                <para>Its constructor requires a default size forwarded from the scheduler.</para>
                <para>Stealing: None. Stealing is not supported by
                        <code>boost::lockfree::spsc_queue</code>. It can only be used
                    Single-Producer / Single-Consumer, which reduces its typical usage to a queue of
                    a <code>multiqueue_threadpool_scheduler</code> as consumer, with a
                        <code>single_thread_scheduler</code> as producer.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_spsc_queue;                </programlisting>
            </sect1>
            <sect1>
                <title>lockfree_stack</title>
                <para>This queue is a light wrapper around a <code>boost::lockfree::stack</code>,
                    which gives lockfree behavior at the cost of an extra dynamic memory allocation.
                    This container creates a task inversion as the last posted tasks will be
                    executed first.</para>
                <para>Its constructor requires a default size forwarded from the scheduler.</para>
                <para>Stealing: from the same queue end as pop. Stealing from the other end is not
                    supported by <code>boost::lockfree::stack</code>. It can be used in all
                    configurations of pools.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class JOB = boost::asynchronous::any_callable>
class lockfree_stack;</programlisting>
            </sect1>
        </chapter>
        <chapter>
            <title>Schedulers</title>
            <para> There is no perfect scheduler. In any case it's a question of trade-off. Here are
                the schedulers offered by Asynchronous.</para>
            <sect1>
                <title>single_thread_scheduler</title>
                <para>The scheduler of choice for all servants which are not thread-safe. Serializes
                    all calls to a single queue and executes them in order. Using
                        <code>any_queue_container</code> as queue will however allow it to support
                    task priority.</para>
                <para>This scheduler does not steal from other queues or pools, and does not get
                    stolen from to avoid races.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue, class CPULoad>
class single_thread_scheduler;               </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
         boost::asynchronous::<emphasis role="bold">single_thread_scheduler</emphasis>&lt;
            boost::asynchronous::lockfree_queue&lt;>>>();  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
       boost::asynchronous::<emphasis role="bold">single_thread_scheduler</emphasis>&lt;
          boost::asynchronous::lockfree_queue&lt;>>>(10); // size of queue
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                                boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>>>>();                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                                boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>>>>(10); // size of queue</programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/single_thread_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry>1</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 thread)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>multiple_thread_scheduler</title>
                <para>The scheduler is an extended version of single_thread_scheduler, where all
                    servants are operated by only one thread at a time, though not always the same
                    one. It creates a n (servants) to m (threads) dependency. The advantages of this
                    scheduler is that one long task will not block other servants, more flexibility
                    in distributing threads among servants, and better cache behaviour (a thread
                    tries to serve servants in order).</para>
                <para>This scheduler does not steal from other queues or pools, and does not get
                    stolen from to avoid races.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue, class CPULoad>
class multiple_thread_scheduler;               </programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
           boost::asynchronous::<emphasis role="bold">multiple_thread_scheduler</emphasis>&lt;
              boost::asynchronous::lockfree_queue&lt;>>>(n,m); // n: max number of servants, m: number of worker threads

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
           boost::asynchronous::<emphasis role="bold">multiple_thread_scheduler</emphasis>&lt;
              boost::asynchronous::lockfree_queue&lt;>>>(n,m,10); // n: max number of servants, m: number of worker threads, 10: size of queue
 
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                                boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>>>>(n,m); // n: max number of servants, m: number of worker threads
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                                boost::asynchronous::single_thread_scheduler&lt;
                                     boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>>>>(n,m,10); // n: max number of servants, m: number of worker threads, 10: size of queue</programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/single_thread_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry>1..n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>No</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>threadpool_scheduler</title>
                <para>The simplest and easiest threadpool using a single queue, though multiqueue
                    behavior could be done using <code>any_queue_container</code>. The advantage is
                    that it allows the pool to be given 0 thread and only be stolen from. The cost
                    is a slight performance loss due to higher contention on the single
                    queue.</para>
                <para>This pool does not steal from other pool's queues.</para>
                <para>Use this pool as default for a quickstart with Asynchronous.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class CPULoad>
class threadpool_scheduler;</programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>>>(4); // 4 threads in pool  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;>>>(4,10); // size of queue=10, 4 threads in pool
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>>>>(4); // 4 threads in pool                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>>>>(4,10); // size of queue=10, 4 threads in pool  </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">0</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 queue)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>multiqueue_threadpool_scheduler</title>
                <para>This is a <code>threadpool_scheduler</code> with multiple queues to reduce
                    contention. On the other hand, this pool requires at least one thread.</para>
                <para>This pool does not steal from other pool's queues though pool threads do steal
                    from each other's queues.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class FindPosition=boost::asynchronous::default_find_position&lt; >, class CPULoad >
class multiqueue_threadpool_scheduler;</programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>>>(4); // 4 threads in pool  

boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;>>>(4,10); // size of queue=10, 4 threads in pool
                </programlisting>
                <para>Or, using logging:</para>
                <programlisting>typedef boost::asynchronous::any_loggable&lt;boost::chrono::high_resolution_clock> <emphasis role="bold">servant_job</emphasis>;

boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;<emphasis role="bold">servant_job</emphasis>>>>(4); // 4 threads in pool                                      
                
boost::asynchronous::any_shared_scheduler_proxy&lt;<emphasis role="bold">servant_job</emphasis>> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::lockfree_queue&lt;<emphasis role="bold">servant_job</emphasis>>>>(4,10); // size of queue=10, 4 threads in pool  </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/multiqueue_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>stealing_threadpool_scheduler</title>
                <para>This is a <code>threadpool_scheduler</code> with the added capability to steal
                    from other pool's queues within a <code>composite_threadpool_scheduler</code>.
                    Not used within a <code>composite_threadpool_scheduler</code>, it is a standard
                        <code>threadpool_scheduler</code>.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class CPULoad, bool /* InternalOnly */ = true >
class stealing_threadpool_scheduler;</programlisting>
                <para>Creation if used within a <code>composite_threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::stealing_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>>>(4); // 4 threads in pool</programlisting>
                <para> However, if used stand-alone, which has little interest outside of unit
                    tests, we need to add a template parameter to inform it:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::stealing_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;><emphasis role="bold">,true</emphasis> >>(4); // 4 threads in pool</programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/stealing_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">0</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A (only 1 queue)</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>stealing_multiqueue_threadpool_scheduler</title>
                <para>This is a <code>multiqueue_threadpool_scheduler</code> with the added
                    capability to steal from other pool's queues within a
                        <code>composite_threadpool_scheduler</code> (of course, threads within this
                    pool do steal from each other queues, with higher priority). Not used within a
                        <code>composite_threadpool_scheduler</code>, it is a standard
                        <code>multiqueue_threadpool_scheduler</code>.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class Queue,class FindPosition=boost::asynchronous::default_find_position&lt; >,class CPULoad, bool /* InternalOnly */= true  >
class stealing_multiqueue_threadpool_scheduler;</programlisting>
                <para>Creation if used within a <code>composite_threadpool_scheduler</code>:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>>>(4); // 4 threads in pool  
                </programlisting>
                <para> However, if used stand-alone, which has little interest outside of unit
                    tests, we need to add a template parameter to inform it:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;
                          boost::asynchronous::threadsafe_list&lt;>,boost::asynchronous::default_find_position&lt;>,<emphasis role="bold">true</emphasis>  >>(4); // 4 threads in pool  </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/stealing_multiqueue_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>composite_threadpool_scheduler</title>
                <para>This pool owns no thread by itself. Its job is to contain other pools,
                    accessible by the priority given by posting, and share all queues of its
                    subpools among them. Only the stealing_* pools and <code>asio_scheduler</code>
                    will make use of this and steal from other pools though.</para>
                <para>For creation we need to create other pool of stealing or not stealing, stolen
                    from or not, schedulers. stealing_xxx pools will try to steal jobs from other
                    pool of the same composite, but only if these schedulers support this. Other
                    threadpools will not steal but get stolen from.
                        <code>single_thread_scheduler</code> will not steal or get stolen
                    from.</para>
                <programlisting>// create a composite threadpool made of:
// a multiqueue_threadpool_scheduler, 0 thread
// This scheduler does not steal from other schedulers, but will lend its queues for stealing
auto tp = boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::threadpool_scheduler&lt;boost::asynchronous::lockfree_queue&lt;>>> (0,100);

// a stealing_multiqueue_threadpool_scheduler, 3 threads, each with a threadsafe_list
// this scheduler will steal from other schedulers if it can. In this case it will manage only with tp, not tp3
auto tp2 = boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::stealing_multiqueue_threadpool_scheduler&lt;boost::asynchronous::threadsafe_list&lt;>>> (3);

// composite pool made of the previous 2
auto tp_worker = boost::asynchronous::make_shared_scheduler_proxy&lt;<emphasis role="bold">boost::asynchronous::composite_threadpool_scheduler&lt;>>(tp,tp2)</emphasis>; 
                </programlisting>
                <para>Declaration:</para>
                <programlisting>template&lt;class Job = boost::asynchronous::any_callable,
         class FindPosition=boost::asynchronous::default_find_position&lt; >,
         class Clock = boost::chrono::high_resolution_clock  >
class composite_threadpool_scheduler;                 
                </programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/scheduler/composite_threadpool_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry>0</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>N/A</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>No</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
            <sect1>
                <title>asio_scheduler</title>
                <para>This pool brings the infrastructure and access to io_service for an integrated
                    usage of Boost.Asio. Furthermore, if used withing a
                        <code>composite_threadpool_scheduler</code>, it will steal jobs from other
                    pool's queues.</para>
                <para>Declaration:</para>
                <programlisting>template&lt;class FindPosition=boost::asynchronous::default_find_position&lt; boost::asynchronous::sequential_push_policy >, class CPULoad >
class asio_scheduler;</programlisting>
                <para>Creation:</para>
                <programlisting>boost::asynchronous::any_shared_scheduler_proxy&lt;> scheduler = 
    boost::asynchronous::make_shared_scheduler_proxy&lt;
                    boost::asynchronous::asio_scheduler&lt;>>(4); // 4 threads in pool</programlisting>
                <para>
                    <table frame="all">
                        <title>#include
                            &lt;boost/asynchronous/extensions/asio/asio_scheduler.hpp></title>
                        <tgroup cols="2">
                            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
                            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
                            <thead>
                                <row>
                                    <entry>Characteristics</entry>
                                    <entry/>
                                </row>
                            </thead>
                            <tbody>
                                <row>
                                    <entry>Number of threads</entry>
                                    <entry><emphasis role="bold">1</emphasis>-n</entry>
                                </row>
                                <row>
                                    <entry>Can be stolen from?</entry>
                                    <entry>No*</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in this pool?</entry>
                                    <entry>Yes</entry>
                                </row>
                                <row>
                                    <entry>Can steal from other threads in other pools?</entry>
                                    <entry>Yes</entry>
                                </row>
                            </tbody>
                        </tgroup>
                    </table>
                </para>
            </sect1>
        </chapter>
        <chapter>
            <title>Compiler, linker, settings</title>
            <sect1>
                <title>C++ 11</title>
                <para>Asynchronous is C++11/14-only. Please check that your compiler has C++11
                    enabled (-std=c++0x or -std=c++11 in different versions of gcc). Usually, C++14
                    is recommended.</para>
            </sect1>
            <sect1>
                <title>Supported compilers</title>
                <para>Asynchronous is tested and ok with: <itemizedlist>
                        <listitem>
                            <para>gcc: >= 4.9</para>
                        </listitem>
                        <listitem>
                            <para>clang: >= 3.5</para>
                        </listitem>
                        <listitem>
                            <para>VS2015 with a limitation: BOOST_ASYNC_FUTURE/POST_MEMBER_1(or _2
                                or _3) as variadic macros are not supported</para>
                        </listitem>
                        <listitem>
                            <para>Intel ICC >= 13.</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
            <sect1>
                <title>Supported targets</title>
                <para>Asynchronous has been tested on Linux and Windows PCs, Intel and AMD, with the
                    above compilers, and with mingw.</para>
                <para>Asynchronous being based on Boost.Thread, can also work on Intel Xeon Phi with
                    a minor change: within Boost, all usage of boost::shared_ptr must be replaced by
                    std::shared_ptr. Strongly recommended is linking with tbbmalloc_proxy for better
                    performance.</para>
            </sect1>
            <sect1>
                <title>Linking</title>
                <para>Asynchronous is header-only, but requires Boost libraries which are not. One
                    should link with: boost_system, boost_thread, boost_chrono and boost_date_time
                    if logging is required</para>
            </sect1>
            <sect1>
                <title>Compile-time switches</title>
                <para>The following symbols will, when defined, influence the behaviour of the library:<itemizedlist>
                        <listitem>
                            <para>BOOST_ASYNCHRONOUS_DEFAULT_JOB replaces
                                boost::asynchronous::any_callable by the required job type.</para>
                        </listitem>
                        <listitem>
                            <para>BOOST_ASYNCHRONOUS_REQUIRE_ALL_ARGUMENTS: forces Asynchronous to
                                only provide servant_proxy macros with all their arguments to avoid
                                accidental forgetting. Precisely:<itemizedlist>
                                    <listitem>
                                        <para>BOOST_ASYNC_FUTURE_MEMBER /BOOST_ASYNC_POST_MEMBER
                                            require priority</para>
                                    </listitem>
                                    <listitem>
                                        <para>BOOST_ASYNC_FUTURE_MEMBER_LOG /
                                            BOOST_ASYNC_POST_MEMBER_LOG require task name and
                                            priority</para>
                                    </listitem>
                                    <listitem>
                                        <para>make_safe_callback requires name and priority</para>
                                    </listitem>
                                    <listitem>
                                        <para>make_lambda_continuation_wrapper requires task
                                            name</para>
                                    </listitem>
                                    <listitem>
                                        <para>parallel algorithms require task name and
                                            priority</para>
                                    </listitem>
                                    <listitem>
                                        <para>asynchronous::vector requires as last arguments name
                                            and priority</para>
                                    </listitem>
                                </itemizedlist></para>
                        </listitem>
                        <listitem>
                            <para>BOOST_ASYNCHRONOUS_NO_SAVING_CPU_LOAD: overrides default of
                                Asynchronous: schedulers will run at full speed. This can slightly
                                increase speed, at the cost of high CPU load.</para>
                        </listitem>
                        <listitem>
                            <para>BOOST_ASYNCHRONOUS_PRCTL_SUPPORT: Allows naming of threads if
                                sys/prctl is supported (Linux).</para>
                        </listitem>
                        <listitem>
                            <para>BOOST_ASYNCHRONOUS_USE_BOOST_SPREADSORT: in older Boost versions,
                                Spreasort was not included. This switch will provide
                                parallel_spreadsort, parallel_quick_spreadsort and
                                parallel_spreadsort_inplace</para>
                        </listitem>
                    </itemizedlist></para>
            </sect1>
        </chapter>
    </part>
</book>
